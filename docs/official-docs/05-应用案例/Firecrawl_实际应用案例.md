# Firecrawl å®é™…åº”ç”¨æ¡ˆä¾‹

> çœŸå®é¡¹ç›®ä¸­çš„Firecrawlåº”ç”¨å®ä¾‹å’Œè§£å†³æ–¹æ¡ˆ
> æ›´æ–°æ—¶é—´: 2024å¹´

## ç›®å½•

1. [æ–°é—»èµ„è®¯é‡‡é›†ç³»ç»Ÿ](#æ–°é—»èµ„è®¯é‡‡é›†ç³»ç»Ÿ)
2. [ç”µå•†ä»·æ ¼ç›‘æ§ç³»ç»Ÿ](#ç”µå•†ä»·æ ¼ç›‘æ§ç³»ç»Ÿ)
3. [å­¦æœ¯è®ºæ–‡æ”¶é›†å™¨](#å­¦æœ¯è®ºæ–‡æ”¶é›†å™¨)
4. [ç«å“åˆ†æå·¥å…·](#ç«å“åˆ†æå·¥å…·)
5. [å†…å®¹èšåˆå¹³å°](#å†…å®¹èšåˆå¹³å°)
6. [SEOåˆ†æå·¥å…·](#seoåˆ†æå·¥å…·)
7. [ç¤¾äº¤åª’ä½“ç›‘æ§](#ç¤¾äº¤åª’ä½“ç›‘æ§)
8. [æˆ¿äº§ä¿¡æ¯é‡‡é›†](#æˆ¿äº§ä¿¡æ¯é‡‡é›†)
9. [æ‹›è˜ä¿¡æ¯èšåˆ](#æ‹›è˜ä¿¡æ¯èšåˆ)
10. [æŠ€æœ¯æ–‡æ¡£æ•´ç†](#æŠ€æœ¯æ–‡æ¡£æ•´ç†)

## æ–°é—»èµ„è®¯é‡‡é›†ç³»ç»Ÿ

### é¡¹ç›®èƒŒæ™¯
æ„å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–æ–°é—»é‡‡é›†ç³»ç»Ÿï¼Œä»å¤šä¸ªæ–°é—»ç½‘ç«™æ”¶é›†æœ€æ–°èµ„è®¯ï¼Œè¿›è¡Œåˆ†ç±»æ•´ç†å¹¶æ¨é€ç»™ç”¨æˆ·ã€‚

### æŠ€æœ¯æ¶æ„
```
æ–°é—»æºç½‘ç«™ â†’ Firecrawl API â†’ å†…å®¹å¤„ç† â†’ æ•°æ®åº“å­˜å‚¨ â†’ ç”¨æˆ·ç•Œé¢
```

### å®ç°ä»£ç 

```python
import asyncio
import json
from datetime import datetime, timedelta
from firecrawl import Firecrawl
from dataclasses import dataclass
from typing import List, Dict
import sqlite3
import hashlib

@dataclass
class NewsArticle:
    title: str
    content: str
    url: str
    published_date: str
    category: str
    source: str
    summary: str
    keywords: List[str]

class NewsCollector:
    def __init__(self, api_key: str):
        self.firecrawl = Firecrawl(api_key=api_key)
        self.news_sources = {
            "ç§‘æŠ€": [
                "https://techcrunch.com",
                "https://www.theverge.com",
                "https://arstechnica.com"
            ],
            "è´¢ç»": [
                "https://www.bloomberg.com",
                "https://www.reuters.com/business",
                "https://finance.yahoo.com"
            ],
            "AI": [
                "https://www.artificialintelligence-news.com",
                "https://venturebeat.com/ai"
            ]
        }
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn = sqlite3.connect('news.db')
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT,
                url TEXT UNIQUE,
                published_date TEXT,
                category TEXT,
                source TEXT,
                summary TEXT,
                keywords TEXT,
                content_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()
    
    def get_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡"""
        return hashlib.md5(content.encode()).hexdigest()
    
    def is_duplicate(self, content_hash: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤å†…å®¹"""
        cursor = self.conn.execute(
            "SELECT 1 FROM articles WHERE content_hash = ?", 
            (content_hash,)
        )
        return cursor.fetchone() is not None
    
    def extract_article_info(self, url: str) -> Dict:
        """æå–æ–‡ç« ä¿¡æ¯"""
        try:
            result = self.firecrawl.scrape(
                url=url,
                formats=[{
                    "type": "json",
                    "prompt": """
                    è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                    1. æ–‡ç« æ ‡é¢˜
                    2. æ–‡ç« æ­£æ–‡å†…å®¹
                    3. å‘å¸ƒæ—¥æœŸ
                    4. ä½œè€…
                    5. æ–‡ç« æ‘˜è¦ï¼ˆå¦‚æœæ²¡æœ‰è¯·ç”Ÿæˆä¸€ä¸ª100å­—ä»¥å†…çš„æ‘˜è¦ï¼‰
                    6. å…³é”®è¯ï¼ˆ3-5ä¸ªï¼‰
                    
                    è¯·ç¡®ä¿æå–çš„å†…å®¹å‡†ç¡®å®Œæ•´ã€‚
                    """,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "content": {"type": "string"},
                            "published_date": {"type": "string"},
                            "author": {"type": "string"},
                            "summary": {"type": "string"},
                            "keywords": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["title", "content"]
                    }
                }],
                only_main_content=True,
                exclude_tags=["nav", "footer", "aside", "ads"]
            )
            
            return result['data']['json']
        except Exception as e:
            print(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥ {url}: {e}")
            return None
    
    def collect_from_source(self, source_url: str, category: str) -> List[NewsArticle]:
        """ä»å•ä¸ªæ–°é—»æºæ”¶é›†æ–‡ç« """
        articles = []
        
        try:
            # é¦–å…ˆè·å–ç½‘ç«™åœ°å›¾
            map_result = self.firecrawl.map(
                url=source_url,
                limit=100
            )
            
            # è¿‡æ»¤å‡ºæ–‡ç« é“¾æ¥
            article_urls = []
            for link in map_result['links']:
                # æ ¹æ®URLæ¨¡å¼è¿‡æ»¤æ–‡ç« é“¾æ¥
                if any(pattern in link.lower() for pattern in 
                      ['/article/', '/news/', '/post/', '/blog/', '/story/']):
                    article_urls.append(link)
            
            # é™åˆ¶æ¯æ¬¡å¤„ç†çš„æ–‡ç« æ•°é‡
            article_urls = article_urls[:20]
            
            print(f"ä» {source_url} å‘ç° {len(article_urls)} ç¯‡æ–‡ç« ")
            
            for url in article_urls:
                article_info = self.extract_article_info(url)
                if not article_info:
                    continue
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                content_hash = self.get_content_hash(article_info.get('content', ''))
                if self.is_duplicate(content_hash):
                    continue
                
                article = NewsArticle(
                    title=article_info.get('title', ''),
                    content=article_info.get('content', ''),
                    url=url,
                    published_date=article_info.get('published_date', ''),
                    category=category,
                    source=source_url,
                    summary=article_info.get('summary', ''),
                    keywords=article_info.get('keywords', [])
                )
                
                articles.append(article)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.save_article(article, content_hash)
                
        except Exception as e:
            print(f"æ”¶é›†æ–°é—»å¤±è´¥ {source_url}: {e}")
        
        return articles
    
    def save_article(self, article: NewsArticle, content_hash: str):
        """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
        try:
            self.conn.execute("""
                INSERT OR IGNORE INTO articles 
                (title, content, url, published_date, category, source, summary, keywords, content_hash)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article.title,
                article.content,
                article.url,
                article.published_date,
                article.category,
                article.source,
                article.summary,
                json.dumps(article.keywords),
                content_hash
            ))
            self.conn.commit()
        except Exception as e:
            print(f"ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
    
    def collect_all_news(self):
        """æ”¶é›†æ‰€æœ‰æ–°é—»æºçš„æ–‡ç« """
        all_articles = []
        
        for category, sources in self.news_sources.items():
            print(f"\nå¼€å§‹æ”¶é›† {category} ç±»åˆ«æ–°é—»...")
            
            for source in sources:
                print(f"æ­£åœ¨å¤„ç†: {source}")
                articles = self.collect_from_source(source, category)
                all_articles.extend(articles)
                print(f"æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        return all_articles
    
    def get_recent_articles(self, hours: int = 24) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æ–‡ç« """
        cursor = self.conn.execute("""
            SELECT title, url, category, summary, created_at 
            FROM articles 
            WHERE created_at > datetime('now', '-{} hours')
            ORDER BY created_at DESC
        """.format(hours))
        
        return [{
            'title': row[0],
            'url': row[1],
            'category': row[2],
            'summary': row[3],
            'created_at': row[4]
        } for row in cursor.fetchall()]
    
    def generate_daily_report(self) -> str:
        """ç”Ÿæˆæ¯æ—¥æ–°é—»æŠ¥å‘Š"""
        articles = self.get_recent_articles(24)
        
        if not articles:
            return "ä»Šæ—¥æš‚æ— æ–°é—»æ›´æ–°"
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = {}
        for article in articles:
            category = article['category']
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(article)
        
        report = f"# æ¯æ—¥æ–°é—»æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d')}\n\n"
        report += f"ä»Šæ—¥å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« \n\n"
        
        for category, cat_articles in by_category.items():
            report += f"## {category} ({len(cat_articles)}ç¯‡)\n\n"
            for article in cat_articles[:5]:  # æ¯ä¸ªç±»åˆ«æ˜¾ç¤ºå‰5ç¯‡
                report += f"- **{article['title']}**\n"
                report += f"  {article['summary']}\n"
                report += f"  [é˜…è¯»åŸæ–‡]({article['url']})\n\n"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    collector = NewsCollector(api_key="fc-YOUR-API-KEY")
    
    # æ”¶é›†æ–°é—»
    print("å¼€å§‹æ”¶é›†æ–°é—»...")
    articles = collector.collect_all_news()
    print(f"\næ€»å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = collector.generate_daily_report()
    print("\n=== æ¯æ—¥æ–°é—»æŠ¥å‘Š ===")
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(f"news_report_{datetime.now().strftime('%Y%m%d')}.md", 'w', encoding='utf-8') as f:
        f.write(report)
```

### å®šæ—¶ä»»åŠ¡é…ç½®

```python
import schedule
import time

def run_news_collection():
    """è¿è¡Œæ–°é—»æ”¶é›†ä»»åŠ¡"""
    collector = NewsCollector(api_key="fc-YOUR-API-KEY")
    collector.collect_all_news()
    
    # å‘é€æ¯æ—¥æŠ¥å‘Š
    report = collector.generate_daily_report()
    # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶å‘é€æˆ–æ¨é€é€šçŸ¥
    print("æ–°é—»æ”¶é›†å®Œæˆ")

# æ¯å¤©æ—©ä¸Š8ç‚¹å’Œä¸‹åˆ6ç‚¹è¿è¡Œ
schedule.every().day.at("08:00").do(run_news_collection)
schedule.every().day.at("18:00").do(run_news_collection)

while True:
    schedule.run_pending()
    time.sleep(60)
```

## ç”µå•†ä»·æ ¼ç›‘æ§ç³»ç»Ÿ

### é¡¹ç›®èƒŒæ™¯
ç›‘æ§ç”µå•†å¹³å°å•†å“ä»·æ ¼å˜åŒ–ï¼Œä¸ºç”¨æˆ·æä¾›ä»·æ ¼è¶‹åŠ¿åˆ†æå’Œé™ä»·æé†’ã€‚

### å®ç°ä»£ç 

```python
import asyncio
from datetime import datetime
from firecrawl import Firecrawl
from dataclasses import dataclass
from typing import List, Optional
import sqlite3
import matplotlib.pyplot as plt
import pandas as pd

@dataclass
class Product:
    name: str
    url: str
    current_price: float
    original_price: float
    availability: str
    rating: float
    reviews_count: int
    description: str
    images: List[str]
    last_updated: datetime

class PriceMonitor:
    def __init__(self, api_key: str):
        self.firecrawl = Firecrawl(api_key=api_key)
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn = sqlite3.connect('price_monitor.db')
        
        # äº§å“è¡¨
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS products (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                url TEXT UNIQUE,
                target_price REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ä»·æ ¼å†å²è¡¨
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS price_history (
                id INTEGER PRIMARY KEY,
                product_id INTEGER,
                price REAL,
                availability TEXT,
                rating REAL,
                reviews_count INTEGER,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (product_id) REFERENCES products (id)
            )
        ''')
        
        self.conn.commit()
    
    def extract_product_info(self, url: str) -> Optional[Product]:
        """æå–å•†å“ä¿¡æ¯"""
        try:
            result = self.firecrawl.scrape(
                url=url,
                formats=[{
                    "type": "json",
                    "prompt": """
                    è¯·æå–ä»¥ä¸‹å•†å“ä¿¡æ¯ï¼š
                    1. å•†å“åç§°
                    2. å½“å‰ä»·æ ¼ï¼ˆæ•°å­—ï¼‰
                    3. åŸä»·ï¼ˆå¦‚æœæœ‰æŠ˜æ‰£ï¼‰
                    4. åº“å­˜çŠ¶æ€ï¼ˆæœ‰è´§/ç¼ºè´§/é¢„è®¢ç­‰ï¼‰
                    5. è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰
                    6. è¯„è®ºæ•°é‡
                    7. å•†å“æè¿°
                    8. å•†å“å›¾ç‰‡é“¾æ¥
                    
                    è¯·ç¡®ä¿ä»·æ ¼æ˜¯çº¯æ•°å­—ï¼Œä¸åŒ…å«è´§å¸ç¬¦å·ã€‚
                    """,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "current_price": {"type": "number"},
                            "original_price": {"type": "number"},
                            "availability": {"type": "string"},
                            "rating": {"type": "number"},
                            "reviews_count": {"type": "integer"},
                            "description": {"type": "string"},
                            "images": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["name", "current_price"]
                    }
                }],
                only_main_content=True,
                wait_for=3000
            )
            
            data = result['data']['json']
            
            return Product(
                name=data.get('name', ''),
                url=url,
                current_price=data.get('current_price', 0),
                original_price=data.get('original_price', data.get('current_price', 0)),
                availability=data.get('availability', 'æœªçŸ¥'),
                rating=data.get('rating', 0),
                reviews_count=data.get('reviews_count', 0),
                description=data.get('description', ''),
                images=data.get('images', []),
                last_updated=datetime.now()
            )
            
        except Exception as e:
            print(f"æå–å•†å“ä¿¡æ¯å¤±è´¥ {url}: {e}")
            return None
    
    def add_product(self, url: str, target_price: float = None) -> int:
        """æ·»åŠ ç›‘æ§å•†å“"""
        product = self.extract_product_info(url)
        if not product:
            raise ValueError("æ— æ³•æå–å•†å“ä¿¡æ¯")
        
        cursor = self.conn.execute("""
            INSERT OR REPLACE INTO products (name, url, target_price)
            VALUES (?, ?, ?)
        """, (product.name, url, target_price))
        
        product_id = cursor.lastrowid
        
        # è®°å½•åˆå§‹ä»·æ ¼
        self.record_price(product_id, product)
        
        self.conn.commit()
        return product_id
    
    def record_price(self, product_id: int, product: Product):
        """è®°å½•ä»·æ ¼å†å²"""
        self.conn.execute("""
            INSERT INTO price_history 
            (product_id, price, availability, rating, reviews_count)
            VALUES (?, ?, ?, ?, ?)
        """, (
            product_id,
            product.current_price,
            product.availability,
            product.rating,
            product.reviews_count
        ))
    
    def check_all_products(self) -> List[Dict]:
        """æ£€æŸ¥æ‰€æœ‰ç›‘æ§å•†å“çš„ä»·æ ¼"""
        cursor = self.conn.execute("SELECT id, url, target_price FROM products")
        products = cursor.fetchall()
        
        alerts = []
        
        for product_id, url, target_price in products:
            print(f"æ£€æŸ¥å•†å“: {url}")
            
            current_product = self.extract_product_info(url)
            if not current_product:
                continue
            
            # è®°å½•æ–°ä»·æ ¼
            self.record_price(product_id, current_product)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€ä»·æ ¼æé†’
            if target_price and current_product.current_price <= target_price:
                alerts.append({
                    'product_id': product_id,
                    'name': current_product.name,
                    'url': url,
                    'current_price': current_product.current_price,
                    'target_price': target_price,
                    'discount': target_price - current_product.current_price
                })
        
        self.conn.commit()
        return alerts
    
    def get_price_history(self, product_id: int, days: int = 30) -> pd.DataFrame:
        """è·å–ä»·æ ¼å†å²"""
        query = """
            SELECT recorded_at, price, availability, rating, reviews_count
            FROM price_history
            WHERE product_id = ? AND recorded_at > datetime('now', '-{} days')
            ORDER BY recorded_at
        """.format(days)
        
        df = pd.read_sql_query(query, self.conn, params=(product_id,))
        df['recorded_at'] = pd.to_datetime(df['recorded_at'])
        return df
    
    def generate_price_chart(self, product_id: int, days: int = 30):
        """ç”Ÿæˆä»·æ ¼è¶‹åŠ¿å›¾"""
        df = self.get_price_history(product_id, days)
        
        if df.empty:
            print("æ²¡æœ‰ä»·æ ¼å†å²æ•°æ®")
            return
        
        plt.figure(figsize=(12, 6))
        plt.plot(df['recorded_at'], df['price'], marker='o', linewidth=2)
        plt.title(f'å•†å“ä»·æ ¼è¶‹åŠ¿ (æœ€è¿‘{days}å¤©)')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('ä»·æ ¼')
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        filename = f'price_chart_{product_id}_{datetime.now().strftime("%Y%m%d")}.png'
        plt.savefig(filename)
        plt.show()
        
        return filename
    
    def get_price_analysis(self, product_id: int) -> Dict:
        """è·å–ä»·æ ¼åˆ†æ"""
        df = self.get_price_history(product_id, 90)  # 90å¤©å†å²
        
        if df.empty:
            return {"error": "æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®"}
        
        current_price = df['price'].iloc[-1]
        min_price = df['price'].min()
        max_price = df['price'].max()
        avg_price = df['price'].mean()
        
        # è®¡ç®—ä»·æ ¼è¶‹åŠ¿
        if len(df) >= 7:
            recent_avg = df['price'].tail(7).mean()
            older_avg = df['price'].head(7).mean()
            trend = "ä¸Šæ¶¨" if recent_avg > older_avg else "ä¸‹è·Œ"
        else:
            trend = "æ•°æ®ä¸è¶³"
        
        return {
            "current_price": current_price,
            "min_price": min_price,
            "max_price": max_price,
            "avg_price": avg_price,
            "trend": trend,
            "discount_from_max": ((max_price - current_price) / max_price) * 100,
            "premium_from_min": ((current_price - min_price) / min_price) * 100
        }
    
    def send_price_alerts(self, alerts: List[Dict]):
        """å‘é€ä»·æ ¼æé†’"""
        if not alerts:
            print("æ²¡æœ‰ä»·æ ¼æé†’")
            return
        
        print(f"\n=== ä»·æ ¼æé†’ ({len(alerts)}ä¸ª) ===")
        for alert in alerts:
            print(f"ğŸ‰ {alert['name']}")
            print(f"   å½“å‰ä»·æ ¼: ${alert['current_price']:.2f}")
            print(f"   ç›®æ ‡ä»·æ ¼: ${alert['target_price']:.2f}")
            print(f"   èŠ‚çœ: ${alert['discount']:.2f}")
            print(f"   é“¾æ¥: {alert['url']}")
            print()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = PriceMonitor(api_key="fc-YOUR-API-KEY")
    
    # æ·»åŠ ç›‘æ§å•†å“
    product_urls = [
        "https://www.amazon.com/dp/B08N5WRWNW",  # Echo Dot
        "https://www.amazon.com/dp/B07XJ8C8F5",  # iPad
    ]
    
    for url in product_urls:
        try:
            product_id = monitor.add_product(url, target_price=50.0)
            print(f"å·²æ·»åŠ å•†å“ç›‘æ§ï¼ŒID: {product_id}")
        except Exception as e:
            print(f"æ·»åŠ å¤±è´¥: {e}")
    
    # æ£€æŸ¥ä»·æ ¼å˜åŒ–
    alerts = monitor.check_all_products()
    monitor.send_price_alerts(alerts)
    
    # ç”Ÿæˆä»·æ ¼åˆ†ææŠ¥å‘Š
    for product_id in [1, 2]:  # å‡è®¾æœ‰è¿™äº›äº§å“ID
        analysis = monitor.get_price_analysis(product_id)
        print(f"\nå•†å“ {product_id} ä»·æ ¼åˆ†æ:")
        print(f"å½“å‰ä»·æ ¼: ${analysis.get('current_price', 0):.2f}")
        print(f"å†å²æœ€ä½: ${analysis.get('min_price', 0):.2f}")
        print(f"å†å²æœ€é«˜: ${analysis.get('max_price', 0):.2f}")
        print(f"å¹³å‡ä»·æ ¼: ${analysis.get('avg_price', 0):.2f}")
        print(f"ä»·æ ¼è¶‹åŠ¿: {analysis.get('trend', 'æœªçŸ¥')}")
```

## å­¦æœ¯è®ºæ–‡æ”¶é›†å™¨

### é¡¹ç›®èƒŒæ™¯
è‡ªåŠ¨æ”¶é›†ç‰¹å®šç ”ç©¶é¢†åŸŸçš„æœ€æ–°å­¦æœ¯è®ºæ–‡ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œåˆ†ç±»æ•´ç†ã€‚

### å®ç°ä»£ç 

```python
from firecrawl import Firecrawl
from dataclasses import dataclass
from typing import List, Dict
import re
import sqlite3
from datetime import datetime
import requests
from urllib.parse import urljoin, urlparse

@dataclass
class Paper:
    title: str
    authors: List[str]
    abstract: str
    publication_date: str
    journal: str
    doi: str
    pdf_url: str
    keywords: List[str]
    citation_count: int
    url: str

class AcademicCollector:
    def __init__(self, api_key: str):
        self.firecrawl = Firecrawl(api_key=api_key)
        self.academic_sources = {
            "arxiv": "https://arxiv.org",
            "pubmed": "https://pubmed.ncbi.nlm.nih.gov",
            "ieee": "https://ieeexplore.ieee.org",
            "acm": "https://dl.acm.org"
        }
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn = sqlite3.connect('academic_papers.db')
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS papers (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                authors TEXT,
                abstract TEXT,
                publication_date TEXT,
                journal TEXT,
                doi TEXT UNIQUE,
                pdf_url TEXT,
                keywords TEXT,
                citation_count INTEGER,
                url TEXT,
                research_field TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.conn.commit()
    
    def search_arxiv(self, query: str, max_results: int = 50) -> List[Paper]:
        """æœç´¢arXivè®ºæ–‡"""
        papers = []
        
        try:
            # ä½¿ç”¨Firecrawlæœç´¢arXiv
            search_results = self.firecrawl.search(
                query=f"site:arxiv.org {query}",
                limit=max_results,
                scrape_options={
                    "formats": ["markdown"]
                }
            )
            
            for result in search_results['data']['web']:
                if 'arxiv.org' in result['url']:
                    paper = self.extract_arxiv_paper(result['url'])
                    if paper:
                        papers.append(paper)
        
        except Exception as e:
            print(f"æœç´¢arXivå¤±è´¥: {e}")
        
        return papers
    
    def extract_arxiv_paper(self, url: str) -> Paper:
        """æå–arXivè®ºæ–‡ä¿¡æ¯"""
        try:
            result = self.firecrawl.scrape(
                url=url,
                formats=[{
                    "type": "json",
                    "prompt": """
                    è¯·æå–ä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼š
                    1. è®ºæ–‡æ ‡é¢˜
                    2. ä½œè€…åˆ—è¡¨ï¼ˆæ•°ç»„æ ¼å¼ï¼‰
                    3. æ‘˜è¦
                    4. æäº¤æ—¥æœŸ
                    5. å­¦ç§‘åˆ†ç±»
                    6. DOIï¼ˆå¦‚æœæœ‰ï¼‰
                    7. PDFä¸‹è½½é“¾æ¥
                    8. å…³é”®è¯ï¼ˆä»æ‘˜è¦ä¸­æå–3-5ä¸ªï¼‰
                    
                    è¯·ç¡®ä¿ä¿¡æ¯å‡†ç¡®å®Œæ•´ã€‚
                    """,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "authors": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "abstract": {"type": "string"},
                            "publication_date": {"type": "string"},
                            "subject_class": {"type": "string"},
                            "doi": {"type": "string"},
                            "pdf_url": {"type": "string"},
                            "keywords": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["title", "authors", "abstract"]
                    }
                }]
            )
            
            data = result['data']['json']
            
            return Paper(
                title=data.get('title', ''),
                authors=data.get('authors', []),
                abstract=data.get('abstract', ''),
                publication_date=data.get('publication_date', ''),
                journal='arXiv',
                doi=data.get('doi', ''),
                pdf_url=data.get('pdf_url', ''),
                keywords=data.get('keywords', []),
                citation_count=0,  # arXivé€šå¸¸ä¸æ˜¾ç¤ºå¼•ç”¨æ•°
                url=url
            )
            
        except Exception as e:
            print(f"æå–arXivè®ºæ–‡å¤±è´¥ {url}: {e}")
            return None
    
    def search_pubmed(self, query: str, max_results: int = 50) -> List[Paper]:
        """æœç´¢PubMedè®ºæ–‡"""
        papers = []
        
        try:
            search_results = self.firecrawl.search(
                query=f"site:pubmed.ncbi.nlm.nih.gov {query}",
                limit=max_results,
                scrape_options={
                    "formats": ["markdown"]
                }
            )
            
            for result in search_results['data']['web']:
                if 'pubmed.ncbi.nlm.nih.gov' in result['url']:
                    paper = self.extract_pubmed_paper(result['url'])
                    if paper:
                        papers.append(paper)
        
        except Exception as e:
            print(f"æœç´¢PubMedå¤±è´¥: {e}")
        
        return papers
    
    def extract_pubmed_paper(self, url: str) -> Paper:
        """æå–PubMedè®ºæ–‡ä¿¡æ¯"""
        try:
            result = self.firecrawl.scrape(
                url=url,
                formats=[{
                    "type": "json",
                    "prompt": """
                    è¯·æå–ä»¥ä¸‹åŒ»å­¦è®ºæ–‡ä¿¡æ¯ï¼š
                    1. è®ºæ–‡æ ‡é¢˜
                    2. ä½œè€…åˆ—è¡¨
                    3. æ‘˜è¦
                    4. å‘è¡¨æ—¥æœŸ
                    5. æœŸåˆŠåç§°
                    6. DOI
                    7. PMID
                    8. å…³é”®è¯/MeSH terms
                    9. å¼•ç”¨æ¬¡æ•°ï¼ˆå¦‚æœæ˜¾ç¤ºï¼‰
                    
                    è¯·ç¡®ä¿ä¿¡æ¯å‡†ç¡®ã€‚
                    """,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "authors": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "abstract": {"type": "string"},
                            "publication_date": {"type": "string"},
                            "journal": {"type": "string"},
                            "doi": {"type": "string"},
                            "pmid": {"type": "string"},
                            "keywords": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "citation_count": {"type": "integer"}
                        },
                        "required": ["title", "authors"]
                    }
                }]
            )
            
            data = result['data']['json']
            
            return Paper(
                title=data.get('title', ''),
                authors=data.get('authors', []),
                abstract=data.get('abstract', ''),
                publication_date=data.get('publication_date', ''),
                journal=data.get('journal', ''),
                doi=data.get('doi', ''),
                pdf_url='',  # PubMedé€šå¸¸ä¸ç›´æ¥æä¾›PDF
                keywords=data.get('keywords', []),
                citation_count=data.get('citation_count', 0),
                url=url
            )
            
        except Exception as e:
            print(f"æå–PubMedè®ºæ–‡å¤±è´¥ {url}: {e}")
            return None
    
    def save_paper(self, paper: Paper, research_field: str):
        """ä¿å­˜è®ºæ–‡åˆ°æ•°æ®åº“"""
        try:
            self.conn.execute("""
                INSERT OR IGNORE INTO papers 
                (title, authors, abstract, publication_date, journal, doi, 
                 pdf_url, keywords, citation_count, url, research_field)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                paper.title,
                ', '.join(paper.authors),
                paper.abstract,
                paper.publication_date,
                paper.journal,
                paper.doi,
                paper.pdf_url,
                ', '.join(paper.keywords),
                paper.citation_count,
                paper.url,
                research_field
            ))
            self.conn.commit()
        except Exception as e:
            print(f"ä¿å­˜è®ºæ–‡å¤±è´¥: {e}")
    
    def collect_papers_by_field(self, research_field: str, keywords: List[str]) -> List[Paper]:
        """æŒ‰ç ”ç©¶é¢†åŸŸæ”¶é›†è®ºæ–‡"""
        all_papers = []
        
        for keyword in keywords:
            print(f"æœç´¢å…³é”®è¯: {keyword}")
            
            # æœç´¢arXiv
            arxiv_papers = self.search_arxiv(keyword, max_results=20)
            print(f"ä»arXivæ‰¾åˆ° {len(arxiv_papers)} ç¯‡è®ºæ–‡")
            
            # æœç´¢PubMedï¼ˆå¦‚æœæ˜¯ç”Ÿç‰©åŒ»å­¦ç›¸å…³ï¼‰
            if research_field.lower() in ['biology', 'medicine', 'biomedical']:
                pubmed_papers = self.search_pubmed(keyword, max_results=20)
                print(f"ä»PubMedæ‰¾åˆ° {len(pubmed_papers)} ç¯‡è®ºæ–‡")
                all_papers.extend(pubmed_papers)
            
            all_papers.extend(arxiv_papers)
            
            # ä¿å­˜è®ºæ–‡
            for paper in arxiv_papers + (pubmed_papers if 'pubmed_papers' in locals() else []):
                self.save_paper(paper, research_field)
        
        return all_papers
    
    def generate_research_report(self, research_field: str, days: int = 30) -> str:
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        cursor = self.conn.execute("""
            SELECT title, authors, journal, publication_date, url, keywords
            FROM papers 
            WHERE research_field = ? AND created_at > datetime('now', '-{} days')
            ORDER BY created_at DESC
        """.format(days), (research_field,))
        
        papers = cursor.fetchall()
        
        if not papers:
            return f"æœ€è¿‘{days}å¤©æ²¡æœ‰æ”¶é›†åˆ°{research_field}é¢†åŸŸçš„è®ºæ–‡"
        
        report = f"# {research_field} ç ”ç©¶æŠ¥å‘Š\n\n"
        report += f"æ—¶é—´èŒƒå›´: æœ€è¿‘{days}å¤©\n"
        report += f"è®ºæ–‡æ•°é‡: {len(papers)}ç¯‡\n\n"
        
        # æŒ‰æœŸåˆŠåˆ†ç»„
        by_journal = {}
        for paper in papers:
            journal = paper[2] or 'æœªçŸ¥æœŸåˆŠ'
            if journal not in by_journal:
                by_journal[journal] = []
            by_journal[journal].append(paper)
        
        report += "## æŒ‰æœŸåˆŠåˆ†å¸ƒ\n\n"
        for journal, journal_papers in by_journal.items():
            report += f"### {journal} ({len(journal_papers)}ç¯‡)\n\n"
            for paper in journal_papers[:5]:  # æ¯ä¸ªæœŸåˆŠæ˜¾ç¤ºå‰5ç¯‡
                report += f"- **{paper[0]}**\n"
                report += f"  ä½œè€…: {paper[1]}\n"
                report += f"  å‘è¡¨æ—¥æœŸ: {paper[3]}\n"
                report += f"  [æŸ¥çœ‹è¯¦æƒ…]({paper[4]})\n\n"
        
        # å…³é”®è¯ç»Ÿè®¡
        all_keywords = []
        for paper in papers:
            if paper[5]:  # keywords
                all_keywords.extend([k.strip() for k in paper[5].split(',')])
        
        keyword_count = {}
        for keyword in all_keywords:
            keyword_count[keyword] = keyword_count.get(keyword, 0) + 1
        
        top_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:10]
        
        report += "## çƒ­é—¨å…³é”®è¯\n\n"
        for keyword, count in top_keywords:
            report += f"- {keyword}: {count}æ¬¡\n"
        
        return report
    
    def download_papers(self, research_field: str, download_dir: str = "./papers"):
        """ä¸‹è½½è®ºæ–‡PDF"""
        import os
        
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
        
        cursor = self.conn.execute("""
            SELECT title, pdf_url FROM papers 
            WHERE research_field = ? AND pdf_url != ''
        """, (research_field,))
        
        papers = cursor.fetchall()
        
        for title, pdf_url in papers:
            try:
                response = requests.get(pdf_url)
                if response.status_code == 200:
                    # æ¸…ç†æ–‡ä»¶å
                    safe_title = re.sub(r'[^\w\s-]', '', title)[:50]
                    filename = f"{safe_title}.pdf"
                    filepath = os.path.join(download_dir, filename)
                    
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    print(f"å·²ä¸‹è½½: {filename}")
                else:
                    print(f"ä¸‹è½½å¤±è´¥: {title}")
            except Exception as e:
                print(f"ä¸‹è½½é”™è¯¯ {title}: {e}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    collector = AcademicCollector(api_key="fc-YOUR-API-KEY")
    
    # å®šä¹‰ç ”ç©¶é¢†åŸŸå’Œå…³é”®è¯
    research_fields = {
        "Machine Learning": [
            "deep learning", "neural networks", "transformer", 
            "computer vision", "natural language processing"
        ],
        "Quantum Computing": [
            "quantum algorithm", "quantum machine learning", 
            "quantum cryptography", "quantum error correction"
        ],
        "Biomedical AI": [
            "medical imaging AI", "drug discovery machine learning",
            "genomics deep learning", "clinical decision support"
        ]
    }
    
    # æ”¶é›†è®ºæ–‡
    for field, keywords in research_fields.items():
        print(f"\nå¼€å§‹æ”¶é›† {field} é¢†åŸŸè®ºæ–‡...")
        papers = collector.collect_papers_by_field(field, keywords)
        print(f"æ”¶é›†åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = collector.generate_research_report(field)
        print(f"\n=== {field} ç ”ç©¶æŠ¥å‘Š ===")
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f"{field.replace(' ', '_')}_report.md", 'w', encoding='utf-8') as f:
            f.write(report)
```

## æ€»ç»“

è¿™äº›å®é™…åº”ç”¨æ¡ˆä¾‹å±•ç¤ºäº†Firecrawlåœ¨ä¸åŒåœºæ™¯ä¸‹çš„å¼ºå¤§èƒ½åŠ›ï¼š

### æ ¸å¿ƒä¼˜åŠ¿
1. **æ™ºèƒ½å†…å®¹æå–** - è‡ªåŠ¨è¯†åˆ«å’Œæå–ç»“æ„åŒ–æ•°æ®
2. **å¤šæºæ•°æ®æ•´åˆ** - æ”¯æŒä»å¤šä¸ªç½‘ç«™æ”¶é›†æ•°æ®
3. **å®æ—¶ç›‘æ§** - å®šæœŸæ£€æŸ¥å†…å®¹å˜åŒ–
4. **æ•°æ®è´¨é‡ä¿è¯** - å†…ç½®å»é‡å’ŒéªŒè¯æœºåˆ¶
5. **æ˜“äºé›†æˆ** - ç®€å•çš„APIæ¥å£ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

### é€‚ç”¨åœºæ™¯
- **å†…å®¹èšåˆå¹³å°** - æ–°é—»ã€åšå®¢ã€è®ºå›å†…å®¹æ”¶é›†
- **ä»·æ ¼ç›‘æ§ç³»ç»Ÿ** - ç”µå•†ã€æˆ¿äº§ã€è‚¡ç¥¨ä»·æ ¼è·Ÿè¸ª
- **ç«å“åˆ†æ** - ç«äº‰å¯¹æ‰‹ä¿¡æ¯æ”¶é›†å’Œåˆ†æ
- **å­¦æœ¯ç ”ç©¶** - è®ºæ–‡ã€ä¸“åˆ©ã€æŠ€æœ¯æ–‡æ¡£æ”¶é›†
- **å¸‚åœºè°ƒç ”** - è¡Œä¸šæŠ¥å‘Šã€ç”¨æˆ·è¯„è®ºã€è¶‹åŠ¿åˆ†æ

### æœ€ä½³å®è·µ
1. **åˆç†è®¾ç½®çˆ¬å–é¢‘ç‡** - é¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
2. **å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶** - æé«˜ç³»ç»Ÿç¨³å®šæ€§
3. **æ•°æ®å»é‡å’ŒéªŒè¯** - ç¡®ä¿æ•°æ®è´¨é‡
4. **å®šæœŸå¤‡ä»½æ•°æ®** - é˜²æ­¢æ•°æ®ä¸¢å¤±
5. **ç›‘æ§APIé…é¢ä½¿ç”¨** - åˆç†æ§åˆ¶æˆæœ¬

é€šè¿‡è¿™äº›å®é™…æ¡ˆä¾‹ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿäº†è§£å¦‚ä½•åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­åº”ç”¨Firecrawlï¼Œæ„å»ºå¼ºå¤§çš„æ•°æ®é‡‡é›†å’Œåˆ†æç³»ç»Ÿã€‚