[
  {
    "content": "# OpenAI Agents Quickstart\n\n## Quickstart\n\n### Create a Project and Virtual Environment\n\nYou only need to do this once.\n\n```bash\nmkdir my_project\ncd my_project\npython -m venv .venv\n```\n\n### Activate the Virtual Environment\n\nRemember to do this every time you start a new terminal session.\n\n```bash\nsource .venv/bin/activate\n```\n\n### Install the Agents SDK\n\nTo install the SDK, use the following command:\n\n```bash\npip install openai-agents\n```\n\n### Set Your OpenAI API Key\n\nIf you do not have an API key, follow the relevant instructions to create one. Once you have it, set the key with the following command:\n\n```bash\nexport OPENAI_API_KEY=sk-...\n```\n\n## Create Your First Agent\n\nAgents are defined by their instructions, a name, and optional configuration settings, such as `model_config`.\n\n```python\nfrom agents import Agent\n```",
    "chunk_id": "5b178182-381c-4882-a3b1-a9387a8809dd"
  },
  {
    "content": "```markdown\n## Set Up Your API Key\n\nFirst, set your OpenAI API key in your environment:\n\n```md-code__content\nexport OPENAI_API_KEY=sk-...\n```\n\n## Create Your First Agent\n\nAgents are characterized by their name, instructions, and optional configurations such as `model_config`. Below is an example of creating a math tutoring agent.\n\n```md-code__content\nfrom agents import Agent\n\nagent = Agent(\n    name=\"Math Tutor\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples.\",\n)\n```\n\n## Add More Agents\n\nYou can define additional agents in the same way. The `handoff_descriptions` parameter offers extra context that can assist with routing between agents.\n\n```md-code__content\nfrom agents import Agent\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions.\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n```",
    "chunk_id": "91367679-84be-4477-97d5-3310cb876981"
  },
  {
    "content": "The following is an improved version of the provided text:\n\n```python\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"Provide assistance with historical queries by clearly explaining important events and their context.\",\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"Help users with math problems by explaining your reasoning at each step and providing examples.\",\n)\n\n```\n\n## Define Your Handoffs\n\nEach agent can be equipped with a set of outgoing handoff options to determine how to progress with the user's request.\n\n```python\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"Determine which agent to use based on the user's homework question.\",\n    handoffs=[history_tutor_agent, math_tutor_agent]\n)\n\n```\n\n## Run the Agent Orchestration\n\nLet's ensure that the workflow executes correctly and that the triage agent successfully routes the inquiries to the appropriate specialist agents.",
    "chunk_id": "0982b579-416a-4ea6-98dd-10f37ab53b43"
  },
  {
    "content": "## Run the Agent Orchestration\n\nLet's verify that the workflow is functioning properly and that the triage agent successfully routes the query to the appropriate specialist agents.\n\n```python\nfrom agents import Runner\n\nasync def main():\n    result = await Runner.run(triage_agent, \"What is the capital of France?\")\n    print(result.final_output)\n```\n\n## Add a Guardrail\n\nYou can define custom guardrails to be applied to the input or output.\n\n```python\nfrom agents import GuardrailFunctionOutput, Agent, Runner\nfrom pydantic import BaseModel\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail Check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n```",
    "chunk_id": "727fdc7c-a4c2-4fa6-b89d-ac3d61fb6f71"
  },
  {
    "content": "```python\nfrom agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\nfrom pydantic import BaseModel\nimport asyncio\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\n## Put it all together\n\nLet's combine everything and execute the entire workflow, incorporating handoffs and the input guardrail.\n```",
    "chunk_id": "32eb4ee9-4be5-43dc-b98c-059223a70767"
  },
  {
    "content": "```md-code__content\nfrom agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner\nfrom pydantic import BaseModel\nimport asyncio\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples.\",\n)\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n```",
    "chunk_id": "6e4a89b7-b882-404d-b4c8-ce5bcc0f5361"
  },
  {
    "content": "```python\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"Provide assistance with historical queries by clearly explaining important events and their context.\"\n)\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n    final_output = result.final_output_as(HomeworkOutput)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"Determine which agent to use based on the user's homework question.\",\n    handoffs=[history_tutor_agent, math_tutor_agent],\n    input_guardrails=[\n        InputGuardrail(guardrail_function=homework_guardrail),\n    ],\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, \"Who was the first president of the United States?\")\n    print(result.final_output)\n```",
    "chunk_id": "b8d65500-8e36-4bc1-aba9-f223b8c27db5"
  },
  {
    "content": "```python\nasync def main():\n    result = await Runner.run(triage_agent, \"who was the first president of the united states?\")\n    print(result.final_output)\n\n    result = await Runner.run(triage_agent, \"what is life\")\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## View your traces\n\nTo review what happened during your agent run, navigate to the Trace viewer in the OpenAI Dashboard to see the traces of your agent runs.\n\n## Next steps\n\nLearn how to build more complex agentic flows:\n\n- Learn about how to configure Agents.\n- Learn about running agents.\n- Discover tools, guardrails, and models.",
    "chunk_id": "441e46bf-c006-42b4-bb1d-dc13c097e42e"
  },
  {
    "content": "## Understanding OpenAI Agents\n\n# Agents\n\nAgents serve as the core building block of your applications. An agent is essentially a large language model (LLM) that is configured with specific instructions and tools.\n\n## Basic Configuration\n\nThe most common properties you will configure for an agent include:\n\n- **Instructions**: Also referred to as a developer message or system prompt.\n- **Model**: The specific LLM to be used, along with optional model settings that allow for tuning parameters like temperature and top_p.\n- **Tools**: The tools that the agent can utilize to accomplish its tasks.\n\n```python\nfrom agents import Agent, ModelSettings, function_tool\n\n@function_tool\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"o3-mini\",\n    tools=[get_weather],\n)\n```",
    "chunk_id": "abb34043-6712-45f5-b47c-d22224d90ce7"
  },
  {
    "content": "```python\n@function_tool\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"o3-mini\",\n    tools=[get_weather],\n)\n\n## Context\n\nAgents rely on a generic `context` type, which serves as a dependency-injection tool. This object is created and passed to `Runner.run()`, where it provides necessary dependencies and state for each agent, tool, and handoff involved in the run. You can utilize any Python object as the context.\n\n```python\n@dataclass\nclass UserContext:\n  uid: str\n  is_pro_user: bool\n\n  async def fetch_purchases() -> list[Purchase]:\n     return ...\n  \nagent = Agent[UserContext](\n    ...,\n)\n``` \n\n## Output Types\n\nIf the text includes code blocks, it should remain unchanged. Enclose it in triple backticks and specify the programming language. Do not fabricate facts or hallucinate. Your task is to enhance the coherence and clarity of the text. If the text contains URLs or hyperlinks, remove them while preserving their descriptive text.\n```",
    "chunk_id": "e35bcaf3-6d75-46d0-ab08-86975252cdea"
  },
  {
    "content": "```md-code__content\n@dataclass\nclass UserContext:\n    uid: str\n    is_pro_user: bool\n\n    async def fetch_purchases(self) -> list[Purchase]:\n        return ...\n\nagent = Agent[UserContext](\n    ...,\n)\n\n```\n\n## Output Types\n\nBy default, agents produce outputs in plain text format (i.e., `str`). If you want the agent to generate a specific type of output, you can specify the `output_type` parameter. A common choice is to use Pydantic objects; however, we also support any type that can be wrapped in a Pydantic TypeAdapter, including dataclasses, lists, and TypedDicts.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom agents import Agent\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n\n```\n\nNote\n\nIf the text contains code blocks, do not change the code. Instead, wrap it in triple backticks along with the corresponding programming language. Avoid creating false information or hallucinations. Your sole responsibility is to enhance the text\u2019s coherence and clarity. \n\nIf the text contains URLs or hyperlinks, remove them while preserving their descriptive text. For example: This is a link. \n",
    "chunk_id": "8744c6e6-3c16-4458-99bb-19cfe351f74a"
  },
  {
    "content": "```python\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n\n```\n\n**Note:**  \nWhen you specify an `output_type`, it informs the model to utilize structured outputs instead of regular plain text responses.\n\n## Handoffs  \nHandoffs are sub-agents to which the main agent can delegate tasks. By providing a list of handoffs, the agent can choose to delegate tasks to them as needed. This setup enables the orchestration of modular, specialized agents that excel at specific tasks. For more information, refer to the handoffs documentation.\n\n```python\nfrom agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n```\n\nIf the text contains code blocks, do not alter the code. Instead, wrap it in triple backticks along with the appropriate programming language designation. Avoid fabricating facts or hallucinating; your task is solely to enhance the coherence and clarity of the text. \n\nIf the text includes URLs or hyperlinks, remove them while preserving the hyperlink text. For example: [This is a link] becomes This is a link.",
    "chunk_id": "596c3469-e70b-48e6-b29c-21cb95c59ef6"
  },
  {
    "content": "from agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=(\n        \"Assist the user with their questions. \"\n        \"If they inquire about bookings, transfer them to the booking agent. \"\n        \"If they ask about refunds, direct them to the refund agent.\"\n    ),\n    handoffs=[booking_agent, refund_agent],\n)\n\n## Dynamic Instructions\n\nTypically, you can provide static instructions when creating the agent. However, it is also possible to supply dynamic instructions using a function. This function will receive both the agent and the context, and must return a prompt. Both regular and asynchronous functions are accepted.\n\n```md-code__content\ndef dynamic_instructions(\n    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n) -> str:\n    return f\"The user's name is {context.context.name}. Please assist them with their questions.\"\n\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n```",
    "chunk_id": "7929ab74-7566-4dc8-978b-56ee120fa401"
  },
  {
    "content": "```python\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n\n## Lifecycle Events (Hooks)\n\nThere are times when you may want to observe the lifecycle of an agent. For instance, you might want to log events or pre-fetch data when certain events occur. You can achieve this by using the `hooks` property. To do so, subclass the AgentHooks class and override the methods that you're interested in.\n\n## Guardrails\n\nGuardrails enable you to run checks and validations on user input while the agent is running. For example, you could filter the user\u2019s input for relevance. For more details, refer to the guardrails documentation.\n\n## Cloning/COPYING Agents\n\nYou can duplicate an agent using the `clone()` method, which allows you to create a copy of an Agent and optionally modify any properties as needed.\n```",
    "chunk_id": "75cfffbf-7d22-4077-aa9a-1bddd3bd86b6"
  },
  {
    "content": "## Cloning/Coping Agents\n\nYou can duplicate an agent by using the `clone()` method, allowing you to optionally modify any properties as needed.\n\n```python\npirate_agent = Agent(\n    name=\"Pirate\",\n    instructions=\"Write like a pirate\",\n    model=\"o3-mini\",\n)\n\nrobot_agent = pirate_agent.clone(\n    name=\"Robot\",\n    instructions=\"Write like a robot\",\n)\n```\n\n## Guardrails for Agents\n\nGuardrails operate in parallel with your agents, enabling checks and validations of user input. For instance, if you have an agent that utilizes a sophisticated (and thus slow/expensive) model to assist with customer requests, you wouldn\u2019t want malicious users to exploit this by asking the model for help with their math homework. To prevent this, you can implement a guardrail using a faster, more economical model. If the guardrail detects misuse, it can promptly raise an error, thereby preventing the expensive model from running and saving you both time and money.",
    "chunk_id": "61cbe1ce-a580-4670-88f3-1d71e99308e2"
  },
  {
    "content": "There are two types of guardrails:\n\n1. Input guardrails, which operate on the initial user input.\n2. Output guardrails, which function on the final agent output.\n\n## Input guardrails\n\nInput guardrails follow three steps:\n\n1. If the text contains code blocks, do not modify the code. Instead, wrap it in triple backticks along with the relevant programming language. \n2. Do not fabricate facts or hallucinate. Your sole purpose is to enhance the coherence and clarity of the text.\n3. If the text includes URLs or hyperlinks, remove them while keeping their descriptive text intact. For example: [This is a link](https://www.example.com) would be transformed to: This is a link.",
    "chunk_id": "fd980ce3-c8ce-45c7-9610-e07b487868f8"
  },
  {
    "content": "1. First, the guardrail receives the same input that is passed to the agent.  \n2. Next, the guardrail function is executed to produce a `GuardrailFunctionOutput`, which is then wrapped in an `InputGuardrailResult`.  \n3. Finally, we check if `.tripwire_triggered` is true. If it is, an `InputGuardrailTripwireTriggered` exception is raised, allowing you to respond appropriately to the user or handle the exception.  ",
    "chunk_id": "0980785b-29e4-4326-9ea8-c3aa26f48f2f"
  },
  {
    "content": "Note\n\nInput guardrails are designed to operate on user input, meaning that an agent's guardrails activate only when the agent is the _first_ agent. You may wonder why the `guardrails` property is associated with the agent instead of being passed to `Runner.run`. This design choice reflects the fact that guardrails are typically related to the specific agent; different agents require different guardrails, making it helpful for readability to colocate the code.\n\n## Output guardrails\n\nOutput guardrails follow three steps:\n\n1. If the text includes code blocks, do not alter the code. Instead, wrap it in triple backticks and specify the programming language.\n2. Do not fabricate information or hallucinate. Your task is solely to enhance the coherence and clarity of the text.\n3. If the text contains URLs or hyperlinks, remove them while keeping the text intact. For example:  \n   [This is a link] -> This is a link",
    "chunk_id": "30ca8dfa-df52-42a2-9a56-0cc4537c5325"
  },
  {
    "content": "1. First, the guardrail receives the same input that is passed to the agent.  \n2. Next, the guardrail function runs and produces a `GuardrailFunctionOutput`, which is then wrapped in an `OutputGuardrailResult`.",
    "chunk_id": "9f9c8cff-987f-4ea0-9589-b80eaa50668e"
  },
  {
    "content": "3. Finally, we check if `.tripwire_triggered` is true. If it is, an `OutputGuardrailTripwireTriggered` exception is raised. This allows you to respond appropriately to the user or handle the exception effectively.",
    "chunk_id": "ecd4b6f6-11c9-4fcf-8f86-fa27881dd0de"
  },
  {
    "content": "Note\n\nOutput guardrails are designed to operate on the final agent input, meaning that an agent's guardrails will only activate if it is the last agent in the sequence. This is similar to input guardrails; we implement this approach because guardrails are typically tailored to specific agents. Consequently, it is beneficial for readability to colocate the code.\n\n## Tripwires\n\nIf either the input or output violates the guardrail, the system can indicate this with a tripwire. When a guardrail triggers a tripwire, we promptly raise a `{Input,Output}GuardrailTripwireTriggered` exception and halt the agent's execution.\n\n## Implementing a Guardrail\n\nTo implement a guardrail, you need to provide a function that takes input and returns a `GuardrailFunctionOutput`. In this example, we will achieve this by executing an Agent in the background.",
    "chunk_id": "24306494-588f-4e9d-9b43-5104c0a37201"
  },
  {
    "content": "The following code defines a guardrail agent designed to evaluate user input for math homework requests. \n\n```python\nfrom pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    InputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    TResponseInputItem,\n    input_guardrail,\n)\n\nclass MathHomeworkOutput(BaseModel):\n    is_math_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking you to do their math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n@input_guardrail\nasync def math_guardrail(\n    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n) -> GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math_homework,\n    )\n``` \n\nThis code utilizes Pydantic models to structure the output of the guardrail agent, which identifies whether the user's request pertains to math homework. The asynchronous function `math_guardrail` leverages the `Runner` to process the input and retrieves the relevant output information for further assessment.",
    "chunk_id": "70d33b35-6dbd-4857-9b77-cae352280919"
  },
  {
    "content": "```python\nreturn GuardrailFunctionOutput(\n    output_info=result.final_output,\n    tripwire_triggered=result.final_output.is_math_homework,\n)\n\nagent = Agent(\n    name=\"Customer Support Agent\",\n    instructions=\"You are a customer support agent. Your role is to assist customers with their inquiries.\",\n    input_guardrails=[math_guardrail],\n)\n\nasync def main():\n    # This should trigger the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"No guardrail was triggered - this is unexpected\")\n\n    except InputGuardrailTripwireTriggered:\n        print(\"Math homework guardrail was triggered\")\n\n```\n\n```python\nfrom pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    OutputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    output_guardrail,\n)\n\nclass MessageOutput(BaseModel):\n    response: str\n\nclass MathOutput(BaseModel):\n    reasoning: str\n    is_math: bool\n```",
    "chunk_id": "e1227822-185d-4fb2-9924-33dc7e9ae17d"
  },
  {
    "content": "```python\nclass MathOutput(BaseModel):\n    reasoning: str\n    is_math: bool\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the output includes any math.\",\n    output_type=MathOutput,\n)\n\n@output_guardrail\nasync def math_guardrail(\n    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n) -> GuardrailFunctionOutput:\n    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)\n\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math,\n    )\n\nagent = Agent(\n    name=\"Customer support agent\",\n    instructions=\"You are a customer support agent. You help customers with their questions.\",\n    output_guardrails=[math_guardrail],\n    output_type=MessageOutput,\n)\n\nasync def main():\n    # This should trip the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"Guardrail didn't trip - this is unexpected\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```",
    "chunk_id": "faec0dc4-0da9-4e64-9075-355ea41a8fcd"
  },
  {
    "content": "```python\nasync def main():\n    # This should trigger the guardrail\n    try:\n        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n        print(\"The guardrail didn't trip - this is unexpected\")\n\n    except OutputGuardrailTripwireTriggered:\n        print(\"Math output guardrail tripped\")\n\n```\n\n## OpenAI Agents Tracing\n\n# Tracing\n\nThe Agents SDK includes built-in tracing, which collects a comprehensive record of events during an agent run, including LLM generations, tool calls, handoffs, guardrails, and even custom events. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during both development and production.\n\nNote\n\nTracing is enabled by default. There are two ways to disable tracing:",
    "chunk_id": "0230c0c8-59c2-4e3f-a818-c39ad6243bba"
  },
  {
    "content": "Note\n\nTracing is enabled by default. There are two methods to disable tracing:\n\n1. You can globally disable tracing by setting the environment variable `OPENAI_AGENTS_DISABLE_TRACING=1`.\n2. You can also disable tracing for a specific run by setting `agents.run.RunConfig.tracing_disabled` to `True`.\n\n**For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.**\n\n## Traces and spans",
    "chunk_id": "b67089d9-8733-4e30-a9a8-169b0b6b1830"
  },
  {
    "content": "- **Traces** represent a complete end-to-end operation within a \"workflow.\" Each trace is composed of multiple spans and possesses the following properties:\n  - `workflow_name`: This identifies the logical workflow or application. Examples include \"Code generation\" or \"Customer service.\"\n  - `trace_id`: A unique identifier for the trace, which is automatically generated if not provided. It must follow the format `trace_<32_alphanumeric>`.\n  - `group_id`: An optional identifier to link multiple traces from the same conversation, such as a chat thread ID.\n  - `disabled`: A boolean value that, if set to True, prevents the trace from being recorded.\n  - `metadata`: Optional additional information pertaining to the trace.\n\n- **Spans** represent specific operations, each with a defined start and end time. Spans include the following attributes:\n  - `started_at` and `ended_at`: Timestamps marking the beginning and end of the operation.\n  - `trace_id`: Indicates the trace to which the span belongs.\n  - `parent_id`: References the parent span of this span, if applicable.",
    "chunk_id": "21742194-ee70-480b-b5b5-41b36cf3ed3c"
  },
  {
    "content": "**Spans** represent operations that have a distinct start and end time. Each Span includes the following components:\n\n- `started_at` and `ended_at` timestamps, which indicate when the Span begins and ends.\n- `trace_id`, which identifies the trace to which the Span belongs.\n- `parent_id`, which references the parent Span of this Span, if applicable.\n- `span_data`, which contains specific information about the Span. For instance, `AgentSpanData` includes details about the Agent, while `GenerationSpanData` pertains to information regarding the LLM generation, among others.",
    "chunk_id": "d568690d-6a55-4d2a-899b-5299c9adfebe"
  },
  {
    "content": "## Default Tracing\n\nBy default, the SDK traces the following:\n\n- The entire `Runner.{run, run_sync, run_streamed}()` is wrapped in a `trace()`.\n- Each time an agent runs, it is wrapped in `agent_span()`.\n- LLM generations are wrapped in `generation_span()`.\n- Function tool calls are each wrapped in `function_span()`.\n- Guardrails are wrapped in `guardrail_span()`.\n- Handoffs are wrapped in `handoff_span()`.\n\nThe default trace is named \"Agent trace.\" You can customize this name by using `trace`, or you can configure it along with other properties using the RunConfig.\n\nAdditionally, you can set up custom trace processors to push traces to other destinations, either as a replacement for or a secondary destination to the default trace.\n\n## Higher Level Traces",
    "chunk_id": "8c907739-6909-48c6-af9b-6c91d3d0f1ee"
  },
  {
    "content": "In addition, you can configure custom trace processors to direct traces to alternative destinations, either as a primary or secondary option.\n\n## Higher Level Traces\n\nAt times, you may wish to group multiple calls to `run()` into a single trace. This can be accomplished by encapsulating the entire code block within a `trace()`.\n\n```python\nfrom agents import Agent, Runner, trace\n\nasync def main():\n    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n\n    with trace(\"Joke workflow\"):\n        first_result = await Runner.run(agent, \"Tell me a joke\")\n        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n        print(f\"Joke: {first_result.final_output}\")\n        print(f\"Rating: {second_result.final_output}\")\n```\n\n## Creating Traces",
    "chunk_id": "a7efa82c-5159-4601-a52d-2c1d53cf4a4a"
  },
  {
    "content": "## Creating Traces\n\nYou can use the `trace()` function to create a trace. Traces must be started and finished properly. There are two options for doing this:\n\n1. **Recommended**: Use the trace as a context manager, such as `with trace(...) as my_trace`. This approach automatically handles starting and ending the trace at the appropriate times.\n   \n2. Alternatively, you can manually control the tracing process by calling `trace.start()` to begin the trace and `trace.finish()` to end it.",
    "chunk_id": "03fe7850-6660-47ba-8107-99981f6a4519"
  },
  {
    "content": "The current trace is tracked using a Python `contextvar`, which automatically manages concurrency. When you manually start or end a trace, it's important to pass `mark_as_current` and `reset_current` to the `start()` and `finish()` functions in order to update the current trace.\n\n## Creating spans\n\nYou can utilize the various `*_span()` methods to create a span. Typically, it is unnecessary to manually create spans, as the `custom_span()` function is available for tracking custom span information.\n\nSpans are automatically integrated into the current trace and are nested under the nearest current span, which is also tracked using a Python `contextvar`.\n\n## Sensitive data",
    "chunk_id": "99320c99-acf8-4425-90a5-00dec26a2708"
  },
  {
    "content": "Spans are automatically included in the current trace and are organized under the nearest active span, which is managed using a Python context variable.\n\n## Sensitive Data\n\nCertain spans may track potentially sensitive information. For instance, the `generation_span()` records the inputs and outputs of model generations, while the `function_span()` captures the inputs and outputs of function calls. Because these spans may contain sensitive data, you can disable the capturing of this information via the `RunConfig.trace_include_sensitive_data` configuration option.\n\n## Custom Tracing Processors\n\nThe high-level architecture for tracing is as follows:\n\n1. If the text contains code blocks, retain the code by wrapping it in triple backticks along with the appropriate programming language.\n2. Avoid fabricating information or introducing inaccuracies; focus solely on enhancing the text's clarity and coherence.\n\n3. If the text includes URLs or hyperlinks, remove them while keeping their descriptive text intact. For example, \"This is a link\" should replace \"[This is a link](https://www.example.com).\"",
    "chunk_id": "7fc72110-31c3-40ff-a87e-b897e06e1a50"
  },
  {
    "content": "## Custom Tracing Processors\n\nThe high-level architecture for tracing is as follows:\n\n- During initialization, we create a global `TraceProvider`, which is responsible for generating traces.\n- The `TraceProvider` is configured with a `BatchTraceProcessor` that sends traces and spans in batches to a `BackendSpanExporter`. This component exports the spans and traces to the OpenAI backend in batches.\n\nTo customize this default setup\u2014whether to send traces to alternative or additional backends or to modify exporter behavior\u2014you have two options:",
    "chunk_id": "d52d28d1-9b3b-424c-8aed-d795be35b03d"
  },
  {
    "content": "To customize the default setup for sending traces to alternative or additional backends, or to modify exporter behavior, you have two options:\n\n1. Use `add_trace_processor()` to add an **additional** trace processor. This processor will receive traces and spans as they become available, allowing you to perform your own processing in addition to sending traces to OpenAI's backend.\n\n2. Utilize `set_trace_processors()` to **replace** the default processors with your own trace processors. When you do this, traces will only be sent to the OpenAI backend if you include a `TracingProcessor` that facilitates this.\n\n## External tracing processors list",
    "chunk_id": "e54d9d0e-67d9-4933-ae58-34b4260a5df9"
  },
  {
    "content": "## External Tracing Processors List\n\n- Arize-Phoenix\n- MLflow\n- Braintrust\n- Pydantic Logfire\n- AgentOps\n- Scorecard\n- Keywords AI\n- LangSmith\n- Maxim AI\n\n## Running Agents\n[Skip to content](https://openai.github.io/openai-agents-python/running_agents/#running-agents)\n\n# Running Agents\n\nIf the text contains code blocks, don't change the code. Wrap it in triple backticks along with the programming language. Don't make up facts or hallucinate. Your job is to make the text more coherent and easier to understand only.\n\nIf the text contains URLs or hyperlinks, remove them while preserving their text. For example: This is a link.",
    "chunk_id": "813212f5-a59e-40c6-aa65-91ad756b736f"
  },
  {
    "content": "## Running Agents\n\n# Running Agents\n\nYou can run agents using the Runner class. There are three options available to you:",
    "chunk_id": "aef8406f-a805-412d-8144-4c0856e4801b"
  },
  {
    "content": "1. `Runner.run()`, an asynchronous method, returns a `RunResult`.\n\n2. `Runner.run_sync()` is a synchronous method that executes `Runner.run()` in the background.\n\n3. `Runner.run_streamed()` is an asynchronous method that delivers a `RunResultStreaming`. It invokes the LLM in streaming mode and provides you with events as they are received.",
    "chunk_id": "f45ae3d7-8ce8-405e-aa55-2379a75ea879"
  },
  {
    "content": "```md-code__content\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n\n    result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")\n    print(result.final_output)\n    # Code within the code,\n    # Functions calling themselves,\n    # The dance of infinite loops.\n\nThe agent loop\n\nWhen you use the run method in `Runner`, you pass in a starting agent and input. The input can either be a string (considered a user message) or a list of input items, which represent the items in the OpenAI Responses API.\n\nThe runner executes a loop:\n    \n    If the text contains code blocks, do not alter the code. Wrap it in triple backticks along with the corresponding programming language. Avoid fabricating facts or hallucinations; your role is solely to enhance the coherence and clarity of the text.\n    \n    If the text includes URLs or hyperlinks, remove them while retaining their descriptive text. For example: \n    [This is a link] -> This is a link\n```",
    "chunk_id": "8a0b64a3-c417-453e-bbed-755fdfb95f65"
  },
  {
    "content": "The runner executes a loop with the following steps:\n\n1. The runner calls the LLM (Large Language Model) for the current agent using the current input.\n2. The LLM generates its output.\n3. If the LLM returns a `final_output`, the loop concludes, and we return the result.\n4. If the LLM performs a handoff, we update the current agent and input, then rerun the loop.\n5. If the LLM produces tool calls, we execute those tool calls, append the results, and rerun the loop.\n6. If we exceed the `max_turns` limit provided, we raise a `MaxTurnsExceeded` exception.\n\n### Note\n\nAn output from the LLM is considered a \"final output\" if it generates text of the desired type and does not include any tool calls.",
    "chunk_id": "203e6579-2ed9-4d3c-a56a-44d6132bced5"
  },
  {
    "content": "Note\n\nThe criterion for determining whether the LLM output is deemed a \"final output\" is based on the production of text that aligns with the desired type, and the absence of tool calls.\n\n## Streaming\n\nStreaming allows you to receive events in real-time as the LLM operates. Once the streaming session is complete, the `RunResultStreaming` will contain comprehensive information about the session, including all the new outputs generated. You can retrieve the streaming events by calling `.stream_events()`. For more details, refer to the streaming guide.\n\n## Run Config\n\nThe `run_config` parameter enables you to set global configurations for the agent run:\n\nIf the text includes code blocks, retain the code as is, wrapping it in triple backticks along with the appropriate programming language. Avoid fabricating information or hallucinating; your sole responsibility is to enhance the coherence and clarity of the text.\n\nIf the text features URLs or hyperlinks, remove them while keeping their corresponding text intact. For instance: This is a link.",
    "chunk_id": "10e1682b-1208-42ec-881e-e2868b91ebf3"
  },
  {
    "content": "- `model`: This attribute allows the setting of a global LLM model to be used, regardless of the specific `model` assigned to each Agent.\n- `model_provider`: This attribute serves as a model provider for looking up model names, with a default value of OpenAI.\n- `model_settings`: This attribute can override agent-specific settings. For instance, you can define a global `temperature` or `top_p`.",
    "chunk_id": "2940bf8d-1497-4aed-8f8e-fba90088bb30"
  },
  {
    "content": "- `input_guardrails` and `output_guardrails`: These are lists of input or output guardrails that should be included in all runs.",
    "chunk_id": "8a20a276-3208-409c-a1df-8ed0538d4a24"
  },
  {
    "content": "- `handoff_input_filter`: This is a global input filter applied to all handoffs that do not already have a specific one. The input filter allows you to modify the inputs sent to the new agent. For more details, refer to the documentation on Handoff.input_filter.\n\n- `tracing_disabled`: This option enables you to disable tracing for the entire run.",
    "chunk_id": "1ec653de-9a62-4a87-afd9-ee7f17d231f7"
  },
  {
    "content": "`trace_include_sensitive_data`: This attribute configures whether traces will include potentially sensitive data, such as inputs and outputs from the LLM and tool calls.",
    "chunk_id": "b8b21c0d-9da8-48bb-a418-7a55b233d48d"
  },
  {
    "content": "- `workflow_name`, `trace_id`, and `group_id`: These attributes set the tracing workflow name, trace ID, and trace group ID for the run. It is advisable to set at least the `workflow_name`. The session ID is an optional field that allows you to link traces across multiple runs.\n  \n- `trace_metadata`: This attribute is used to include metadata on all traces.",
    "chunk_id": "5cbd123e-c976-487c-974b-ebb0bdb212f0"
  },
  {
    "content": "## Conversations/Chat Threads\n\nCalling any of the run methods can result in one or more agents executing tasks, which may lead to one or more calls to the language model. However, this process represents a single logical turn in a chat conversation. For example:\n\n1. **User Turn:** The user enters text.\n2. **Runner Run:** The first agent calls the language model, runs various tools, hands off tasks to a second agent, which then runs additional tools and produces an output.\n\nAt the conclusion of the agent's run, you can choose what to display to the user. For instance, you might show the user every new item generated by the agents or just the final output. After receiving this, the user may ask a follow-up question, prompting you to call the run method again.\n\nTo prepare for the next turn, you can utilize the base `RunResultBase.to_input_list()` method to gather the necessary inputs.",
    "chunk_id": "bbaf1a3a-6f1f-47e5-997f-c0a362b6b454"
  },
  {
    "content": "```md-code__content\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n        print(result.final_output)  # San Francisco\n\n        # Second turn\n        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n        result = await Runner.run(agent, new_input)\n        print(result.final_output)  # California\n```\n\n## Exceptions\n\nThe SDK raises exceptions in certain cases. The full list is in agents.exceptions. As an overview:\n\nIf the text contains code blocks, don't change the code. Wrap it in triple backticks along with the programming language. Don't make up facts or hallucinate. Your job is to make the text more coherent and easier to understand only. \n\nIf the text contains URLs or hyperlinks, remove them while preserving their text. For example: This is a link.",
    "chunk_id": "029ff982-4696-420f-8cd2-2ca6b9508888"
  },
  {
    "content": "- `AgentsException` is the base class for all exceptions raised in the SDK.\n- `MaxTurnsExceeded` is raised when the run exceeds the `max_turns` specified in the run methods.\n- `ModelBehaviorError` occurs when the model produces invalid outputs, such as malformed JSON or attempts to use non-existent tools.\n- `UserError` is raised when you, as the developer using the SDK, make an error.",
    "chunk_id": "5278b631-f18d-48bf-a4f9-2b94c4ffefb3"
  },
  {
    "content": "- `UserError` is raised when you, as the person writing code using the SDK, make an error with the SDK.\n- `InputGuardrailTripwireTriggered` and `OutputGuardrailTripwireTriggered` are raised when a guardrail is tripped.",
    "chunk_id": "e727a5bc-9496-4575-a5a7-2c20fc02567a"
  },
  {
    "content": "## Agent Task Handoffs\n\n# Handoffs\n\nHandoffs enable an agent to delegate tasks to another agent, which is particularly useful in scenarios where agents have specialized skills in distinct areas. For instance, in a customer support application, different agents might handle specific tasks such as order status, refunds, and frequently asked questions.\n\nIn this context, handoffs are represented as tools within the LLM. For example, if there is a handoff to an agent designated as the `Refund Agent`, the corresponding tool would be named `transfer_to_refund_agent`.\n\n## Creating a Handoff\n\nAll agents possess a `handoffs` parameter, which can either accept an `Agent` directly or a `Handoff` object that allows for customization of the handoff process.",
    "chunk_id": "7fcb95af-7d98-4fe2-9dc9-1497170fceb3"
  },
  {
    "content": "You can create a handoff using the `handoff()` function provided by the Agents SDK. This function allows you to specify the agent to whom the handoff will be made, as well as optional overrides and input filters.\n\n### Basic Usage\n\nHere\u2019s how you can create a simple handoff:\n\n```python\nfrom agents import Agent, handoff\n\nbilling_agent = Agent(name=\"Billing agent\")\nrefund_agent = Agent(name=\"Refund agent\")\n\ntriage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n```\n\n### Customizing Handoffs with the `handoff()` Function\n\nThe `handoff()` function offers customization options for your handoffs.",
    "chunk_id": "6e7119a3-2906-4460-9407-95ffd1a9dcf4"
  },
  {
    "content": "The `handoff()` function allows for customization in various ways:\n\n- `agent`: This parameter specifies the agent to whom tasks will be handed off.\n- `tool_name_override`: By default, the `Handoff.default_tool_name()` function is employed, which resolves to `transfer_to_<agent_name>`. You have the option to override this default.\n- `tool_description_override`: You can customize the default tool description provided by `Handoff.default_tool_description()`.\n- `on_handoff`: This is a callback function that is executed when the handoff occurs. It is useful for initiating processes, such as data fetching, as soon as the handoff is triggered. This function receives the agent context and may optionally receive LLM-generated input, determined by the `input_type` parameter.\n- `input_type`: This specifies the type of input expected during the handoff (optional).\n- `input_filter`: This feature enables filtering of the input received by the next agent; further details are provided below.",
    "chunk_id": "16dfff3c-d256-4dfe-b202-367f9bceb329"
  },
  {
    "content": "```md-code__content\nfrom agents import Agent, handoff, RunContextWrapper\n\ndef on_handoff(ctx: RunContextWrapper[None]):\n    print(\"Handoff called\")\n\nagent = Agent(name=\"My agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    tool_name_override=\"custom_handoff_tool\",\n    tool_description_override=\"Custom description\",\n)\n\n```\n\n## Handoff Inputs\n\nIn certain situations, you may want the LLM to provide specific data when it initiates a handoff. For instance, when handing off to an \"Escalation agent,\" it might be useful to include a reason for the escalation, allowing for better logging of the situation.\n\n```md-code__content\nfrom pydantic import BaseModel\n\nfrom agents import Agent, handoff, RunContextWrapper\n\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation agent called with reason: {input_data.reason}\")\n\nagent = Agent(name=\"Escalation agent\")\n```",
    "chunk_id": "2d74c100-e19d-4bc3-8c00-7b60ffae9314"
  },
  {
    "content": "```python\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation agent called with reason: {input_data.reason}\")\n\nagent = Agent(name=\"Escalation agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    input_type=EscalationData,\n)\n\n## Input Filters\n\nWhen a handoff occurs, the new agent takes over the conversation and can access the entire previous conversation history. If you wish to modify this behavior, you can specify an input filter. An input filter is a function that receives the existing input via a HandoffInputData instance and must return a new HandoffInputData.\n```",
    "chunk_id": "2a89e0c3-bddf-40d2-9761-6685229fcd3c"
  },
  {
    "content": "There are some common patterns, such as removing all tool calls from the history, which are implemented for you in the `agents.extensions.handoff_filters` module.\n\n```python\nfrom agents import Agent, handoff\nfrom agents.extensions import handoff_filters\n\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools,\n)\n```",
    "chunk_id": "e8a052e3-1d29-4f31-bc03-46f66f227ba3"
  },
  {
    "content": "```python\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools,\n)\n\n## Recommended Prompts\n\nTo ensure that LLMs understand handoffs effectively, it is advisable to include relevant information about handoffs in your agents. We suggest using the prefix found in agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX, or you can call agents.extensions.handoff_prompt.prompt_with_handoff_instructions to automatically incorporate recommended data into your prompts.\n```",
    "chunk_id": "c0bb14db-bd96-4ace-ba35-96988297092a"
  },
  {
    "content": "```md-code__content\nfrom agents import Agent\nfrom agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n\nbilling_agent = Agent(\n    name=\"Billing agent\",\n    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n    <Fill in the rest of your prompt here>.\"\"\"\n)\n\n## Context Management Overview\n\n# Context Management\n\nThe term \"context\" can have multiple meanings. Here are two key types of context you may encounter:\n\n1. **Local Context**: This encompasses the data and dependencies required by your code when executing functions, during callbacks such as `on_handoff`, and throughout lifecycle hooks.\n2. **Context for LLMs**: This refers to the information accessible to the language model when it generates a response.\n\n## Local Context\n\nIf the text contains code blocks, maintain the integrity of the code by wrapping it in triple backticks along with the programming language. Avoid altering facts or making assumptions. Your role is solely to enhance the coherence and clarity of the text. \n\nAdditionally, if the text includes URLs or hyperlinks, remove them while retaining their descriptive text. For instance, [This is a link] would be reformatted as This is a link.\n```",
    "chunk_id": "87e9fcc7-6642-41e7-9c5a-1daf847a469d"
  },
  {
    "content": "## Local Context\n\nThis is represented via the `RunContextWrapper` class and the `context` property within it. The process works as follows:\n\n1. Create any Python object you wish to use. A common practice is to utilize a dataclass or a Pydantic object.\n2. Pass that object to the various run methods (e.g., `Runner.run(..., **context=whatever**)`).\n3. All tool calls, lifecycle hooks, etc., will receive a wrapper object, `RunContextWrapper[T]`, where `T` represents the type of your context object, which you can access via `wrapper.context`.\n\nThe most important point to note is that every agent, tool function, lifecycle, etc., for a given agent run must utilize the same type of context.\n\nYou can use the context for various purposes.",
    "chunk_id": "afaec831-9efc-4b00-a2fb-393d08d6c536"
  },
  {
    "content": "The **most important** thing to be aware of is that every agent, tool function, and lifecycle for a given agent run must utilize the same _type_ of context.\n\nYou can use this context for various purposes, including:\n\n- Contextual data for your run (e.g., user-related information such as a username or user ID)\n- Dependencies (e.g., logger objects, data fetchers, etc.)\n- Helper functions\n\n**Note:** The context object is **not** sent to the LLM. It serves purely as a local object that you can read from, write to, and invoke methods on.\n\n```python\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom agents import Agent, RunContextWrapper, Runner, function_tool\n\n@dataclass\nclass UserInfo:\n    name: str\n    uid: int\n\n@function_tool\nasync def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:\n    return f\"User {wrapper.context.name} is 47 years old\"\n\nasync def main():\n    user_info = UserInfo(name=\"John\", uid=123)\n\n    agent = Agent[UserInfo](\n        name=\"Assistant\",\n        tools=[fetch_user_age],\n    )\n```",
    "chunk_id": "7f5d293d-13c8-426c-a329-9b11ee924fcd"
  },
  {
    "content": "```python\nasync def main():\n    user_info = UserInfo(name=\"John\", uid=123)\n\n    agent = Agent[UserInfo](\n        name=\"Assistant\",\n        tools=[fetch_user_age],\n    )\n\n    result = await Runner.run(\n        starting_agent=agent,\n        input=\"What is the age of the user?\",\n        context=user_info,\n    )\n\n    print(result.final_output)  # The user John is 47 years old.\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n``` \n\n## Agent/LLM context\n\nWhen an LLM is called, the only data it can access is from the conversation history. If you want to provide new data for the LLM to utilize, it must be incorporated into that conversation history. There are several methods to achieve this:\n\n- If the text includes code blocks, keep them unchanged and wrap them in triple backticks along with the programming language.\n- Avoid inventing facts or hallucinating.\n- Your role is solely to enhance the coherence and clarity of the text. \n\nIf the text contains URLs or hyperlinks, remove them while maintaining their text. For example: \n- [This is a link](https://www.example.com) becomes This is a link.",
    "chunk_id": "04a7ea38-c693-455e-a0a5-e46beb7fbb53"
  },
  {
    "content": "1. You can add it to the Agent's `instructions`, often referred to as a \"system prompt\" or \"developer message.\" System prompts can be either static strings or dynamic functions that receive context and output a string. This approach is commonly used for information that is always beneficial, such as the user's name or the current date.\n\n2. Alternatively, you can include it in the `input` when calling the `Runner.run` functions. This method is similar to the `instructions` tactic but allows for messages that are lower in the chain of command.\n\n3. Another option is to expose it through function tools. This is particularly useful for on-demand context, as the LLM can decide when it needs certain data and can then call the tool to retrieve it.\n\n4. Finally, consider using retrieval or web search tools. These tools can fetch relevant data from files or databases (retrieval) or from the web (web search). This practice helps to \"ground\" the response in pertinent contextual information.",
    "chunk_id": "eeec6717-7381-4dc7-9926-6a8c9d535445"
  },
  {
    "content": "## OpenAI Agents Models\n\n# Models\n\nThe Agents SDK offers seamless integration with OpenAI models in two main categories:\n\n- **Recommended**: The OpenAIResponsesModel, which utilizes the new Responses API to call OpenAI APIs effectively.\n- The OpenAIChatCompletionsModel, which leverages the Chat Completions API for API calls.\n\n## Mixing and Matching Models",
    "chunk_id": "1224f688-1213-4589-ac3c-727090dfc3c7"
  },
  {
    "content": "## Mixing and Matching Models\n\nWithin a single workflow, you may want to use different models for each agent. For instance, you could employ a smaller, faster model for triage tasks while utilizing a larger, more capable model for more complex tasks. When configuring an Agent, you can select a specific model by:\n\n1. Passing the name of an OpenAI model.\n2. Passing any model name along with a ModelProvider that can map that name to a Model instance.\n3. Directly providing a Model implementation.\n\nNote",
    "chunk_id": "596012e0-e859-423c-9928-f1f46f4096b8"
  },
  {
    "content": "Note\n\nWhile our SDK supports both the OpenAIResponsesModel and the OpenAIChatCompletionsModel shapes, we recommend using a single model shape for each workflow. This is because the two shapes support different sets of features and tools. If your workflow requires using both model shapes, ensure that all the features you intend to use are available in both models.\n\n```python\nfrom agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"o3-mini\",\n)\n```",
    "chunk_id": "930e1a0b-cff4-430f-b390-f3166f1c0f8f"
  },
  {
    "content": "```md-code__content\nfrom agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\n# Define agents for different languages\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"o3-mini\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English.\",\n    model=OpenAIChatCompletionsModel(\n        model=\"gpt-4o\",\n        openai_client=AsyncOpenAI()\n    ),\n)\n\n# Triage agent to direct requests to the appropriate language-specific agent\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    model=\"gpt-3.5-turbo\",\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, \u00bfc\u00f3mo est\u00e1s?\")\n    print(result.final_output)\n\n## Using Other LLM Providers\n\nYou can use other LLM providers in three ways (examples available in the documentation):\n```",
    "chunk_id": "8b0e7ce9-8b86-4a09-b59b-161f9570a343"
  },
  {
    "content": "1. The `set_default_openai_client` function is useful when you want to globally use an instance of `AsyncOpenAI` as the LLM client. This applies in scenarios where the LLM provider has an OpenAI compatible API endpoint, allowing you to set the `base_url` and `api_key`. You can find a configurable example in the examples folder.\n\n2. The `ModelProvider` operates at the `Runner.run` level, enabling you to specify a custom model provider for all agents within that run. A configurable example is available in the examples folder.",
    "chunk_id": "e6d9c4d3-bb91-4164-941e-a5c67ed93148"
  },
  {
    "content": "3. `Agent.model` allows you to specify the model for a specific Agent instance. This functionality enables you to mix and match different providers for various agents. An example of a configurable implementation can be found in the custom example agent script.",
    "chunk_id": "402aa400-6070-4d6a-8eeb-e509c5cf4dfd"
  },
  {
    "content": "In situations where you do not possess an API key from platform.openai.com, we recommend disabling tracing by using `set_tracing_disabled()`, or alternatively, setting up a different tracing processor.\n\nNote: In these examples, we utilize the Chat Completions API/model, as most LLM providers do not yet support the Responses API. If your LLM provider does support it, we advise using Responses.\n\n## Common Issues with Other LLM Providers\n\n### Tracing Client Error 401\n\nIf you encounter errors related to tracing, it is likely because traces are being uploaded to OpenAI servers without an OpenAI API key. To resolve this issue, you have three options:",
    "chunk_id": "81e6161a-871e-4a0a-9425-175522adb121"
  },
  {
    "content": "## Common Issues with Using Other LLM Providers\n\n### Tracing Client Error 401\n\nIf you encounter errors related to tracing, it is likely due to the fact that traces are uploaded to OpenAI servers, and you do not possess an OpenAI API key. You have three options to resolve this issue:\n\n1. Disable tracing entirely by using the method `set_tracing_disabled(True)`.\n2. Set an OpenAI key specifically for tracing with the method `set_tracing_export_api_key(...)`. This API key will be exclusively used for uploading traces and must be obtained from the OpenAI platform.\n3. Utilize a non-OpenAI trace processor. Please refer to the tracing documentation for more details.\n\n### Responses API Support\n\nIf the text includes code blocks, maintain the original code format by wrapping it in triple backticks, along with the appropriate programming language. It is essential not to fabricate facts or create false information. Your role is solely to enhance the coherence and clarity of the text.",
    "chunk_id": "045be187-e19b-49f6-9f60-687dc8aa84c7"
  },
  {
    "content": "### Responses API Support\n\nThe SDK defaults to using the Responses API; however, most other LLM providers currently do not support it. This can lead to 404 errors or similar issues. To resolve this, you have two options:\n\n1. Call `set_default_openai_api(\"chat_completions\")`. This method is effective when setting `OPENAI_API_KEY` and `OPENAI_BASE_URL` through environment variables.\n2. Utilize the `OpenAIChatCompletionsModel`. Examples can be found in the provided resources.\n\n### Structured Outputs Support\n\nSome model providers lack support for structured outputs, which may result in errors. If the text includes code blocks, do not alter the code. Instead, wrap it in triple backticks along with the appropriate programming language. Avoid fabricating facts or hallucinating. Your role is solely to enhance the clarity and coherence of the text. \n\nIf the text contains URLs or hyperlinks, remove the links while preserving the accompanying text. For example, convert \"This is a link\" to \"This is a link.\"\n",
    "chunk_id": "29f8093f-0264-405e-9c92-feb74526cc83"
  },
  {
    "content": "### Structured Outputs Support\n\nSome model providers lack support for structured outputs. This limitation can lead to errors resembling the following:\n\n```md-code__content\nBadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n```\n\nThis is a shortcoming of certain model providers; they offer JSON outputs but do not permit specifying the required `json_schema` for the output. While we are working on a solution to this issue, we recommend using providers that support JSON schema output. Failing to do so may result in your application frequently breaking due to malformed JSON.\n\n## OpenAI SDK Configuration\n\n# Configuring the SDK\n\n## API Keys and Clients \n\nIf the text contains code blocks, ensure they remain unchanged, wrapped in triple backticks along with the programming language. Your focus should be on enhancing coherence and clarity without altering facts or creating hallucinations. Additionally, remove URLs or hyperlinks while preserving their descriptive text.",
    "chunk_id": "f8082c9d-fde1-4b6c-8362-d9f74ba47ee5"
  },
  {
    "content": "## OpenAI SDK Configuration\n\n# Configuring the SDK\n\n## API Keys and Clients\n\nBy default, the SDK searches for the `OPENAI_API_KEY` environment variable for Large Language Model (LLM) requests and tracing as soon as it is imported. If you're unable to set this environment variable before your application starts, you can use the `set_default_openai_key()` function to define the key manually.\n\n```python\nfrom agents import set_default_openai_key\n\nset_default_openai_key(\"sk-...\")\n```",
    "chunk_id": "a378c4a0-0412-4e64-84c7-207bcd878e13"
  },
  {
    "content": "To set the default OpenAI API key, you can use the following code:\n\n```python\nfrom agents import set_default_openai_key\n\nset_default_openai_key(\"sk-...\")\n```\n\nAlternatively, you can configure a custom OpenAI client. By default, the SDK creates an `AsyncOpenAI` instance that uses the API key from the environment variable or the key set above. To change this behavior, you can use the `set_default_openai_client()` function.\n\n```python\nfrom openai import AsyncOpenAI\nfrom agents import set_default_openai_client\n\ncustom_client = AsyncOpenAI(base_url=\"...\", api_key=\"...\")\nset_default_openai_client(custom_client)\n```\n\nAdditionally, you can customize the OpenAI API that is being used. By default, the SDK uses the OpenAI Responses API, but you can override this to use the Chat Completions API by using the `set_default_openai_api()` function.",
    "chunk_id": "6bc98b81-4be1-4f4f-a19d-ef7fa63f62b6"
  },
  {
    "content": "```md\nfrom agents import set_default_openai_api\n\nset_default_openai_api(\"chat_completions\")\n\n## Tracing\n\nTracing is enabled by default. It uses the OpenAI API keys from the section above, either from the environment variable or the default key you set. To specify the API key used for tracing, you can use the `set_tracing_export_api_key` function.\n\n```md\nfrom agents import set_tracing_export_api_key\n\nset_tracing_export_api_key(\"sk-...\")\n```\n\nAdditionally, you can disable tracing entirely by using the `set_tracing_disabled()` function.\n\n```md\nfrom agents import set_tracing_disabled\n\nset_tracing_disabled(True)\n```\n\n## Debug Logging\n```",
    "chunk_id": "791db350-1e3b-45d1-bd25-d6184573ed4a"
  },
  {
    "content": "```md-code__content\nfrom agents import set_tracing_disabled\n\nset_tracing_disabled(True)\n\n```\n\n## Debug Logging\n\nThe SDK includes two Python loggers that do not have any handlers set. By default, this means that warnings and errors will be displayed on `stdout`, while other log messages will be suppressed.\n\nTo enable verbose logging, use the `enable_verbose_stdout_logging()` function.\n\n```md-code__content\nfrom agents import enable_verbose_stdout_logging\n\nenable_verbose_stdout_logging()\n\n```\n\nAlternatively, you can customize the logging setup by adding your own handlers, filters, and formatters. For more details, refer to the Python logging guide.\n\n```md-code__content\nimport logging\n\nlogger = logging.getLogger(\"openai.agents\")  # or use openai.agents.tracing for the Tracing logger\n```",
    "chunk_id": "457db754-4659-49ff-93fe-8cec94e9ade1"
  },
  {
    "content": "```markdown\nimport logging\n\nlogger = logging.getLogger(\"openai.agents\")  # or openai.agents.tracing for the Tracing logger\n\n# To make all logs show up\nlogger.setLevel(logging.DEBUG)\n\n# To make info and above show up\nlogger.setLevel(logging.INFO)\n\n# To make warning and above show up\nlogger.setLevel(logging.WARNING)\n\n# You can customize this as needed; this will output to `stderr` by default\nlogger.addHandler(logging.StreamHandler())\n\n### Sensitive Data in Logs\n\nCertain logs may contain sensitive information, such as user data. If you want to prevent this data from being logged, set the following environment variables.\n\nTo disable logging of LLM inputs and outputs:\n\n```bash\nexport OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1\n```\n\nTo disable logging of tool inputs and outputs:\n\n```bash\nexport OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1\n```\n\n## OpenAI Agents Module\n\n# Agents Module\n\nIf the text contains code blocks, don't change the code. Wrap it in triple backticks along with the programming language. Don't make up facts or hallucinate. Your job is to make the text more coherent and easier to understand only.\n\nIf the text contains URLs or hyperlinks, remove them while preserving their text. For example: This is a link.\n```",
    "chunk_id": "31823352-6d15-491c-82de-e7413d4ea307"
  },
  {
    "content": "To disable logging of tool inputs and outputs, set the following environment variable:\n\n```md-code__content\nexport OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1\n```\n\n## OpenAI Agents Module\n\n### set_default_openai_key\n\n```md-code__content\nset_default_openai_key(\n    key: str, use_for_tracing: bool = True\n) -> None\n```\n\nThis function sets the default OpenAI API key for LLM requests and optionally for tracing. It is only necessary to use this function if the OPENAI_API_KEY environment variable has not been set.\n\nIf provided, this key will be used in place of the OPENAI_API_KEY environment variable.\n\n**Parameters:**",
    "chunk_id": "f2c1fb58-bcb1-4df2-bc7c-65abe992d265"
  },
  {
    "content": "If provided, this key will be utilized instead of the `OPENAI_API_KEY` environment variable.\n\n### Parameters:\n\n| Name                  | Type   | Description                                                                                                        | Default   |\n|-----------------------|--------|--------------------------------------------------------------------------------------------------------------------|-----------|\n| `key`                 | `str`  | The OpenAI key to use.                                                                                             | _required_|\n| `use_for_tracing`     | `bool` | Indicates whether this key will also be used to send traces to OpenAI. Defaults to True. If set to False, you must either set the `OPENAI_API_KEY` environment variable or call `set_tracing_export_api_key()` with the API key intended for tracing. | `True`    |\n\nSource code is located in `src/agents/__init__.py`.",
    "chunk_id": "da165d0d-2746-415f-bed6-4d0db8a680ee"
  },
  {
    "content": "Source code in `src/agents/__init__.py`\n\n```python\ndef set_default_openai_key(key: str, use_for_tracing: bool = True) -> None:\n    \"\"\"Set the default OpenAI API key for LLM requests and optional tracing.\n\n    This function is only necessary if the OPENAI_API_KEY environment variable is not already set. \n    If a key is provided, it will be used instead of the OPENAI_API_KEY environment variable.\n\n    Args:\n        key: The OpenAI key to use.\n        use_for_tracing: Indicates whether to use this key for sending traces to OpenAI. Defaults to True.\n            If set to False, you will either need to set the OPENAI_API_KEY environment variable or \n            call set_tracing_export_api_key() with the desired API key for tracing.\n    \"\"\"\n    _config.set_default_openai_key(key, use_for_tracing)\n```",
    "chunk_id": "e05dd379-7b82-4d36-a6f2-676db6d821ea"
  },
  {
    "content": "### set\\_default\\_openai\\_client\n\n```md-code__content\nset_default_openai_client(\n    client: AsyncOpenAI, use_for_tracing: bool = True\n) -> None\n```\n\nThis function sets the default OpenAI client to be used for large language model (LLM) requests and tracing. If specified, this client will replace the default OpenAI client.\n\n**Parameters:**\n\n| Name                  | Type        | Description                                                                                                                                                                          | Default   |\n|-----------------------|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|\n| `client`              | `AsyncOpenAI` | The OpenAI client to be utilized.                                                                                                                                                  | _required_ |\n| `use_for_tracing`     | `bool`      | Indicates whether to use the API key from this client for uploading traces. If set to False, you will need to either set the `OPENAI_API_KEY` environment variable or use `set_tracing_export_api_key()` with the desired API key for tracing. | `True`    |\n\nSource code is located in `src/agents/__init__.py`.",
    "chunk_id": "8a758212-a3be-4e10-9e05-60874be88cd8"
  },
  {
    "content": "Source code in `src/agents/__init__.py`\n\n|     |     |\n| --- | --- |\n| ```<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>``` | ```python<br>def set_default_openai_client(client: AsyncOpenAI, use_for_tracing: bool = True) -> None:<br>    \"\"\"Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this<br>    client will be used instead of the default OpenAI client.<br>    Args:<br>        client: The OpenAI client to use.<br>        use_for_tracing: Whether to use the API key from this client for uploading traces. If False,<br>            you'll either need to set the OPENAI_API_KEY environment variable or call<br>            set_tracing_export_api_key() with the API key you want to use for tracing.<br>    \"\"\"<br>    _config.set_default_openai_client(client, use_for_tracing)<br>``` |\n\n### set_default_openai_api\n\n```python\ndef set_default_openai_api(\n    api: Literal[\"chat_completions\", \"responses\"],\n) -> None:\n```",
    "chunk_id": "9f94becb-26ef-4a1b-bc37-2d638cac5e96"
  },
  {
    "content": "### set_default_openai_api\n\n```python\ndef set_default_openai_api(api: Literal[\"chat_completions\", \"responses\"]) -> None:\n    \"\"\"Set the default API to use for OpenAI LLM requests. By default, the responses API will be used,\n    but you can change this to use the chat completions API instead.\n    \"\"\"\n    _config.set_default_openai_api(api)\n```\n\nSets the default API for OpenAI LLM requests. By default, the responses API is used, but you have the option to switch to the chat completions API.\n\nSource code in `src/agents/__init__.py`\n\n|     |     |\n| --- | --- |\n| 124 |   |\n| 125 |   |\n| 126 |   |\n| 127 |   |\n| 128 |   |\n\n### set_tracing_export_api_key\n\n```python\ndef set_tracing_export_api_key(api_key: str) -> None:\n    \"\"\"Set the OpenAI API key for the backend exporter.\"\"\"\n```\n\nConfigures the OpenAI API key for the backend exporter.\n\nSource code in `src/agents/tracing/__init__.py`",
    "chunk_id": "5a0315e5-f252-4eb6-b075-8407bd490319"
  },
  {
    "content": "### set_tracing_export_api_key\n\n```python\ndef set_tracing_export_api_key(api_key: str) -> None:\n    \"\"\"\n    Set the OpenAI API key for the backend exporter.\n    \"\"\"\n    default_exporter().set_api_key(api_key)\n```\n\nSource code in `src/agents/tracing/__init__.py`\n\n|     |     |\n| --- | --- |\n| ```<br>84<br>85<br>86<br>87<br>88<br>``` | ```python<br>def set_tracing_export_api_key(api_key: str) -> None:<br>    \"\"\"<br>    Set the OpenAI API key for the backend exporter.<br>    \"\"\"<br>    default_exporter().set_api_key(api_key)<br>``` |\n\n### set_tracing_disabled\n\n```python\ndef set_tracing_disabled(disabled: bool) -> None:\n    \"\"\"\n    Set whether tracing is globally disabled.\n    \"\"\"\n    GLOBAL_TRACE_PROVIDER.set_disabled(disabled)\n```\n\nSource code in `src/agents/tracing/__init__.py`\n\n|     |     |\n| --- | --- |\n| ```<br>77<br>78<br>79<br>80<br>81<br>``` | ```python<br>def set_tracing_disabled(disabled: bool) -> None:<br>    \"\"\"<br>    Set whether tracing is globally disabled.<br>    \"\"\"<br>    GLOBAL_TRACE_PROVIDER.set_disabled(disabled)<br>``` |\n\n### set_trace_processors",
    "chunk_id": "a1f4a458-72d6-4564-90d0-478510a2d911"
  },
  {
    "content": "### set_trace_processors\n\n```python\ndef set_trace_processors(processors: list[TracingProcessor]) -> None:\n    \"\"\"\n    Set the list of trace processors. This will replace the current list of processors.\n    \"\"\"\n    GLOBAL_TRACE_PROVIDER.set_processors(processors)\n```\n\nThis function sets a new list of trace processors, replacing any existing processors. The source code can be found in `src/agents/tracing/__init__.py`.\n\n### enable_verbose_stdout_logging\n\n```python\ndef enable_verbose_stdout_logging():\n```\n\nThis function enables verbose logging to standard output (stdout), which is useful for debugging purposes. The source code is located in `src/agents/__init__.py`.",
    "chunk_id": "2198e9dc-3632-4fc3-9444-2b587ca6a4fc"
  },
  {
    "content": "### enable\\_verbose\\_stdout\\_logging\n\n```python\nenable_verbose_stdout_logging()\n```\n\nThis function enables verbose logging to standard output (stdout), which is particularly useful for debugging purposes.\n\nThe source code can be found in `src/agents/__init__.py`:\n\n|          |          |\n|----------|----------|\n| ```     | ```python|\n| 131      | def enable_verbose_stdout_logging(): |\n| 132      |     \"\"\"Enables verbose logging to stdout. This is useful for debugging.\"\"\" |\n| 133      |     logger = logging.getLogger(\"openai.agents\") |\n| 134      |     logger.setLevel(logging.DEBUG) |\n| 135      |     logger.addHandler(logging.StreamHandler(sys.stdout)) |\n| ```      | ```     |",
    "chunk_id": "affb3ce7-b567-46e5-85fd-93edb1c0cb2a"
  }
]
