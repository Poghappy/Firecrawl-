{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q unsloth\n!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nwb_token = user_secrets.get_secret(\"WANDB_TOKEN\")\n\n# Login to both services\nlogin(hf_token)\nwandb.login(key=wb_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run = wandb.init(\n    project='fine-tune-deepseek-firecrawl', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length=2048,\n    dtype=torch.bfloat16,\n    load_in_4bit=True,\n    token=hf_token\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\n\nFastLanguageModel.for_inference(model)\n\nprompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. \n\n### Instruction:\n{}\n\n### Response:\n{}\n\"\"\"\ninstruction = \"How do I extract repo name, number of stars, repo link from the https://github.com/trending page using Firecrawl?\"\n\nmessage = prompt.format(instruction, \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512, use_cache=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n    max_seq_length=2048,\n    dtype=torch.bfloat16,\n    load_in_4bit=True,\n    token=hf_token\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"bexgboost/firecrawl-instructions\"\ndataset = load_dataset(\n    dataset_name, split = \"train[0:500]\", trust_remote_code=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token\n\ndef format_instruction(example):\n\n    prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n    Write a response that appropriately completes the request. \n    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n    ### Instruction:\n    You are a web scraping expert with advanced knowledge in Firecrawl, which is an AI-based web-scraping engine. \n    Please answer the following question about Firecrawl. \n\n    ### Question:\n    {}\n\n    ### Response:\n    {}\"\"\"\n\n    return {\n        \"text\": prompt.format(example['instruction'], example['answer']) + EOS_TOKEN\n    }\n\ndataset = dataset.map(format_instruction)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  \n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  \n    bias=\"none\",  \n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=1000,\n    use_rslora=False,  \n    loftq_config=None,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\nmodel_name = \"firecrawl-assistant\"\nlocal_path = f\"./models/{model_name}\"\ntraining_arguments = TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=1000,\n        output_dir=local_path,\n        push_to_hub=True,\n        hub_model_id=f\"bexgboost/{model_name}\",\n        report_to=\"wandb\",\n        run_name=\"firecrawl-deepseek-ft\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    args=training_arguments,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\n\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512, use_cache=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(local_path)\ntrainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}