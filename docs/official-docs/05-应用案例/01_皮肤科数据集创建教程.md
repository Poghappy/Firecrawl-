# How to Create a Dermatology Q&A Dataset with OpenAI Harmony & Firecrawl Search

**作者**: Abid Ali Awan  
**发布时间**: Aug 15, 2025  
**原文链接**: https://www.firecrawl.dev/blog/creating_dermatology_dataset_with_openai_harmony_firecrawl_search  
**分类**: AI Engineering, Web Extraction

## 摘要

本文提供了一个详细的分步指南，介绍如何使用 Firecrawl 从网络收集皮肤科数据，使用 Harmony prompt 风格处理数据，使用 GPT-OSS 120B 生成结构化的问答数据集，并将其发布到 Hugging Face，同时提供检查点功能以确保可靠性。

## 主要内容

### 环境设置

文章首先介绍了如何设置开发环境，包括安装必要的依赖包：

```python
# 安装必要的依赖
pip install firecrawl-py datasets huggingface_hub openai
```

### 数据模型定义

使用 Pydantic 定义数据结构：

```python
from pydantic import BaseModel
from typing import List

class QAPair(BaseModel):
    question: str
    answer: str
    source_url: str
    confidence: float

class DermatologyDataset(BaseModel):
    qa_pairs: List[QAPair]
    metadata: dict
```

### Web 发现过程

使用 Firecrawl 的搜索功能发现相关的皮肤科网站：

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="your-api-key")

# 搜索皮肤科相关内容
search_results = app.search(
    query="dermatology skin conditions treatment",
    limit=50
)
```

### Prompt 构建

文章详细介绍了如何构建有效的 prompt 来提取结构化数据：

```python
harmony_prompt = """
You are a medical expert specializing in dermatology. 
Analyze the following content and create question-answer pairs 
that would be useful for training a medical AI assistant.

Focus on:
- Common skin conditions
- Treatment options
- Diagnostic criteria
- Patient care guidelines

Content: {content}
"""
```

### 数据处理流程

#### 1. 数据收集

```python
def collect_dermatology_data():
    """收集皮肤科相关数据"""
    firecrawl = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
    
    # 定义搜索查询
    queries = [
        "dermatology skin conditions",
        "skin disease treatment",
        "dermatological diagnosis",
        "skin care medical advice"
    ]
    
    all_results = []
    for query in queries:
        results = firecrawl.search(
            query=query,
            limit=20,
            scrapeOptions={
                "formats": ["markdown"],
                "onlyMainContent": True
            }
        )
        all_results.extend(results)
    
    return all_results
```

#### 2. 内容处理

```python
def process_content(content):
    """处理和清洗内容"""
    # 移除不相关的内容
    cleaned_content = remove_navigation_and_ads(content)
    
    # 提取医疗相关段落
    medical_paragraphs = extract_medical_content(cleaned_content)
    
    # 验证内容质量
    if is_medical_content_valid(medical_paragraphs):
        return medical_paragraphs
    
    return None
```

#### 3. 问答对生成

```python
def generate_qa_pairs(content):
    """使用 GPT-OSS 生成问答对"""
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": harmony_prompt
            },
            {
                "role": "user",
                "content": content
            }
        ],
        temperature=0.3
    )
    
    return parse_qa_response(response.choices[0].message.content)
```

### 质量控制

#### 数据验证

```python
def validate_qa_pair(qa_pair):
    """验证问答对的质量"""
    checks = [
        len(qa_pair.question) > 10,
        len(qa_pair.answer) > 20,
        is_medical_relevant(qa_pair.question),
        is_factually_accurate(qa_pair.answer),
        qa_pair.confidence > 0.7
    ]
    
    return all(checks)
```

#### 去重处理

```python
def remove_duplicates(qa_pairs):
    """移除重复的问答对"""
    seen_questions = set()
    unique_pairs = []
    
    for pair in qa_pairs:
        question_hash = hash(pair.question.lower().strip())
        if question_hash not in seen_questions:
            seen_questions.add(question_hash)
            unique_pairs.append(pair)
    
    return unique_pairs
```

### 数据集发布

#### Hugging Face 集成

```python
from datasets import Dataset
from huggingface_hub import HfApi

def publish_to_huggingface(qa_pairs, dataset_name):
    """发布数据集到 Hugging Face"""
    # 转换为 Hugging Face 格式
    dataset_dict = {
        "question": [pair.question for pair in qa_pairs],
        "answer": [pair.answer for pair in qa_pairs],
        "source_url": [pair.source_url for pair in qa_pairs],
        "confidence": [pair.confidence for pair in qa_pairs]
    }
    
    dataset = Dataset.from_dict(dataset_dict)
    
    # 上传到 Hugging Face
    dataset.push_to_hub(
        dataset_name,
        private=False,
        token=HF_TOKEN
    )
    
    print(f"Dataset published: https://huggingface.co/datasets/{dataset_name}")
```

### 检查点机制

```python
import pickle
import os

def save_checkpoint(data, checkpoint_name):
    """保存检查点"""
    checkpoint_dir = "checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint_path = os.path.join(checkpoint_dir, f"{checkpoint_name}.pkl")
    with open(checkpoint_path, "wb") as f:
        pickle.dump(data, f)
    
    print(f"Checkpoint saved: {checkpoint_path}")

def load_checkpoint(checkpoint_name):
    """加载检查点"""
    checkpoint_path = os.path.join("checkpoints", f"{checkpoint_name}.pkl")
    
    if os.path.exists(checkpoint_path):
        with open(checkpoint_path, "rb") as f:
            return pickle.load(f)
    
    return None
```

### 完整工作流程

```python
def main():
    """主要工作流程"""
    print("开始皮肤科数据集创建流程...")
    
    # 1. 检查是否有现有检查点
    existing_data = load_checkpoint("raw_data")
    
    if existing_data is None:
        # 2. 收集数据
        print("收集皮肤科数据...")
        raw_data = collect_dermatology_data()
        save_checkpoint(raw_data, "raw_data")
    else:
        raw_data = existing_data
        print("从检查点加载原始数据")
    
    # 3. 处理内容
    print("处理内容...")
    processed_content = []
    for item in raw_data:
        content = process_content(item['content'])
        if content:
            processed_content.append({
                'content': content,
                'url': item['url']
            })
    
    save_checkpoint(processed_content, "processed_content")
    
    # 4. 生成问答对
    print("生成问答对...")
    all_qa_pairs = []
    
    for item in processed_content:
        qa_pairs = generate_qa_pairs(item['content'])
        for pair in qa_pairs:
            pair.source_url = item['url']
            if validate_qa_pair(pair):
                all_qa_pairs.append(pair)
    
    # 5. 去重和最终处理
    unique_qa_pairs = remove_duplicates(all_qa_pairs)
    save_checkpoint(unique_qa_pairs, "final_dataset")
    
    # 6. 发布到 Hugging Face
    print(f"发布 {len(unique_qa_pairs)} 个问答对到 Hugging Face...")
    publish_to_huggingface(unique_qa_pairs, "dermatology-qa-dataset")
    
    print("数据集创建完成！")

if __name__ == "__main__":
    main()
```

## 技术亮点

1. **智能数据收集**: 使用 Firecrawl 的搜索 API 自动发现相关医疗内容
2. **结构化提取**: 通过 GPT-OSS 120B 将非结构化内容转换为标准化问答对
3. **质量控制**: 实施多层验证确保数据质量
4. **可靠性保证**: 通过检查点机制防止数据丢失
5. **开放共享**: 直接发布到 Hugging Face 平台供社区使用

## 最佳实践

### 数据质量

1. **多源验证**: 从多个可靠来源收集数据
2. **专家审核**: 邀请医疗专家审核生成的内容
3. **持续更新**: 定期更新数据集以反映最新医疗知识

### 伦理考虑

1. **医疗免责声明**: 明确标注数据集仅供研究使用
2. **隐私保护**: 确保不包含患者个人信息
3. **准确性警告**: 提醒用户验证医疗建议的准确性

### 技术优化

1. **并行处理**: 使用多线程加速数据处理
2. **缓存机制**: 缓存 API 响应以减少重复调用
3. **错误处理**: 实施健壮的错误处理和重试机制

## 结论

通过结合 Firecrawl 的强大网页抓取能力和 OpenAI 的先进语言模型，我们可以高效地创建高质量的医疗问答数据集。这种方法不仅节省了大量手动工作，还确保了数据的一致性和可靠性。

生成的数据集可以用于训练医疗 AI 助手、构建知识库或支持医疗研究项目。通过遵循本文介绍的最佳实践，您可以创建符合行业标准的专业医疗数据集。