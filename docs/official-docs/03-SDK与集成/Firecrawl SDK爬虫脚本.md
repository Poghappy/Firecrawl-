# Firecrawl SDK çˆ¬è™«è„šæœ¬

**åŠŸèƒ½ï¼š** ç½‘é¡µæŠ“å–ã€æ‰¹é‡çˆ¬å–ã€ç»“æ„åŒ–æ•°æ®æå–

**ä½œè€…ï¼š** Trae IDE Agent

**åˆ›å»ºæ—¶é—´ï¼š** 2025-01-17

## æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Firecrawl SDK Python çˆ¬è™«è„šæœ¬ï¼Œæ”¯æŒå•é¡µé¢æŠ“å–ã€æ‰¹é‡ç½‘ç«™çˆ¬å–å’Œç»“æ„åŒ–æ•°æ®æå–åŠŸèƒ½ã€‚

## ä¸»è¦åŠŸèƒ½

- ğŸ” **å•é¡µé¢æŠ“å–** - æŠ“å–æŒ‡å®šURLçš„å†…å®¹
- ğŸ•·ï¸ **æ‰¹é‡ç½‘ç«™çˆ¬å–** - çˆ¬å–æ•´ä¸ªç½‘ç«™çš„å¤šä¸ªé¡µé¢
- ğŸ“Š **ç»“æ„åŒ–æ•°æ®æå–** - æ ¹æ®è‡ªå®šä¹‰schemaæå–ç»“æ„åŒ–æ•°æ®
- ğŸ’¾ **å¤šæ ¼å¼ä¿å­˜** - æ”¯æŒJSONã€Markdownã€æ–‡æœ¬æ ¼å¼ä¿å­˜
- âš™ï¸ **çµæ´»é…ç½®** - æ”¯æŒè‡ªå®šä¹‰æ ‡ç­¾è¿‡æ»¤ã€è¾“å‡ºæ ¼å¼ç­‰

## ä»£ç å®ç°

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Firecrawl SDK çˆ¬è™«è„šæœ¬
åŠŸèƒ½ï¼šç½‘é¡µæŠ“å–ã€æ‰¹é‡çˆ¬å–ã€ç»“æ„åŒ–æ•°æ®æå–ã€APIé›†æˆ
ä½œè€…ï¼šTrae IDE Agent
åˆ›å»ºæ—¶é—´ï¼š2025-01-17
ç‰ˆæœ¬ï¼šv2.0 - å¢åŠ APIé›†æˆåŠŸèƒ½
"""

import os
import json
import time
import requests
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime, timezone
from dataclasses import dataclass, field
from enum import Enum
from urllib.parse import urljoin, urlparse
import hashlib
import re
from pathlib import Path
from firecrawl import FirecrawlApp

@dataclass
class ScrapingConfig:
    """çˆ¬å–é…ç½®ç±»"""
    api_key: str
    output_format: List[str] = None  # ["markdown", "html", "structured"]
    max_pages: int = 10
    include_tags: List[str] = None
    exclude_tags: List[str] = None
    wait_for: int = 0  # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    
    def __post_init__(self):
        if self.output_format is None:
            self.output_format = ["markdown"]
        if self.include_tags is None:
            self.include_tags = []
        if self.exclude_tags is None:
            self.exclude_tags = ["nav", "footer", "script", "style"]

class FirecrawlScraper:
    """Firecrawl çˆ¬è™«ç±»"""
    
    def __init__(self, config: ScrapingConfig):
        self.config = config
        [self.app](http://self.app) = FirecrawlApp(api_key=config.api_key)
        
    def scrape_single_page(self, url: str) -> Optional[Dict[str, Any]]:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        try:
            options = {
                'formats': self.config.output_format,
                'includeTags': self.config.include_tags,
                'excludeTags': self.config.exclude_tags,
                'waitFor': self.config.wait_for
            }
            
            result = [self.app](http://self.app).scrape_url(url, options)
            return result
            
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {str(e)}")
            return None
    
    def crawl_website(self, url: str) -> List[Dict[str, Any]]:
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        try:
            options = {
                'formats': self.config.output_format,
                'includeTags': self.config.include_tags,
                'excludeTags': self.config.exclude_tags,
                'limit': self.config.max_pages
            }
            
            results = [self.app](http://self.app).crawl_url(url, options)
            return results
            
        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥: {str(e)}")
            return []

# ================== API é›†æˆæ¨¡å— ==================

class PublishStatus(Enum):
    """å‘å¸ƒçŠ¶æ€æšä¸¾"""
    DRAFT = "draft"              # è‰ç¨¿
    PUBLISHED = "published"      # å·²å‘å¸ƒ
    SCHEDULED = "scheduled"      # å®šæ—¶å‘å¸ƒ
    ARCHIVED = "archived"        # å·²å½’æ¡£
    DELETED = "deleted"          # å·²åˆ é™¤

class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    ARTICLE = "article"          # æ–‡ç« 
    NEWS = "news"                # æ–°é—»
    BLOG = "blog"                # åšå®¢
    ANNOUNCEMENT = "announcement" # å…¬å‘Š

@dataclass
class APIConfig:
    """APIé…ç½®"""
    # åŸºç¡€é…ç½®
    base_url: str
    api_key: str
    timeout: int = 30
    
    # é‡è¯•é…ç½®
    max_retries: int = 3
    retry_delay: float = 1.0
    backoff_factor: float = 2.0
    
    # è¯·æ±‚é…ç½®
    user_agent: str = "Firecrawl-HuoNiao-Integration/1.0"
    verify_ssl: bool = True
    
    # é™æµé…ç½®
    rate_limit: int = 60  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
    rate_window: int = 60  # æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    
    # é»˜è®¤é…ç½®
    default_category_id: Optional[int] = None
    default_author_id: Optional[int] = None
    default_status: PublishStatus = PublishStatus.DRAFT
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if not self.base_url:
            raise ValueError("base_urlä¸èƒ½ä¸ºç©º")
        if not self.api_key:
            raise ValueError("api_keyä¸èƒ½ä¸ºç©º")
            
        # ç¡®ä¿base_urlä»¥/ç»“å°¾
        if not self.base_url.endswith('/'):
            self.base_url += '/'

@dataclass
class PublishRequest:
    """å‘å¸ƒè¯·æ±‚"""
    title: str
    content: str
    
    # å¯é€‰å­—æ®µ
    summary: Optional[str] = None
    category_id: Optional[int] = None
    author_id: Optional[int] = None
    tags: List[str] = field(default_factory=list)
    
    # å‘å¸ƒé…ç½®
    status: PublishStatus = PublishStatus.DRAFT
    publish_time: Optional[datetime] = None
    
    # SEOé…ç½®
    seo_title: Optional[str] = None
    seo_description: Optional[str] = None
    seo_keywords: List[str] = field(default_factory=list)
    
    # åª’ä½“é…ç½®
    featured_image: Optional[str] = None
    images: List[str] = field(default_factory=list)
    
    # å…ƒæ•°æ®
    source_url: Optional[str] = None
    external_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_api_data(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºAPIæ•°æ®æ ¼å¼"""
        data = {
            'title': self.title,
            'content': self.content,
            'status': self.status.value
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if self.summary:
            data['summary'] = self.summary
        if self.category_id:
            data['category_id'] = self.category_id
        if [self.author](http://self.author)_id:
            data['author_id'] = [self.author](http://self.author)_id
        if self.tags:
            data['tags'] = self.tags
        if self.publish_time:
            data['publish_time'] = self.publish_time.isoformat()
            
        # SEOå­—æ®µ
        if self.seo_title:
            data['seo_title'] = self.seo_title
        if self.seo_description:
            data['seo_description'] = self.seo_description
        if self.seo_keywords:
            data['seo_keywords'] = self.seo_keywords
            
        # åª’ä½“å­—æ®µ
        if self.featured_image:
            data['featured_image'] = self.featured_image
        if self.images:
            data['images'] = self.images
            
        # å…ƒæ•°æ®
        if self.source_url:
            data['source_url'] = self.source_url
        if self.external_id:
            data['external_id'] = self.external_id
        if self.metadata:
            data['metadata'] = self.metadata
            
        return data

@dataclass
class PublishResponse:
    """å‘å¸ƒå“åº”"""
    success: bool
    article_id: Optional[int] = None
    message: Optional[str] = None
    error_code: Optional[str] = None
    data: Optional[Dict[str, Any]] = None
    
    @classmethod
    def from_api_response(cls, response_data: Dict[str, Any]) -> 'PublishResponse':
        """ä»APIå“åº”åˆ›å»ºå¯¹è±¡"""
        return cls(
            success=response_data.get('success', False),
            article_id=response_data.get('data', {}).get('id'),
            message=response_data.get('message'),
            error_code=response_data.get('error_code'),
            data=response_data.get('data')
        )

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []
        self.logger = logging.getLogger(__name__)
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚"""
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
        self.requests = [req_time for req_time in self.requests
                        if now - req_time < self.time_window]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.requests) >= self.max_requests:
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            oldest_request = min(self.requests)
            wait_time = self.time_window - (now - oldest_request)
            
            if wait_time > 0:
                [self.logger.info](http://self.logger.info)(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                time.sleep(wait_time)
        
        # è®°å½•å½“å‰è¯·æ±‚
        self.requests.append(now)

class HuoNiaoAPIClient:
    """ç«é¸Ÿé—¨æˆ·APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: APIConfig):
        self.config = config
        self.session = requests.Session()
        self.rate_limiter = RateLimiter(config.rate_limit, config.rate_window)
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': config.user_agent,
            'Authorization': f'Bearer {config.api_key}',
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })
        
        # SSLéªŒè¯
        self.session.verify = config.verify_ssl
    
    def _make_request(self, method: str, endpoint: str,
                     data: Optional[Dict[str, Any]] = None,
                     params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å‘é€APIè¯·æ±‚"""
        url = urljoin(self.config.base_url, endpoint)
        
        for attempt in range(self.config.max_retries + 1):
            try:
                # é€Ÿç‡é™åˆ¶
                self.rate_limiter.wait_if_needed()
                
                # å‘é€è¯·æ±‚
                response = self.session.request(
                    method=method,
                    url=url,
                    json=data,
                    params=params,
                    timeout=self.config.timeout
                )
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                response.raise_for_status()
                
                # è§£æå“åº”
                try:
                    return response.json()
                except json.JSONDecodeError:
                    return {'success': True, 'data': response.text}
                    
            except requests.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries + 1}): {str(e)}")
                
                if attempt < self.config.max_retries:
                    # æŒ‡æ•°é€€é¿
                    delay = self.config.retry_delay * (self.config.backoff_factor ** attempt)
                    time.sleep(delay)
                else:
                    raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            response = self._make_request('GET', 'api/system/status')
            return response.get('success', False)
        except Exception as e:
            self.logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    def get_categories(self) -> List[Dict[str, Any]]:
        """è·å–åˆ†ç±»åˆ—è¡¨"""
        try:
            response = self._make_request('GET', 'api/categories')
            return response.get('data', [])
        except Exception as e:
            self.logger.error(f"è·å–åˆ†ç±»å¤±è´¥: {str(e)}")
            return []
    
    def publish_article(self, request: PublishRequest) -> PublishResponse:
        """å‘å¸ƒæ–‡ç« """
        try:
            data = [request.to](http://request.to)_api_data()
            response = self._make_request('POST', 'api/articles', data=data)
            return PublishResponse.from_api_response(response)
        except Exception as e:
            self.logger.error(f"å‘å¸ƒæ–‡ç« å¤±è´¥: {str(e)}")
            return PublishResponse(
                success=False,
                message=f"å‘å¸ƒå¤±è´¥: {str(e)}",
                error_code="PUBLISH_ERROR"
            )
    
    def update_article(self, article_id: int, request: PublishRequest) -> PublishResponse:
        """æ›´æ–°æ–‡ç« """
        try:
            data = [request.to](http://request.to)_api_data()
            response = self._make_request('PUT', f'api/articles/{article_id}', data=data)
            return PublishResponse.from_api_response(response)
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ–‡ç« å¤±è´¥: {str(e)}")
            return PublishResponse(
                success=False,
                message=f"æ›´æ–°å¤±è´¥: {str(e)}",
                error_code="UPDATE_ERROR"
            )
    
    def delete_article(self, article_id: int) -> PublishResponse:
        """åˆ é™¤æ–‡ç« """
        try:
            response = self._make_request('DELETE', f'api/articles/{article_id}')
            return PublishResponse.from_api_response(response)
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–‡ç« å¤±è´¥: {str(e)}")
            return PublishResponse(
                success=False,
                message=f"åˆ é™¤å¤±è´¥: {str(e)}",
                error_code="DELETE_ERROR"
            )
    
    def search_articles(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡ç« """
        try:
            params = {'q': query, 'limit': limit}
            response = self._make_request('GET', 'api/articles/search', params=params)
            return response.get('data', [])
        except Exception as e:
            self.logger.error(f"æœç´¢æ–‡ç« å¤±è´¥: {str(e)}")
            return []
    
    def batch_publish(self, requests: List[PublishRequest]) -> List[PublishResponse]:
        """æ‰¹é‡å‘å¸ƒæ–‡ç« """
        responses = []
        
        for i, request in enumerate(requests):
            [self.logger.info](http://self.logger.info)(f"æ‰¹é‡å‘å¸ƒè¿›åº¦: {i + 1}/{len(requests)}")
            response = self.publish_article(request)
            responses.append(response)
            
            # å¦‚æœå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
            if not response.success:
                self.logger.error(f"æ‰¹é‡å‘å¸ƒç¬¬ {i + 1} é¡¹å¤±è´¥: {response.message}")
        
        return responses

class APIIntegration:
    """APIé›†æˆä¸»ç±»"""
    
    def __init__(self, firecrawl_config: ScrapingConfig, api_config: APIConfig):
        self.firecrawl_config = firecrawl_config
        self.api_config = api_config
        self.scraper = FirecrawlScraper(firecrawl_config)
        self.api_client = HuoNiaoAPIClient(api_config)
        self.logger = logging.getLogger(__name__)
        
        # å‘å¸ƒç»Ÿè®¡
        self.stats = {
            'total_processed': 0,
            'successful_publishes': 0,
            'failed_publishes': 0,
            'skipped_items': 0
        }
    
    def scrape_and_publish(self, url: str, auto_publish: bool = False) -> PublishResponse:
        """æŠ“å–é¡µé¢å¹¶å‘å¸ƒåˆ°API"""
        try:
            self.stats['total_processed'] += 1
            
            # æŠ“å–æ•°æ®
            raw_data = self.scraper.scrape_single_page(url)
            
            if not raw_data:
                self.logger.warning(f"æŠ“å–å¤±è´¥ï¼Œè·³è¿‡å‘å¸ƒ: {url}")
                self.stats['skipped_items'] += 1
                return PublishResponse(
                    success=False,
                    message="æŠ“å–å¤±è´¥",
                    error_code="SCRAPING_ERROR"
                )
            
            # åˆ›å»ºå‘å¸ƒè¯·æ±‚
            publish_request = PublishRequest(
                title=raw_data.get('title', 'æ— æ ‡é¢˜'),
                content=raw_data.get('content', ''),
                summary=raw_data.get('metadata', {}).get('description', ''),
                source_url=url,
                status=PublishStatus.PUBLISHED if auto_publish else self.api_config.default_status,
                metadata={
                    'scraped_at': [datetime.now](http://datetime.now)(timezone.utc).isoformat(),
                    'firecrawl_source': True,
                    'original_url': url
                }
            )
            
            # å‘å¸ƒæ–‡ç« 
            response = self.api_client.publish_article(publish_request)
            
            if response.success:
                self.stats['successful_publishes'] += 1
                [self.logger.info](http://self.logger.info)(f"æ–‡ç« å‘å¸ƒæˆåŠŸ: {publish_request.title} (ID: {response.article_id})")
            else:
                self.stats['failed_publishes'] += 1
                self.logger.error(f"æ–‡ç« å‘å¸ƒå¤±è´¥: {response.message}")
            
            return response
            
        except Exception as e:
            self.stats['failed_publishes'] += 1
            self.logger.error(f"æŠ“å–å’Œå‘å¸ƒå¤±è´¥: {str(e)}")
            return PublishResponse(
                success=False,
                message=f"æŠ“å–å’Œå‘å¸ƒå¤±è´¥: {str(e)}",
                error_code="INTEGRATION_ERROR"
            )
    
    def batch_scrape_and_publish(self, urls: List[str], auto_publish: bool = False) -> List[PublishResponse]:
        """æ‰¹é‡æŠ“å–å’Œå‘å¸ƒ"""
        responses = []
        
        for i, url in enumerate(urls):
            [self.logger.info](http://self.logger.info)(f"æ‰¹é‡å¤„ç†è¿›åº¦: {i + 1}/{len(urls)} - {url}")
            response = self.scrape_and_publish(url, auto_publish)
            responses.append(response)
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¿‡åº¦è¯·æ±‚
            time.sleep(1)
        
        return responses
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if stats['total_processed'] > 0:
            stats['success_rate'] = stats['successful_publishes'] / stats['total_processed']
            stats['failure_rate'] = stats['failed_publishes'] / stats['total_processed']
            stats['skip_rate'] = stats['skipped_items'] / stats['total_processed']
        else:
            stats['success_rate'] = 0.0
            stats['failure_rate'] = 0.0
            stats['skip_rate'] = 0.0
        
        return stats

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=[logging.INFO](http://logging.INFO),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Firecrawlé…ç½®
    scraping_config = ScrapingConfig(
        api_key="your_firecrawl_api_key_here",
        output_format=["markdown", "html"],
        max_pages=5
    )
    
    # APIé…ç½®
    api_config = APIConfig(
        base_url="[https://api.huoniao.com](https://api.huoniao.com)",
        api_key="your_huoniao_api_key_here",
        default_category_id=1,
        default_author_id=1,
        default_status=PublishStatus.DRAFT
    )
    
    # åˆ›å»ºé›†æˆå®ä¾‹
    integration = APIIntegration(scraping_config, api_config)
    
    # æµ‹è¯•è¿æ¥
    if integration.api_client.test_connection():
        print("APIè¿æ¥æˆåŠŸ")
        
        # æŠ“å–å¹¶å‘å¸ƒå•ä¸ªé¡µé¢
        response = integration.scrape_and_publish("[https://example.com](https://example.com)", auto_publish=False)
        
        if response.success:
            print(f"æ–‡ç« å‘å¸ƒæˆåŠŸï¼ŒID: {response.article_id}")
        else:
            print(f"æ–‡ç« å‘å¸ƒå¤±è´¥: {response.message}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = integration.get_statistics()
        print("\nç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    else:
        print("APIè¿æ¥å¤±è´¥")
```

## ä½¿ç”¨è¯´æ˜

### 1. å®‰è£…ä¾èµ–

```bash
pip install firecrawl-py
```

### 2. é…ç½®APIå¯†é’¥

è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export FIRECRAWL_API_KEY="your-api-key-here"
```

æˆ–åœ¨è„šæœ¬ä¸­ç›´æ¥ä¿®æ”¹ `API_KEY` å˜é‡ã€‚

### 3. è¿è¡Œè„šæœ¬

```bash
python firecrawl_
```

## é…ç½®é€‰é¡¹

- **`output_format`** - è¾“å‡ºæ ¼å¼ï¼š`["markdown", "html", "structured"]`
- **`max_pages`** - æœ€å¤§çˆ¬å–é¡µé¢æ•°
- **`include_tags`** - åŒ…å«çš„HTMLæ ‡ç­¾
- **`exclude_tags`** - æ’é™¤çš„HTMLæ ‡ç­¾
- **`wait_for`** - ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

## è¾“å‡ºæ–‡ä»¶

æ‰€æœ‰ç»“æœæ–‡ä»¶å°†ä¿å­˜åœ¨ `output/` ç›®å½•ä¸­ï¼š

- `single_page_result.json` - å•é¡µé¢æŠ“å–ç»“æœ
- `single_page_[content.md](http://content.md)` - å•é¡µé¢Markdownå†…å®¹
- `website_crawl_results.json` - ç½‘ç«™çˆ¬å–ç»“æœ
- `structured_data.json` - ç»“æ„åŒ–æ•°æ®æå–ç»“æœ

## ç›¸å…³ Firecrawl é¡¹ç›®

åŸºäº [Firecrawl GitHub ç»„ç»‡](https://github.com/orgs/firecrawl/repositories?type=all)çš„å®˜æ–¹å’Œç¤¾åŒºé¡¹ç›®ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦çš„ç›¸å…³ä»“åº“ï¼š

### æ ¸å¿ƒé¡¹ç›®

- [**firecrawl**](https://github.com/firecrawl/firecrawl) â­ 52k
    
    ä¸»é¡¹ç›® - é¢å‘AIçš„Webæ•°æ®APIï¼Œå°†æ•´ä¸ªç½‘ç«™è½¬æ¢ä¸ºLLMå°±ç»ªçš„markdownæˆ–ç»“æ„åŒ–æ•°æ®
    
    `TypeScript` â€¢ `GNU AGPL v3.0`
    
- [**firecrawl-mcp-server**](https://github.com/firecrawl/firecrawl-mcp-server) â­ 4.3k
    
    å®˜æ–¹ Firecrawl MCP æœåŠ¡å™¨ - ä¸º Cursorã€Claude å’Œå…¶ä»–LLMå®¢æˆ·ç«¯æ·»åŠ å¼ºå¤§çš„ç½‘é¡µæŠ“å–åŠŸèƒ½
    
    `JavaScript` â€¢ `MIT License`
    
- [**firecrawl-docs**](https://github.com/firecrawl/firecrawl-docs)
    
    Firecrawl å®˜æ–¹æ–‡æ¡£
    
    `MDX`
    

### SDK å’Œé›†æˆ

- [**firecrawl-java-sdk**](https://github.com/firecrawl/firecrawl-java-sdk)
    
    Java SDK
    
    `Java` â€¢ `MIT License`
    
- [**firecrawl-go**](https://github.com/firecrawl/firecrawl-go)
    
    Go SDK
    
    `Go` â€¢ `MIT License`
    
- [**n8n-nodes-firecrawl**](https://github.com/firecrawl/n8n-nodes-firecrawl)
    
    n8n èŠ‚ç‚¹é›†æˆ
    
    `TypeScript` â€¢ `MIT License`
    

### åº”ç”¨ç¤ºä¾‹å’Œå·¥å…·

- [**firecrawl-app-examples**](https://github.com/firecrawl/firecrawl-app-examples) â­ 520
    
    å®Œæ•´çš„åº”ç”¨ç¤ºä¾‹é›†åˆï¼ŒåŒ…æ‹¬ä½¿ç”¨ Firecrawl å¼€å‘çš„ç½‘ç«™å’Œå…¶ä»–é¡¹ç›®
    
    `Jupyter Notebook`
    
- [**fireplexity**](https://github.com/firecrawl/fireplexity) â­ 1.4k
    
    åŸºäº Firecrawl çš„è¶…å¿«AIæœç´¢å¼•æ“ï¼Œæ”¯æŒå®æ—¶å¼•ç”¨ã€æµå¼å“åº”å’Œå®æ—¶æ•°æ®
    
    `TypeScript`
    
- [**open-lovable**](https://github.com/firecrawl/open-lovable) â­ 17k
    
    åœ¨å‡ ç§’é’Ÿå†…å…‹éš†å¹¶é‡æ–°åˆ›å»ºä»»ä½•ç½‘ç«™ä¸ºç°ä»£Reactåº”ç”¨
    
    `TypeScript` â€¢ `MIT License`
    

### ä¸“ä¸šå·¥å…·

- [**firecrawl-observer**](https://github.com/firecrawl/firecrawl-observer) â­ 279
    
    ä½¿ç”¨ Firecrawl å¼ºå¤§çš„å˜åŒ–æ£€æµ‹åŠŸèƒ½ç›‘æ§ç½‘ç«™å˜åŒ–
    
    `TypeScript`
    
- [**fire-enrich**](https://github.com/firecrawl/fire-enrich) â­ 717
    
    AIé©±åŠ¨çš„æ•°æ®ä¸°å¯Œå·¥å…·ï¼Œå°†é‚®ä»¶è½¬æ¢ä¸ºåŒ…å«å…¬å¸ç®€ä»‹ã€èµ„é‡‘æ•°æ®ã€æŠ€æœ¯æ ˆç­‰çš„ä¸°å¯Œæ•°æ®é›†
    
    `TypeScript` â€¢ `MIT License`
    
- [**firesearch**](https://github.com/firecrawl/firesearch) â­ 351
    
    AIé©±åŠ¨çš„æ·±åº¦ç ”ç©¶å·¥å…·ï¼Œä½¿ç”¨ Firecrawl å’Œ LangGraph æä¾›å¼•ç”¨çš„ç»¼åˆç»“æœ
    
    `TypeScript`
    
- [**firestarter**](https://github.com/firecrawl/firestarter) â­ 463
    
    ä¸ºä»»ä½•ç½‘ç«™å³æ—¶åˆ›å»ºå…·æœ‰RAGæœç´¢åŠŸèƒ½çš„AIèŠå¤©æœºå™¨äºº
    
    `TypeScript`
    

### å®ç”¨å·¥å…·

- [**llmstxt-generator**](https://github.com/firecrawl/llmstxt-generator) â­ 457
    
    LLMs.txt ç”Ÿæˆå™¨
    
    `TypeScript`
    
- [**open-researcher**](https://github.com/firecrawl/open-researcher) â­ 236
    
    å¯è§†åŒ–AIç ”ç©¶åŠ©æ‰‹ï¼Œæ˜¾ç¤ºå®æ—¶æ€è€ƒè¿‡ç¨‹
    
    `TypeScript`
    
- [**firecrawl-migrator**](https://github.com/firecrawl/firecrawl-migrator)
    
    è¿ç§»å·¥å…·
    
    `TypeScript`
    

> ğŸ’¡ **æç¤ºï¼š** è¿™äº›é¡¹ç›®å±•ç¤ºäº† Firecrawl ç”Ÿæ€ç³»ç»Ÿçš„ä¸°å¯Œæ€§ï¼Œæ¶µç›–äº†ä»åŸºç¡€SDKåˆ°å¤æ‚åº”ç”¨çš„å„ç§ç”¨ä¾‹ã€‚æ‚¨å¯ä»¥å‚è€ƒè¿™äº›é¡¹ç›®æ¥æ‰©å±•æ‚¨çš„çˆ¬è™«è„šæœ¬åŠŸèƒ½ã€‚
> 

---

> **æ³¨æ„ï¼š** ä½¿ç”¨å‰è¯·ç¡®ä¿å·²è·å–æœ‰æ•ˆçš„ Firecrawl API å¯†é’¥ã€‚
> 

## ä»£ç åº“

ä»¥ä¸‹æ˜¯ Firecrawl SDK çˆ¬è™«è„šæœ¬çš„å®Œæ•´ä»£ç å®ç°:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Firecrawl SDK çˆ¬è™«è„šæœ¬
åŠŸèƒ½ï¼šç½‘é¡µæŠ“å–ã€æ‰¹é‡çˆ¬å–ã€ç»“æ„åŒ–æ•°æ®æå–
ä½œè€…ï¼šTrae IDE Agent
åˆ›å»ºæ—¶é—´ï¼š2025-01-17
"""

import os
import json
import time
import argparse
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from urllib.parse import urlparse
from firecrawl import FirecrawlApp

@dataclass
class ScrapingConfig:
    """çˆ¬å–é…ç½®ç±»"""
    api_key: str
    output_format: List[str] = None  # ["markdown", "html", "structured"]
    max_pages: int = 10
    include_tags: List[str] = None
    exclude_tags: List[str] = None
    wait_for: int = 0  # ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    
    def __post_init__(self):
        if self.output_format is None:
            self.output_format = ["markdown"]
        if self.include_tags is None:
            self.include_tags = []
        if self.exclude_tags is None:
            self.exclude_tags = ["nav", "footer", "script", "style"]

class FirecrawlScraper:
    """Firecrawl çˆ¬è™«ç±»"""
    
    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.client = FirecrawlApp(api_key=config.api_key)
        self.output_dir = "output"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def scrape_single_page(self, url: str) -> Dict[str, Any]:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        print(f"æ­£åœ¨æŠ“å–é¡µé¢: {url}")
        
        result = self.client.fetch(
            url=url,
            output_format=self.config.output_format,
            include_tags=self.config.include_tags,
            exclude_tags=self.config.exclude_tags,
            wait_for=self.config.wait_for
        )
        
        # ä¿å­˜ç»“æœ
        with open(f"{self.output_dir}/single_page_result.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # å¦‚æœæœ‰markdownæ ¼å¼ï¼Œä¿å­˜ä¸ºmdæ–‡ä»¶
        if "markdown" in self.config.output_format and "markdown" in result:
            with open(f"{self.output_dir}/single_page_content.md", "w", encoding="utf-8") as f:
                f.write(result["markdown"])
        
        return result
    
    def crawl_website(self, start_url: str, depth: int = 1) -> List[Dict[str, Any]]:
        """çˆ¬å–æ•´ä¸ªç½‘ç«™"""
        print(f"æ­£åœ¨çˆ¬å–ç½‘ç«™: {start_url}ï¼Œæ·±åº¦: {depth}")
        
        domain = urlparse(start_url).netloc
        results = []
        
        crawl_result = self.client.crawl(
            url=start_url,
            max_pages=self.config.max_pages,
            max_depth=depth,
            same_domain=True,
            output_format=self.config.output_format,
            include_tags=self.config.include_tags,
            exclude_tags=self.config.exclude_tags
        )
        
        results.extend(crawl_result)
        
        # ä¿å­˜ç»“æœ
        with open(f"{self.output_dir}/website_crawl_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results
    
    def extract_structured_data(self, url: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ®"""
        print(f"æ­£åœ¨ä» {url} æå–ç»“æ„åŒ–æ•°æ®")
        
        result = self.client.extract(
            url=url,
            schema=schema,
            wait_for=self.config.wait_for
        )
        
        # ä¿å­˜ç»“æœ
        with open(f"{self.output_dir}/structured_data.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        return result
    
    def batch_process(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
        results = []
        for url in urls:
            try:
                result = self.scrape_single_page(url)
                results.append({
                    "url": url,
                    "success": True,
                    "data": result
                })
            except Exception as e:
                results.append({
                    "url": url,
                    "success": False,
                    "error": str(e)
                })
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Firecrawlçˆ¬è™«è„šæœ¬")
    parser.add_argument("--url", type=str, help="è¦æŠ“å–çš„URL")
    parser.add_argument("--mode", type=str, default="single", 
                        choices=["single", "crawl", "structured", "batch"],
                        help="æŠ“å–æ¨¡å¼: single(å•é¡µé¢), crawl(ç½‘ç«™çˆ¬å–), structured(ç»“æ„åŒ–æ•°æ®), batch(æ‰¹é‡å¤„ç†)")
    parser.add_argument("--depth", type=int, default=1, help="çˆ¬å–æ·±åº¦")
    parser.add_argument("--max-pages", type=int, default=10, help="æœ€å¤§çˆ¬å–é¡µé¢æ•°")
    parser.add_argument("--url-list", type=str, help="URLåˆ—è¡¨æ–‡ä»¶è·¯å¾„(ç”¨äºæ‰¹é‡å¤„ç†)")
    parser.add_argument("--schema", type=str, help="ç»“æ„åŒ–æ•°æ®æå–schemaæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è·å–APIå¯†é’¥
    api_key = os.getenv("FIRECRAWL_API_KEY")
    if not api_key:
        api_key = input("è¯·è¾“å…¥æ‚¨çš„Firecrawl APIå¯†é’¥: ")
    
    config = ScrapingConfig(
        api_key=api_key,
        output_format=["markdown", "html", "structured"],
        max_pages=args.max_pages
    )
    
    scraper = FirecrawlScraper(config)
    
    if args.mode == "single":
        if not args.url:
            args.url = input("è¯·è¾“å…¥è¦æŠ“å–çš„URL: ")
        scraper.scrape_single_page(args.url)
    
    elif args.mode == "crawl":
        if not args.url:
            args.url = input("è¯·è¾“å…¥èµ·å§‹URL: ")
        scraper.crawl_website(args.url, args.depth)
    
    elif args.mode == "structured":
        if not args.url:
            args.url = input("è¯·è¾“å…¥è¦æŠ“å–çš„URL: ")
        if not args.schema:
            args.schema = input("è¯·è¾“å…¥schemaæ–‡ä»¶è·¯å¾„: ")
        
        with open(args.schema, "r", encoding="utf-8") as f:
            schema = json.load(f)
        
        scraper.extract_structured_data(args.url, schema)
    
    elif args.mode == "batch":
        if not args.url_list:
            args.url_list = input("è¯·è¾“å…¥URLåˆ—è¡¨æ–‡ä»¶è·¯å¾„: ")
        
        with open(args.url_list, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
        
        scraper.batch_process(urls)

if __name__ == "__main__":
    main()

```

### ç¤ºä¾‹ Schema æ–‡ä»¶

```json
{
  "title": {
    "selector": "h1, .article-title, .entry-title",
    "type": "text"
  },
  "content": {
    "selector": "article, .post-content, .entry-content",
    "type": "html"
  },
  "author": {
    "selector": ".author, .byline",
    "type": "text"
  },
  "published_date": {
    "selector": ".date, time, .published",
    "type": "text"
  },
  "categories": {
    "selector": ".categories a, .tags a",
    "type": "list"
  },
  "images": {
    "selector": "img",
    "type": "list",
    "attribute": "src"
  }
}
```

### URL åˆ—è¡¨æ–‡ä»¶ç¤ºä¾‹

```
https://example.com/page1
https://example.com/page2
https://example.com/blog/article1
https://example.com/blog/article2
```

### ä½¿ç”¨ç¤ºä¾‹

```bash
# å•é¡µé¢æŠ“å–
python firecrawl_scraper.py --mode single --url https://example.com

# ç½‘ç«™çˆ¬å–
python firecrawl_scraper.py --mode crawl --url https://example.com --depth 2 --max-pages 20

# ç»“æ„åŒ–æ•°æ®æå–
python firecrawl_scraper.py --mode structured --url https://example.com/article --schema schema.json

# æ‰¹é‡å¤„ç†
python firecrawl_scraper.py --mode batch --url-list urls.txt
```