# ç½‘ç»œçˆ¬è™«Top 10å·¥å…·å®Œæ•´æŒ‡å—

## ğŸ“‹ æ–‡ç« ä¿¡æ¯

- **ä½œè€…**: Mendel
- **å‘å¸ƒæ—¶é—´**: 2024å¹´12æœˆ17æ—¥
- **åŸæ–‡é“¾æ¥**: https://www.firecrawl.dev/blog/top-10-tools-for-web-scraping
- **åˆ†ç±»**: å·¥å…·æŒ‡å—
- **æ ‡ç­¾**: ç½‘ç»œçˆ¬è™«, å·¥å…·å¯¹æ¯”, æ•°æ®é‡‡é›†, è‡ªåŠ¨åŒ–

## ğŸ“ æ‘˜è¦

æœ¬æ–‡å…¨é¢ä»‹ç»äº†2024å¹´æœ€ä¼˜ç§€çš„10æ¬¾ç½‘ç»œçˆ¬è™«å·¥å…·ï¼Œæ¶µç›–AIé©±åŠ¨å·¥å…·ã€æ— ä»£ç å¹³å°ã€Pythonåº“å’Œæµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶å››å¤§ç±»åˆ«ã€‚æ¯ç§å·¥å…·éƒ½åŒ…å«è¯¦ç»†çš„åŠŸèƒ½ä»‹ç»ã€ä¼˜ç¼ºç‚¹åˆ†æã€å®šä»·ä¿¡æ¯å’Œå®é™…ä»£ç ç¤ºä¾‹ï¼Œå¸®åŠ©å¼€å‘è€…æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„çˆ¬è™«è§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ ä¸»è¦å†…å®¹

### å·¥å…·åˆ†ç±»æ¦‚è§ˆ

ç½‘ç»œçˆ¬è™«å·¥å…·å¯ä»¥åˆ†ä¸ºå››å¤§ç±»åˆ«ï¼š

1. **AIé©±åŠ¨å·¥å…·** - æ™ºèƒ½åŒ–æ•°æ®æå–
2. **æ— ä»£ç /ä½ä»£ç å¹³å°** - å¯è§†åŒ–æ“ä½œç•Œé¢
3. **Pythonåº“** - ç¼–ç¨‹çµæ´»æ€§é«˜
4. **æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶** - å¤„ç†å¤æ‚äº¤äº’

---

## ğŸ¤– AIé©±åŠ¨å·¥å…·

### 1. Firecrawl

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **AIé©±åŠ¨æå–**: æ™ºèƒ½è¯†åˆ«å’Œæå–ç»“æ„åŒ–æ•°æ®
- **å¤šæ ¼å¼æ”¯æŒ**: Markdownã€HTMLã€JSONç­‰å¤šç§è¾“å‡ºæ ¼å¼
- **åçˆ¬è™«å¤„ç†**: è‡ªåŠ¨å¤„ç†éªŒè¯ç å’Œåçˆ¬è™«æœºåˆ¶
- **æ‰¹é‡å¤„ç†**: æ”¯æŒå¤§è§„æ¨¡ç½‘ç«™çˆ¬å–
- **å®æ—¶API**: RESTful APIæ¥å£ï¼Œæ˜“äºé›†æˆ

#### ğŸ’° å®šä»·
- **å…è´¹å¥—é¤**: 500æ¬¡è¯·æ±‚/æœˆ
- **Starter**: $20/æœˆï¼Œ5,000æ¬¡è¯·æ±‚
- **Standard**: $50/æœˆï¼Œ20,000æ¬¡è¯·æ±‚
- **Scale**: $200/æœˆï¼Œ100,000æ¬¡è¯·æ±‚

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
from firecrawl import FirecrawlApp

# åˆå§‹åŒ–å®¢æˆ·ç«¯
app = FirecrawlApp(api_key="your-api-key")

# å•é¡µçˆ¬å–
result = app.scrape_url(
    url="https://example.com",
    params={
        "formats": ["markdown", "html"],
        "only_main_content": True,
        "include_tags": ["title", "meta", "article"]
    }
)

# æ‰¹é‡çˆ¬å–
crawl_result = app.crawl_url(
    url="https://example.com",
    params={
        "limit": 100,
        "scrape_options": {
            "formats": ["markdown"]
        }
    }
)

# æ™ºèƒ½æœç´¢
search_result = app.search(
    query="AI technology trends 2024",
    limit=10,
    scrape_options={
        "formats": ["markdown"]
    }
)

# ç»“æ„åŒ–æ•°æ®æå–
extract_result = app.extract(
    urls=["https://example.com/product1", "https://example.com/product2"],
    schema={
        "type": "object",
        "properties": {
            "product_name": {"type": "string"},
            "price": {"type": "number"},
            "description": {"type": "string"},
            "availability": {"type": "boolean"}
        }
    },
    prompt="Extract product information from e-commerce pages"
)

print(f"æå–çš„äº§å“æ•°æ®: {extract_result}")
```

#### âœ… ä¼˜åŠ¿
- æ™ºèƒ½åŒ–ç¨‹åº¦é«˜ï¼Œå‡å°‘æ‰‹åŠ¨é…ç½®
- å¼ºå¤§çš„åçˆ¬è™«èƒ½åŠ›
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
- äº‘ç«¯å¤„ç†ï¼Œæ— éœ€ç»´æŠ¤åŸºç¡€è®¾æ–½
- ä¼˜ç§€çš„æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ

#### âŒ åŠ£åŠ¿
- æŒ‰ä½¿ç”¨é‡ä»˜è´¹ï¼Œå¤§è§„æ¨¡ä½¿ç”¨æˆæœ¬è¾ƒé«˜
- ä¾èµ–ç½‘ç»œè¿æ¥
- å®šåˆ¶åŒ–ç¨‹åº¦ç›¸å¯¹æœ‰é™

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- æ–°é—»èµ„è®¯é‡‡é›†
- ç”µå•†æ•°æ®ç›‘æ§
- å­¦æœ¯èµ„æ–™æ”¶é›†
- ç«å“åˆ†æ
- å†…å®¹èšåˆå¹³å°

---

### 2. Scrapfly

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½ä»£ç†è½®æ¢**: è‡ªåŠ¨å¤„ç†IPå°ç¦
- **JavaScriptæ¸²æŸ“**: æ”¯æŒSPAåº”ç”¨çˆ¬å–
- **æ•°æ®æ¸…æ´—**: è‡ªåŠ¨å»é™¤å¹¿å‘Šå’Œæ— å…³å†…å®¹
- **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§çˆ¬å–çŠ¶æ€
- **APIä¼˜å…ˆ**: å®Œæ•´çš„RESTful API

#### ğŸ’° å®šä»·
- **å…è´¹å¥—é¤**: 1,000æ¬¡è¯·æ±‚/æœˆ
- **Micro**: $15/æœˆï¼Œ10,000æ¬¡è¯·æ±‚
- **Startup**: $75/æœˆï¼Œ100,000æ¬¡è¯·æ±‚
- **Business**: $300/æœˆï¼Œ500,000æ¬¡è¯·æ±‚

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
import requests
from scrapfly import ScrapeConfig, ScrapflyClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = ScrapflyClient(key="your-api-key")

# åŸºç¡€çˆ¬å–
config = ScrapeConfig(
    url="https://example.com",
    render_js=True,  # å¯ç”¨JavaScriptæ¸²æŸ“
    proxy_pool="public_residential_pool",  # ä½¿ç”¨ä½å®…ä»£ç†
    country="US",  # æŒ‡å®šä»£ç†å›½å®¶
    format="clean_html",  # æ¸…æ´—HTML
    cache=True,  # å¯ç”¨ç¼“å­˜
    session="my_session"  # ä¼šè¯ç®¡ç†
)

result = client.scrape(config)

# å¤„ç†ç»“æœ
print(f"çŠ¶æ€ç : {result.status_code}")
print(f"å†…å®¹é•¿åº¦: {len(result.content)}")
print(f"ä½¿ç”¨çš„ä»£ç†: {result.context['proxy_used']}")

# æ‰¹é‡çˆ¬å–
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]

configs = [
    ScrapeConfig(
        url=url,
        render_js=True,
        proxy_pool="public_datacenter_pool"
    ) for url in urls
]

# å¹¶å‘çˆ¬å–
results = client.concurrent_scrape(configs)

for i, result in enumerate(results):
    print(f"é¡µé¢ {i+1}: {result.status_code}")
    
    # ä¿å­˜ç»“æœ
    with open(f"page_{i+1}.html", "w", encoding="utf-8") as f:
        f.write(result.content)
```

#### âœ… ä¼˜åŠ¿
- å¼ºå¤§çš„ä»£ç†ç®¡ç†åŠŸèƒ½
- ä¼˜ç§€çš„JavaScriptæ¸²æŸ“æ”¯æŒ
- è‡ªåŠ¨æ•°æ®æ¸…æ´—
- è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡
- çµæ´»çš„å®šä»·æ–¹æ¡ˆ

#### âŒ åŠ£åŠ¿
- å­¦ä¹ æ›²çº¿ç›¸å¯¹é™¡å³­
- é«˜çº§åŠŸèƒ½éœ€è¦ä»˜è´¹
- æ–‡æ¡£ç›¸å¯¹ç®€å•

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å¤§è§„æ¨¡æ•°æ®é‡‡é›†
- éœ€è¦ä»£ç†è½®æ¢çš„é¡¹ç›®
- JavaScripté‡åº¦åº”ç”¨çˆ¬å–
- å•†ä¸šæ•°æ®ç›‘æ§

---

## ğŸ¨ æ— ä»£ç /ä½ä»£ç å¹³å°

### 3. Octoparse

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **å¯è§†åŒ–æ“ä½œ**: ç‚¹å‡»å¼é…ç½®ï¼Œæ— éœ€ç¼–ç¨‹
- **äº‘ç«¯æ‰§è¡Œ**: 24/7è‡ªåŠ¨è¿è¡Œ
- **æ•°æ®å¯¼å‡º**: æ”¯æŒExcelã€CSVã€JSONç­‰æ ¼å¼
- **å®šæ—¶ä»»åŠ¡**: çµæ´»çš„è°ƒåº¦ç³»ç»Ÿ
- **æ¨¡æ¿åº“**: é¢„ç½®å¸¸ç”¨ç½‘ç«™æ¨¡æ¿

#### ğŸ’° å®šä»·
- **å…è´¹ç‰ˆ**: æœ‰é™åŠŸèƒ½ï¼Œæœ¬åœ°è¿è¡Œ
- **Standard**: $75/æœˆï¼Œäº‘ç«¯æ‰§è¡Œ
- **Professional**: $209/æœˆï¼Œé«˜çº§åŠŸèƒ½
- **Enterprise**: å®šåˆ¶ä»·æ ¼

#### ğŸ“ ä½¿ç”¨ç¤ºä¾‹
```python
# Octoparseä¸»è¦é€šè¿‡GUIæ“ä½œï¼Œä½†ä¹Ÿæä¾›API
import requests
import json

class OctoparseAPI:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.token = self._get_token()
    
    def _get_token(self):
        """è·å–è®¿é—®ä»¤ç‰Œ"""
        login_url = "https://dataapi.octoparse.com/api/token"
        data = {
            "username": self.username,
            "password": self.password,
            "grant_type": "password"
        }
        
        response = requests.post(login_url, data=data)
        if response.status_code == 200:
            return response.json()["access_token"]
        else:
            raise Exception("ç™»å½•å¤±è´¥")
    
    def get_task_list(self):
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        url = "https://dataapi.octoparse.com/api/task/gettasklist"
        headers = {"Authorization": f"Bearer {self.token}"}
        
        response = requests.get(url, headers=headers)
        return response.json()
    
    def start_task(self, task_id):
        """å¯åŠ¨ä»»åŠ¡"""
        url = "https://dataapi.octoparse.com/api/task/starttask"
        headers = {"Authorization": f"Bearer {self.token}"}
        data = {"taskId": task_id}
        
        response = requests.post(url, headers=headers, json=data)
        return response.json()
    
    def get_data(self, task_id, size=100, offset=0):
        """è·å–çˆ¬å–æ•°æ®"""
        url = "https://dataapi.octoparse.com/api/alldata/GetDataOfTaskByOffset"
        headers = {"Authorization": f"Bearer {self.token}"}
        params = {
            "taskId": task_id,
            "size": size,
            "offset": offset
        }
        
        response = requests.get(url, headers=headers, params=params)
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
api = OctoparseAPI("your_username", "your_password")

# è·å–ä»»åŠ¡åˆ—è¡¨
tasks = api.get_task_list()
print(f"ä»»åŠ¡æ•°é‡: {len(tasks['data'])}")

# å¯åŠ¨ç¬¬ä¸€ä¸ªä»»åŠ¡
if tasks['data']:
    task_id = tasks['data'][0]['taskId']
    result = api.start_task(task_id)
    print(f"ä»»åŠ¡å¯åŠ¨ç»“æœ: {result}")
    
    # è·å–æ•°æ®
    data = api.get_data(task_id)
    print(f"è·å–åˆ° {len(data['data'])} æ¡æ•°æ®")
```

#### âœ… ä¼˜åŠ¿
- é›¶ç¼–ç¨‹åŸºç¡€å³å¯ä½¿ç”¨
- ä¸°å¯Œçš„é¢„ç½®æ¨¡æ¿
- äº‘ç«¯è‡ªåŠ¨æ‰§è¡Œ
- ç›´è§‚çš„å¯è§†åŒ–ç•Œé¢
- è‰¯å¥½çš„å®¢æˆ·æ”¯æŒ

#### âŒ åŠ£åŠ¿
- å®šåˆ¶åŒ–èƒ½åŠ›æœ‰é™
- ä»·æ ¼ç›¸å¯¹è¾ƒé«˜
- å¤æ‚é€»è¾‘å¤„ç†å›°éš¾
- ä¾èµ–å¹³å°ç¨³å®šæ€§

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- éæŠ€æœ¯äººå‘˜æ•°æ®é‡‡é›†
- ç®€å•çš„å®šæœŸæ•°æ®ç›‘æ§
- å¿«é€ŸåŸå‹éªŒè¯
- å°è§„æ¨¡å•†ä¸šåº”ç”¨

---

### 4. ParseHub

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½é€‰æ‹©å™¨**: è‡ªåŠ¨è¯†åˆ«é¡µé¢å…ƒç´ 
- **æ¡ä»¶é€»è¾‘**: æ”¯æŒå¤æ‚çš„çˆ¬å–é€»è¾‘
- **APIé›†æˆ**: å®Œæ•´çš„REST API
- **å®æ—¶ç›‘æ§**: ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ç›‘æ§
- **æ•°æ®æ¸…æ´—**: å†…ç½®æ•°æ®å¤„ç†åŠŸèƒ½

#### ğŸ’° å®šä»·
- **å…è´¹ç‰ˆ**: 5ä¸ªé¡¹ç›®ï¼Œ200é¡µ/æœˆ
- **Standard**: $149/æœˆï¼Œ20ä¸ªé¡¹ç›®
- **Professional**: $499/æœˆï¼Œæ— é™é¡¹ç›®
- **Enterprise**: å®šåˆ¶ä»·æ ¼

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
import requests
import time
import json

class ParseHubAPI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://www.parsehub.com/api/v2"
    
    def get_projects(self):
        """è·å–é¡¹ç›®åˆ—è¡¨"""
        url = f"{self.base_url}/projects"
        params = {"api_key": self.api_key}
        
        response = requests.get(url, params=params)
        return response.json()
    
    def run_project(self, project_token, start_url=None, start_template=None):
        """è¿è¡Œé¡¹ç›®"""
        url = f"{self.base_url}/projects/{project_token}/run"
        data = {"api_key": self.api_key}
        
        if start_url:
            data["start_url"] = start_url
        if start_template:
            data["start_template"] = start_template
        
        response = requests.post(url, data=data)
        return response.json()
    
    def get_run_status(self, run_token):
        """è·å–è¿è¡ŒçŠ¶æ€"""
        url = f"{self.base_url}/runs/{run_token}"
        params = {"api_key": self.api_key}
        
        response = requests.get(url, params=params)
        return response.json()
    
    def get_run_data(self, run_token, format="json"):
        """è·å–è¿è¡Œæ•°æ®"""
        url = f"{self.base_url}/runs/{run_token}/data"
        params = {
            "api_key": self.api_key,
            "format": format
        }
        
        response = requests.get(url, params=params)
        if format == "json":
            return response.json()
        else:
            return response.text
    
    def wait_for_completion(self, run_token, max_wait=3600):
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            status = self.get_run_status(run_token)
            
            if status["status"] == "complete":
                return True
            elif status["status"] == "error":
                raise Exception(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {status.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
            print(f"ä»»åŠ¡çŠ¶æ€: {status['status']}, è¿›åº¦: {status.get('pages_run', 0)}/{status.get('total_pages', '?')}")
            time.sleep(30)  # ç­‰å¾…30ç§’åå†æ¬¡æ£€æŸ¥
        
        raise Exception("ä»»åŠ¡æ‰§è¡Œè¶…æ—¶")

# ä½¿ç”¨ç¤ºä¾‹
api = ParseHubAPI("your-api-key")

# è·å–é¡¹ç›®åˆ—è¡¨
projects = api.get_projects()
print(f"é¡¹ç›®æ•°é‡: {len(projects['projects'])}")

# è¿è¡Œç¬¬ä¸€ä¸ªé¡¹ç›®
if projects['projects']:
    project_token = projects['projects'][0]['token']
    
    # å¯åŠ¨çˆ¬å–ä»»åŠ¡
    run_result = api.run_project(
        project_token,
        start_url="https://example.com"
    )
    
    run_token = run_result['run_token']
    print(f"ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¿è¡Œä»¤ç‰Œ: {run_token}")
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
    try:
        api.wait_for_completion(run_token)
        print("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        
        # è·å–æ•°æ®
        data = api.get_run_data(run_token)
        print(f"è·å–åˆ° {len(data)} æ¡æ•°æ®")
        
        # ä¿å­˜æ•°æ®
        with open("parsehub_data.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
```

#### âœ… ä¼˜åŠ¿
- å¼ºå¤§çš„å¯è§†åŒ–ç¼–è¾‘å™¨
- æ”¯æŒå¤æ‚çš„æ¡ä»¶é€»è¾‘
- è‰¯å¥½çš„APIé›†æˆ
- å…è´¹ç‰ˆåŠŸèƒ½ç›¸å¯¹å®Œæ•´
- æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒ

#### âŒ åŠ£åŠ¿
- å­¦ä¹ æ›²çº¿è¾ƒé™¡
- é«˜çº§åŠŸèƒ½ä»·æ ¼æ˜‚è´µ
- æ‰§è¡Œé€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢
- å¯¹ä¸­æ–‡æ”¯æŒä¸€èˆ¬

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å¤æ‚ç½‘ç«™ç»“æ„çˆ¬å–
- éœ€è¦æ¡ä»¶åˆ¤æ–­çš„åœºæ™¯
- APIé›†æˆéœ€æ±‚
- ä¸­ç­‰è§„æ¨¡æ•°æ®é‡‡é›†

---

## ğŸ Pythonåº“

### 5. Scrapy

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **é«˜æ€§èƒ½æ¡†æ¶**: å¼‚æ­¥å¤„ç†ï¼Œé€Ÿåº¦æå¿«
- **å¯æ‰©å±•æ¶æ„**: ä¸°å¯Œçš„ä¸­é—´ä»¶å’Œæ‰©å±•
- **å†…ç½®åŠŸèƒ½**: å»é‡ã€ç¼“å­˜ã€é‡è¯•ç­‰
- **æ•°æ®ç®¡é“**: çµæ´»çš„æ•°æ®å¤„ç†æµæ°´çº¿
- **åˆ†å¸ƒå¼æ”¯æŒ**: æ”¯æŒåˆ†å¸ƒå¼çˆ¬å–

#### ğŸ’° å®šä»·
- **å®Œå…¨å…è´¹**: å¼€æºé¡¹ç›®ï¼Œæ— ä½¿ç”¨é™åˆ¶

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
import json

class ProductSpider(scrapy.Spider):
    name = 'products'
    allowed_domains = ['example-shop.com']
    start_urls = ['https://example-shop.com/products']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 1,  # è¯·æ±‚é—´éš”
        'CONCURRENT_REQUESTS': 16,  # å¹¶å‘è¯·æ±‚æ•°
        'AUTOTHROTTLE_ENABLED': True,  # è‡ªåŠ¨è°ƒèŠ‚
        'ROBOTSTXT_OBEY': True,  # éµå®ˆrobots.txt
        'USER_AGENT': 'Mozilla/5.0 (compatible; ProductBot/1.0)',
        
        # æ•°æ®ç®¡é“
        'ITEM_PIPELINES': {
            'myproject.pipelines.ValidationPipeline': 300,
            'myproject.pipelines.DuplicatesPipeline': 400,
            'myproject.pipelines.JsonWriterPipeline': 500,
        },
        
        # ä¸­é—´ä»¶
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
            'myproject.middlewares.RotateUserAgentMiddleware': 400,
            'myproject.middlewares.ProxyMiddleware': 410,
        }
    }
    
    def parse(self, response):
        """è§£æäº§å“åˆ—è¡¨é¡µ"""
        # æå–äº§å“é“¾æ¥
        product_links = response.css('.product-item a::attr(href)').getall()
        
        for link in product_links:
            yield response.follow(link, self.parse_product)
        
        # å¤„ç†åˆ†é¡µ
        next_page = response.css('.pagination .next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_product(self, response):
        """è§£æäº§å“è¯¦æƒ…é¡µ"""
        yield {
            'name': response.css('h1.product-title::text').get(),
            'price': response.css('.price .amount::text').re_first(r'\d+\.\d+'),
            'description': response.css('.product-description::text').getall(),
            'images': response.css('.product-gallery img::attr(src)').getall(),
            'availability': response.css('.stock-status::text').get(),
            'rating': response.css('.rating .stars::attr(data-rating)').get(),
            'reviews_count': response.css('.reviews-count::text').re_first(r'\d+'),
            'category': response.css('.breadcrumb a::text').getall()[:-1],
            'sku': response.css('.product-sku::text').get(),
            'url': response.url,
            'scraped_at': scrapy.utils.misc.datetime.now().isoformat()
        }

# è‡ªå®šä¹‰æ•°æ®ç®¡é“
class ValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""
    
    def process_item(self, item, spider):
        if not item.get('name') or not item.get('price'):
            raise scrapy.exceptions.DropItem(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {item}")
        
        # ä»·æ ¼æ ¼å¼åŒ–
        if item.get('price'):
            try:
                item['price'] = float(item['price'])
            except ValueError:
                item['price'] = None
        
        return item

class DuplicatesPipeline:
    """å»é‡ç®¡é“"""
    
    def __init__(self):
        self.ids_seen = set()
    
    def process_item(self, item, spider):
        item_id = item.get('sku') or item.get('url')
        if item_id in self.ids_seen:
            raise scrapy.exceptions.DropItem(f"é‡å¤é¡¹ç›®: {item_id}")
        else:
            self.ids_seen.add(item_id)
            return item

class JsonWriterPipeline:
    """JSONè¾“å‡ºç®¡é“"""
    
    def open_spider(self, spider):
        self.file = open(f'{spider.name}_items.json', 'w', encoding='utf-8')
        self.file.write('[')
        self.first_item = True
    
    def close_spider(self, spider):
        self.file.write(']')
        self.file.close()
    
    def process_item(self, item, spider):
        if not self.first_item:
            self.file.write(',')
        else:
            self.first_item = False
        
        line = json.dumps(dict(item), ensure_ascii=False, indent=2)
        self.file.write(line)
        return item

# è‡ªå®šä¹‰ä¸­é—´ä»¶
class RotateUserAgentMiddleware:
    """ç”¨æˆ·ä»£ç†è½®æ¢ä¸­é—´ä»¶"""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
    
    def process_request(self, request, spider):
        import random
        ua = random.choice(self.user_agents)
        request.headers['User-Agent'] = ua
        return None

# è¿è¡Œçˆ¬è™«
if __name__ == '__main__':
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (compatible; ProductBot/1.0)',
        'ROBOTSTXT_OBEY': True,
        'DOWNLOAD_DELAY': 1,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 16,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 60,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'LOG_LEVEL': 'INFO'
    })
    
    process.crawl(ProductSpider)
    process.start()
```

#### âœ… ä¼˜åŠ¿
- æ€§èƒ½æé«˜ï¼Œé€‚åˆå¤§è§„æ¨¡çˆ¬å–
- æ¶æ„çµæ´»ï¼Œå¯æ‰©å±•æ€§å¼º
- åŠŸèƒ½å®Œæ•´ï¼Œå†…ç½®å¸¸ç”¨åŠŸèƒ½
- ç¤¾åŒºæ´»è·ƒï¼Œæ–‡æ¡£è¯¦ç»†
- å®Œå…¨å…è´¹å¼€æº

#### âŒ åŠ£åŠ¿
- å­¦ä¹ æ›²çº¿é™¡å³­
- é…ç½®å¤æ‚
- éœ€è¦ç¼–ç¨‹åŸºç¡€
- è°ƒè¯•ç›¸å¯¹å›°éš¾

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å¤§è§„æ¨¡æ•°æ®é‡‡é›†
- å¤æ‚çˆ¬å–é€»è¾‘
- é«˜æ€§èƒ½è¦æ±‚
- å®šåˆ¶åŒ–éœ€æ±‚é«˜
- é•¿æœŸç»´æŠ¤é¡¹ç›®

---

### 6. Beautiful Soup + Requests

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **ç®€å•æ˜“ç”¨**: è¯­æ³•ç›´è§‚ï¼Œå­¦ä¹ æˆæœ¬ä½
- **å¼ºå¤§è§£æ**: æ”¯æŒå¤šç§è§£æå™¨
- **çµæ´»é€‰æ‹©**: CSSé€‰æ‹©å™¨å’ŒXPath
- **è½»é‡çº§**: ä¾èµ–å°‘ï¼Œéƒ¨ç½²ç®€å•
- **ç¨³å®šå¯é **: æˆç†Ÿçš„åº“ï¼Œbugå°‘

#### ğŸ’° å®šä»·
- **å®Œå…¨å…è´¹**: å¼€æºåº“ï¼Œæ— ä½¿ç”¨é™åˆ¶

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
import requests
from bs4 import BeautifulSoup
import json
import time
import random
from urllib.parse import urljoin, urlparse
import csv
from datetime import datetime

class SimpleScraper:
    """ç®€å•ç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, delay_range=(1, 3)):
        self.session = requests.Session()
        self.delay_range = delay_range
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_page(self, url, **kwargs):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, **kwargs)
            response.raise_for_status()
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(*self.delay_range)
            time.sleep(delay)
            
            return response
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
    
    def parse_html(self, html_content, parser='html.parser'):
        """è§£æHTMLå†…å®¹"""
        return BeautifulSoup(html_content, parser)
    
    def scrape_news_site(self, base_url, max_pages=5):
        """çˆ¬å–æ–°é—»ç½‘ç«™ç¤ºä¾‹"""
        articles = []
        
        for page in range(1, max_pages + 1):
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # æ„å»ºåˆ†é¡µURL
            page_url = f"{base_url}?page={page}"
            response = self.get_page(page_url)
            
            if not response:
                continue
            
            soup = self.parse_html(response.text)
            
            # æå–æ–‡ç« é“¾æ¥
            article_links = soup.find_all('a', class_='article-link')
            
            for link in article_links:
                article_url = urljoin(base_url, link.get('href'))
                article_data = self.scrape_article(article_url)
                
                if article_data:
                    articles.append(article_data)
        
        return articles
    
    def scrape_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        response = self.get_page(url)
        if not response:
            return None
        
        soup = self.parse_html(response.text)
        
        # æå–æ–‡ç« ä¿¡æ¯
        article = {
            'url': url,
            'title': self.safe_extract(soup, 'h1.article-title', 'text'),
            'author': self.safe_extract(soup, '.author-name', 'text'),
            'publish_date': self.safe_extract(soup, '.publish-date', 'text'),
            'content': self.extract_content(soup),
            'tags': self.extract_tags(soup),
            'images': self.extract_images(soup, url),
            'scraped_at': datetime.now().isoformat()
        }
        
        return article
    
    def safe_extract(self, soup, selector, attr='text'):
        """å®‰å…¨æå–å…ƒç´ å†…å®¹"""
        try:
            element = soup.select_one(selector)
            if element:
                if attr == 'text':
                    return element.get_text(strip=True)
                else:
                    return element.get(attr)
        except Exception as e:
            print(f"æå–å¤±è´¥ {selector}: {e}")
        return None
    
    def extract_content(self, soup):
        """æå–æ–‡ç« æ­£æ–‡"""
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content',
            'article .content'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # æ¸…ç†å†…å®¹
                for script in content_div(["script", "style"]):
                    script.decompose()
                
                paragraphs = content_div.find_all('p')
                return '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        return None
    
    def extract_tags(self, soup):
        """æå–æ ‡ç­¾"""
        tag_selectors = [
            '.tags a',
            '.post-tags a',
            '.article-tags .tag'
        ]
        
        for selector in tag_selectors:
            tags = soup.select(selector)
            if tags:
                return [tag.get_text(strip=True) for tag in tags]
        
        return []
    
    def extract_images(self, soup, base_url):
        """æå–å›¾ç‰‡é“¾æ¥"""
        images = []
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = urljoin(base_url, src)
                images.append({
                    'url': absolute_url,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
        
        return images
    
    def scrape_ecommerce_products(self, category_url, max_products=100):
        """çˆ¬å–ç”µå•†äº§å“ç¤ºä¾‹"""
        products = []
        page = 1
        
        while len(products) < max_products:
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µäº§å“...")
            
            page_url = f"{category_url}?page={page}"
            response = self.get_page(page_url)
            
            if not response:
                break
            
            soup = self.parse_html(response.text)
            product_links = soup.select('.product-item a')
            
            if not product_links:
                print("æ²¡æœ‰æ‰¾åˆ°æ›´å¤šäº§å“")
                break
            
            for link in product_links:
                if len(products) >= max_products:
                    break
                
                product_url = urljoin(category_url, link.get('href'))
                product_data = self.scrape_product(product_url)
                
                if product_data:
                    products.append(product_data)
            
            page += 1
        
        return products
    
    def scrape_product(self, url):
        """çˆ¬å–å•ä¸ªäº§å“"""
        response = self.get_page(url)
        if not response:
            return None
        
        soup = self.parse_html(response.text)
        
        # æå–ä»·æ ¼ï¼ˆå¤„ç†å¤šç§æ ¼å¼ï¼‰
        price_text = self.safe_extract(soup, '.price .amount', 'text')
        price = None
        if price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', price_text.replace(',', ''))
            if price_match:
                price = float(price_match.group())
        
        product = {
            'url': url,
            'name': self.safe_extract(soup, 'h1.product-title', 'text'),
            'price': price,
            'original_price': self.extract_original_price(soup),
            'description': self.safe_extract(soup, '.product-description', 'text'),
            'images': [img['src'] for img in soup.select('.product-gallery img') if img.get('src')],
            'availability': self.safe_extract(soup, '.stock-status', 'text'),
            'rating': self.extract_rating(soup),
            'reviews_count': self.extract_reviews_count(soup),
            'specifications': self.extract_specifications(soup),
            'scraped_at': datetime.now().isoformat()
        }
        
        return product
    
    def extract_original_price(self, soup):
        """æå–åŸä»·"""
        original_price_text = self.safe_extract(soup, '.price .original-price', 'text')
        if original_price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', original_price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_rating(self, soup):
        """æå–è¯„åˆ†"""
        rating_element = soup.select_one('.rating .stars')
        if rating_element:
            rating_text = rating_element.get('data-rating') or rating_element.get_text()
            try:
                return float(rating_text)
            except ValueError:
                pass
        return None
    
    def extract_reviews_count(self, soup):
        """æå–è¯„è®ºæ•°é‡"""
        reviews_text = self.safe_extract(soup, '.reviews-count', 'text')
        if reviews_text:
            import re
            count_match = re.search(r'\d+', reviews_text)
            if count_match:
                return int(count_match.group())
        return 0
    
    def extract_specifications(self, soup):
        """æå–äº§å“è§„æ ¼"""
        specs = {}
        spec_rows = soup.select('.specifications tr')
        
        for row in spec_rows:
            cells = row.select('td')
            if len(cells) >= 2:
                key = cells[0].get_text(strip=True)
                value = cells[1].get_text(strip=True)
                specs[key] = value
        
        return specs
    
    def save_to_json(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def save_to_csv(self, data, filename):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            return
        
        fieldnames = data[0].keys()
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    scraper = SimpleScraper(delay_range=(1, 2))
    
    # çˆ¬å–æ–°é—»ç½‘ç«™
    print("å¼€å§‹çˆ¬å–æ–°é—»...")
    news_articles = scraper.scrape_news_site('https://example-news.com', max_pages=3)
    scraper.save_to_json(news_articles, 'news_articles.json')
    
    # çˆ¬å–ç”µå•†äº§å“
    print("å¼€å§‹çˆ¬å–äº§å“...")
    products = scraper.scrape_ecommerce_products('https://example-shop.com/category/electronics', max_products=50)
    scraper.save_to_csv(products, 'products.csv')
    
    print(f"çˆ¬å–å®Œæˆï¼è·å¾— {len(news_articles)} ç¯‡æ–‡ç« å’Œ {len(products)} ä¸ªäº§å“")
```

#### âœ… ä¼˜åŠ¿
- å­¦ä¹ æˆæœ¬ä½ï¼Œé€‚åˆåˆå­¦è€…
- è¯­æ³•ç®€æ´ç›´è§‚
- æ–‡æ¡£è¯¦ç»†ï¼Œç¤¾åŒºæ”¯æŒå¥½
- è½»é‡çº§ï¼Œä¾èµ–å°‘
- è°ƒè¯•å®¹æ˜“

#### âŒ åŠ£åŠ¿
- æ€§èƒ½ç›¸å¯¹è¾ƒä½
- ä¸æ”¯æŒå¼‚æ­¥å¤„ç†
- ç¼ºå°‘é«˜çº§åŠŸèƒ½
- å¤§è§„æ¨¡çˆ¬å–æ•ˆç‡ä½

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å°è§„æ¨¡æ•°æ®é‡‡é›†
- å­¦ä¹ å’ŒåŸå‹å¼€å‘
- ç®€å•çš„ä¸€æ¬¡æ€§ä»»åŠ¡
- å¿«é€ŸéªŒè¯æƒ³æ³•
- æ•™å­¦æ¼”ç¤º

---

## ğŸŒ æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶

### 7. Selenium

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **çœŸå®æµè§ˆå™¨**: å®Œå…¨æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º
- **JavaScriptæ”¯æŒ**: å¤„ç†åŠ¨æ€å†…å®¹
- **å¤šæµè§ˆå™¨æ”¯æŒ**: Chromeã€Firefoxã€Safariç­‰
- **äº¤äº’èƒ½åŠ›**: ç‚¹å‡»ã€è¾“å…¥ã€æ»šåŠ¨ç­‰æ“ä½œ
- **æˆªå›¾åŠŸèƒ½**: é¡µé¢æˆªå›¾å’Œå…ƒç´ æˆªå›¾

#### ğŸ’° å®šä»·
- **å®Œå…¨å…è´¹**: å¼€æºé¡¹ç›®ï¼Œæ— ä½¿ç”¨é™åˆ¶

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import json
import csv
from datetime import datetime

class SeleniumScraper:
    """Seleniumç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, headless=True, window_size=(1920, 1080)):
        self.setup_driver(headless, window_size)
        self.wait = WebDriverWait(self.driver, 10)
    
    def setup_driver(self, headless, window_size):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')
        
        # åŸºç¡€è®¾ç½®
        chrome_options.add_argument(f'--window-size={window_size[0]},{window_size[1]}')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        
        # åæ£€æµ‹è®¾ç½®
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ç”¨æˆ·ä»£ç†
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    def get_page(self, url, wait_for_element=None, timeout=10):
        """è®¿é—®é¡µé¢å¹¶ç­‰å¾…åŠ è½½"""
        try:
            self.driver.get(url)
            
            if wait_for_element:
                self.wait_for_element(wait_for_element, timeout)
            
            return True
        except Exception as e:
            print(f"è®¿é—®é¡µé¢å¤±è´¥ {url}: {e}")
            return False
    
    def wait_for_element(self, selector, timeout=10, by=By.CSS_SELECTOR):
        """ç­‰å¾…å…ƒç´ å‡ºç°"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, selector))
            )
            return element
        except TimeoutException:
            print(f"ç­‰å¾…å…ƒç´ è¶…æ—¶: {selector}")
            return None
    
    def scroll_to_bottom(self, pause_time=1):
        """æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # æ»šåŠ¨åˆ°åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            time.sleep(pause_time)
            
            # è®¡ç®—æ–°çš„æ»šåŠ¨é«˜åº¦
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
            
            last_height = new_height
    
    def infinite_scroll(self, max_scrolls=10, pause_time=2):
        """æ— é™æ»šåŠ¨åŠ è½½"""
        for i in range(max_scrolls):
            print(f"æ»šåŠ¨ç¬¬ {i+1} æ¬¡...")
            
            # æ»šåŠ¨åˆ°åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            time.sleep(pause_time)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"åŠ è½½æ›´å¤š"æŒ‰é’®
            try:
                load_more_btn = self.driver.find_element(By.CSS_SELECTOR, '.load-more, .show-more, [data-load-more]')
                if load_more_btn.is_displayed():
                    self.driver.execute_script("arguments[0].click();", load_more_btn)
                    time.sleep(pause_time)
            except NoSuchElementException:
                pass
    
    def scrape_social_media_posts(self, profile_url, max_posts=50):
        """çˆ¬å–ç¤¾äº¤åª’ä½“å¸–å­ç¤ºä¾‹"""
        if not self.get_page(profile_url):
            return []
        
        posts = []
        
        # ç­‰å¾…å¸–å­åŠ è½½
        self.wait_for_element('.post, .tweet, .feed-item')
        
        # æ— é™æ»šåŠ¨åŠ è½½æ›´å¤šå¸–å­
        self.infinite_scroll(max_scrolls=10)
        
        # æå–å¸–å­
        post_elements = self.driver.find_elements(By.CSS_SELECTOR, '.post, .tweet, .feed-item')
        
        for i, post_element in enumerate(post_elements[:max_posts]):
            try:
                post_data = self.extract_post_data(post_element)
                if post_data:
                    posts.append(post_data)
                    print(f"å·²æå–ç¬¬ {i+1} ä¸ªå¸–å­")
            except Exception as e:
                print(f"æå–å¸–å­å¤±è´¥: {e}")
                continue
        
        return posts
    
    def extract_post_data(self, post_element):
        """æå–å•ä¸ªå¸–å­æ•°æ®"""
        try:
            # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
            self.driver.execute_script("arguments[0].scrollIntoView(true);", post_element)
            time.sleep(0.5)
            
            post_data = {
                'text': self.safe_get_text(post_element, '.post-text, .tweet-text, .content'),
                'author': self.safe_get_text(post_element, '.author, .username, .user-name'),
                'timestamp': self.safe_get_text(post_element, '.timestamp, .time, .date'),
                'likes': self.extract_count(post_element, '.likes, .like-count'),
                'shares': self.extract_count(post_element, '.shares, .retweet-count'),
                'comments': self.extract_count(post_element, '.comments, .reply-count'),
                'images': self.extract_images(post_element),
                'links': self.extract_links(post_element),
                'hashtags': self.extract_hashtags(post_element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return post_data
            
        except Exception as e:
            print(f"æå–å¸–å­æ•°æ®å¤±è´¥: {e}")
            return None
    
    def safe_get_text(self, parent_element, selector):
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
        try:
            element = parent_element.find_element(By.CSS_SELECTOR, selector)
            return element.text.strip()
        except NoSuchElementException:
            return None
    
    def extract_count(self, parent_element, selector):
        """æå–æ•°é‡ï¼ˆç‚¹èµã€åˆ†äº«ç­‰ï¼‰"""
        text = self.safe_get_text(parent_element, selector)
        if text:
            import re
            # å¤„ç†Kã€Mç­‰å•ä½
            if 'K' in text.upper():
                number = re.search(r'([\d.]+)K', text.upper())
                if number:
                    return int(float(number.group(1)) * 1000)
            elif 'M' in text.upper():
                number = re.search(r'([\d.]+)M', text.upper())
                if number:
                    return int(float(number.group(1)) * 1000000)
            else:
                number = re.search(r'\d+', text)
                if number:
                    return int(number.group())
        return 0
    
    def extract_images(self, parent_element):
        """æå–å›¾ç‰‡é“¾æ¥"""
        try:
            img_elements = parent_element.find_elements(By.CSS_SELECTOR, 'img')
            return [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]
        except:
            return []
    
    def extract_links(self, parent_element):
        """æå–é“¾æ¥"""
        try:
            link_elements = parent_element.find_elements(By.CSS_SELECTOR, 'a')
            return [link.get_attribute('href') for link in link_elements if link.get_attribute('href')]
        except:
            return []
    
    def extract_hashtags(self, parent_element):
        """æå–è¯é¢˜æ ‡ç­¾"""
        text = self.safe_get_text(parent_element, '.post-text, .tweet-text, .content')
        if text:
            import re
            hashtags = re.findall(r'#\w+', text)
            return hashtags
        return []
    
    def scrape_ecommerce_with_filters(self, base_url, filters=None, max_products=100):
        """çˆ¬å–ç”µå•†ç½‘ç«™ï¼ˆå¸¦ç­›é€‰åŠŸèƒ½ï¼‰"""
        if not self.get_page(base_url):
            return []
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if filters:
            self.apply_filters(filters)
        
        products = []
        page = 1
        
        while len(products) < max_products:
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # ç­‰å¾…äº§å“åŠ è½½
            self.wait_for_element('.product-item, .product-card')
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šäº§å“
            self.scroll_to_bottom()
            
            # æå–å½“å‰é¡µé¢çš„äº§å“
            product_elements = self.driver.find_elements(By.CSS_SELECTOR, '.product-item, .product-card')
            
            current_page_products = []
            for product_element in product_elements[len(products):]:
                if len(products) >= max_products:
                    break
                
                product_data = self.extract_product_data(product_element)
                if product_data:
                    products.append(product_data)
                    current_page_products.append(product_data)
            
            if not current_page_products:
                print("æ²¡æœ‰æ‰¾åˆ°æ–°äº§å“ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µ
            if not self.go_to_next_page():
                print("æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œçˆ¬å–å®Œæˆ")
                break
            
            page += 1
        
        return products
    
    def apply_filters(self, filters):
        """åº”ç”¨ç­›é€‰æ¡ä»¶"""
        for filter_type, filter_value in filters.items():
            try:
                if filter_type == 'price_range':
                    # è®¾ç½®ä»·æ ¼èŒƒå›´
                    min_price, max_price = filter_value
                    
                    min_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name="min_price"], .price-min')
                    min_input.clear()
                    min_input.send_keys(str(min_price))
                    
                    max_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name="max_price"], .price-max')
                    max_input.clear()
                    max_input.send_keys(str(max_price))
                    
                elif filter_type == 'category':
                    # é€‰æ‹©åˆ†ç±»
                    category_element = self.driver.find_element(By.XPATH, f"//a[contains(text(), '{filter_value}')]")
                    category_element.click()
                    
                elif filter_type == 'brand':
                    # é€‰æ‹©å“ç‰Œ
                    brand_checkbox = self.driver.find_element(By.XPATH, f"//input[@value='{filter_value}' or following-sibling::text()[contains(., '{filter_value}')]]")
                    if not brand_checkbox.is_selected():
                        brand_checkbox.click()
                
                time.sleep(1)  # ç­‰å¾…ç­›é€‰ç”Ÿæ•ˆ
                
            except NoSuchElementException:
                print(f"æœªæ‰¾åˆ°ç­›é€‰æ¡ä»¶: {filter_type} = {filter_value}")
    
    def extract_product_data(self, product_element):
        """æå–äº§å“æ•°æ®"""
        try:
            # æ»šåŠ¨åˆ°äº§å“ä½ç½®
            self.driver.execute_script("arguments[0].scrollIntoView(true);", product_element)
            time.sleep(0.3)
            
            product_data = {
                'name': self.safe_get_text(product_element, '.product-name, .title, h3'),
                'price': self.extract_price(product_element),
                'original_price': self.extract_original_price(product_element),
                'rating': self.extract_rating(product_element),
                'reviews_count': self.extract_reviews_count(product_element),
                'image': self.safe_get_attribute(product_element, 'img', 'src'),
                'link': self.safe_get_attribute(product_element, 'a', 'href'),
                'availability': self.safe_get_text(product_element, '.stock, .availability'),
                'discount': self.extract_discount(product_element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return product_data
            
        except Exception as e:
            print(f"æå–äº§å“æ•°æ®å¤±è´¥: {e}")
            return None
    
    def safe_get_attribute(self, parent_element, selector, attribute):
        """å®‰å…¨è·å–å…ƒç´ å±æ€§"""
        try:
            element = parent_element.find_element(By.CSS_SELECTOR, selector)
            return element.get_attribute(attribute)
        except NoSuchElementException:
            return None
    
    def extract_price(self, product_element):
        """æå–ä»·æ ¼"""
        price_text = self.safe_get_text(product_element, '.price, .current-price, .sale-price')
        if price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_original_price(self, product_element):
        """æå–åŸä»·"""
        original_price_text = self.safe_get_text(product_element, '.original-price, .regular-price, .was-price')
        if original_price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', original_price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_rating(self, product_element):
        """æå–è¯„åˆ†"""
        # å°è¯•å¤šç§è¯„åˆ†æ ¼å¼
        rating_element = product_element.find_elements(By.CSS_SELECTOR, '.rating, .stars, .score')
        
        for element in rating_element:
            # æ£€æŸ¥dataå±æ€§
            rating = element.get_attribute('data-rating') or element.get_attribute('data-score')
            if rating:
                try:
                    return float(rating)
                except ValueError:
                    pass
            
            # æ£€æŸ¥æ–‡æœ¬å†…å®¹
            rating_text = element.text
            if rating_text:
                import re
                rating_match = re.search(r'([\d.]+)', rating_text)
                if rating_match:
                    try:
                        return float(rating_match.group(1))
                    except ValueError:
                        pass
        
        return None
    
    def extract_reviews_count(self, product_element):
        """æå–è¯„è®ºæ•°é‡"""
        reviews_text = self.safe_get_text(product_element, '.reviews, .review-count, .comments')
        if reviews_text:
            import re
            count_match = re.search(r'\d+', reviews_text)
            if count_match:
                return int(count_match.group())
        return 0
    
    def extract_discount(self, product_element):
        """æå–æŠ˜æ‰£ä¿¡æ¯"""
        discount_text = self.safe_get_text(product_element, '.discount, .sale, .off')
        if discount_text:
            import re
            discount_match = re.search(r'(\d+)%', discount_text)
            if discount_match:
                return int(discount_match.group(1))
        return None
    
    def go_to_next_page(self):
        """è½¬åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # å°è¯•å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            next_selectors = [
                '.next-page',
                '.pagination .next',
                'a[aria-label="Next"]',
                '.pager .next',
                '[data-page="next"]'
            ]
            
            for selector in next_selectors:
                try:
                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if next_button.is_enabled() and next_button.is_displayed():
                        self.driver.execute_script("arguments[0].click();", next_button)
                        time.sleep(2)
                        return True
                except NoSuchElementException:
                    continue
            
            return False
            
        except Exception as e:
            print(f"è½¬åˆ°ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            return False
    
    def take_screenshot(self, filename=None):
        """æˆªå›¾"""
        if not filename:
            filename = f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        
        self.driver.save_screenshot(filename)
        print(f"æˆªå›¾å·²ä¿å­˜: {filename}")
        return filename
    
    def save_data(self, data, filename, format='json'):
        """ä¿å­˜æ•°æ®"""
        if format == 'json':
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        elif format == 'csv':
            if data:
                fieldnames = data[0].keys()
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        self.driver.quit()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    scraper = SeleniumScraper(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ç”¨äºè°ƒè¯•
    
    try:
        # çˆ¬å–ç¤¾äº¤åª’ä½“å¸–å­
        print("å¼€å§‹çˆ¬å–ç¤¾äº¤åª’ä½“å¸–å­...")
        posts = scraper.scrape_social_media_posts('https://example-social.com/profile', max_posts=20)
        scraper.save_data(posts, 'social_posts.json')
        
        # çˆ¬å–ç”µå•†äº§å“ï¼ˆå¸¦ç­›é€‰ï¼‰
        print("å¼€å§‹çˆ¬å–ç”µå•†äº§å“...")
        filters = {
            'price_range': (100, 500),
            'brand': 'Apple',
            'category': 'Electronics'
        }
        products = scraper.scrape_ecommerce_with_filters(
            'https://example-shop.com/search',
            filters=filters,
            max_products=30
        )
        scraper.save_data(products, 'filtered_products.csv', format='csv')
        
        print(f"çˆ¬å–å®Œæˆï¼è·å¾— {len(posts)} ä¸ªå¸–å­å’Œ {len(products)} ä¸ªäº§å“")
        
    finally:
        scraper.close()
```

#### âœ… ä¼˜åŠ¿
- å®Œå…¨æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
- æ”¯æŒå¤æ‚çš„JavaScriptåº”ç”¨
- å¯ä»¥å¤„ç†å„ç§äº¤äº’æ“ä½œ
- æ”¯æŒå¤šç§æµè§ˆå™¨
- åŠŸèƒ½å¼ºå¤§ï¼Œæ‰©å±•æ€§å¥½

#### âŒ åŠ£åŠ¿
- èµ„æºæ¶ˆè€—å¤§ï¼Œé€Ÿåº¦æ…¢
- å®¹æ˜“è¢«æ£€æµ‹
- ç»´æŠ¤æˆæœ¬é«˜
- ç¨³å®šæ€§ç›¸å¯¹è¾ƒå·®

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- é‡åº¦JavaScriptåº”ç”¨
- éœ€è¦ç”¨æˆ·äº¤äº’çš„ç½‘ç«™
- å¤æ‚çš„è¡¨å•æäº¤
- éœ€è¦æˆªå›¾çš„åœºæ™¯
- æµ‹è¯•å’Œè°ƒè¯•

---

### 8. Playwright

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **ç°ä»£åŒ–è®¾è®¡**: ä¸“ä¸ºç°ä»£Webåº”ç”¨è®¾è®¡
- **å¤šæµè§ˆå™¨æ”¯æŒ**: Chromeã€Firefoxã€Safariã€Edge
- **é«˜æ€§èƒ½**: æ¯”Seleniumæ›´å¿«æ›´ç¨³å®š
- **è‡ªåŠ¨ç­‰å¾…**: æ™ºèƒ½ç­‰å¾…å…ƒç´ åŠ è½½
- **ç½‘ç»œæ‹¦æˆª**: å¯ä»¥æ‹¦æˆªå’Œä¿®æ”¹ç½‘ç»œè¯·æ±‚

#### ğŸ’° å®šä»·
- **å®Œå…¨å…è´¹**: å¼€æºé¡¹ç›®ï¼Œæ— ä½¿ç”¨é™åˆ¶

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class PlaywrightScraper:
    """Playwrightç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, headless=True, browser_type='chromium'):
        self.playwright = sync_playwright().start()
        self.browser_type = browser_type
        self.headless = headless
        self.browser = None
        self.context = None
        self.page = None
        
        self.setup_browser()
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        # é€‰æ‹©æµè§ˆå™¨ç±»å‹
        if self.browser_type == 'chromium':
            self.browser = self.playwright.chromium.launch(headless=self.headless)
        elif self.browser_type == 'firefox':
            self.browser = self.playwright.firefox.launch(headless=self.headless)
        elif self.browser_type == 'webkit':
            self.browser = self.playwright.webkit.launch(headless=self.headless)
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        
        # åˆ›å»ºé¡µé¢
        self.page = self.context.new_page()
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶
        self.page.set_default_timeout(30000)
    
    def scrape_spa_application(self, url, max_items=50):
        """çˆ¬å–SPAåº”ç”¨ç¤ºä¾‹"""
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.page.goto(url)
            
            # ç­‰å¾…åº”ç”¨åŠ è½½
            self.page.wait_for_selector('.app-container, #app, .main-content')
            
            items = []
            
            # æ¨¡æ‹Ÿæ— é™æ»šåŠ¨
            while len(items) < max_items:
                # ç­‰å¾…å†…å®¹åŠ è½½
                self.page.wait_for_selector('.item, .card, .post', timeout=5000)
                
                # è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰é¡¹ç›®
                current_items = self.page.query_selector_all('.item, .card, .post')
                
                # æå–æ–°é¡¹ç›®
                for item_element in current_items[len(items):]:
                    if len(items) >= max_items:
                        break
                    
                    item_data = self.extract_item_data(item_element)
                    if item_data:
                        items.append(item_data)
                        print(f"å·²æå–ç¬¬ {len(items)} ä¸ªé¡¹ç›®")
                
                # æ»šåŠ¨åŠ è½½æ›´å¤š
                self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                
                # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                try:
                    self.page.wait_for_function(
                        f'document.querySelectorAll(".item, .card, .post").length > {len(current_items)}',
                        timeout=5000
                    )
                except:
                    print("æ²¡æœ‰æ›´å¤šå†…å®¹åŠ è½½")
                    break
            
            return items
            
        except Exception as e:
            print(f"çˆ¬å–SPAåº”ç”¨å¤±è´¥: {e}")
            return []
    
    def extract_item_data(self, element):
        """æå–é¡¹ç›®æ•°æ®"""
        try:
            # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
            element.scroll_into_view_if_needed()
            
            item_data = {
                'title': self.safe_get_text(element, '.title, h2, h3'),
                'description': self.safe_get_text(element, '.description, .summary, p'),
                'link': self.safe_get_attribute(element, 'a', 'href'),
                'image': self.safe_get_attribute(element, 'img', 'src'),
                'date': self.safe_get_text(element, '.date, .time, .timestamp'),
                'author': self.safe_get_text(element, '.author, .user, .creator'),
                'tags': self.extract_tags(element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return item_data
            
        except Exception as e:
            print(f"æå–é¡¹ç›®æ•°æ®å¤±è´¥: {e}")
            return None
    
    def safe_get_text(self, element, selector):
        """å®‰å…¨è·å–æ–‡æœ¬å†…å®¹"""
        try:
            target = element.query_selector(selector)
            return target.text_content().strip() if target else None
        except:
            return None
    
    def safe_get_attribute(self, element, selector, attribute):
        """å®‰å…¨è·å–å±æ€§å€¼"""
        try:
            target = element.query_selector(selector)
            return target.get_attribute(attribute) if target else None
        except:
            return None
    
    def extract_tags(self, element):
        """æå–æ ‡ç­¾"""
        try:
            tag_elements = element.query_selector_all('.tag, .label, .category')
            return [tag.text_content().strip() for tag in tag_elements]
        except:
            return []
    
    def scrape_with_authentication(self, login_url, username, password, target_url):
        """å¸¦è®¤è¯çš„çˆ¬å–"""
        try:
            # è®¿é—®ç™»å½•é¡µé¢
            print("æ­£åœ¨ç™»å½•...")
            self.page.goto(login_url)
            
            # å¡«å†™ç™»å½•è¡¨å•
            self.page.fill('input[name="username"], input[type="email"]', username)
            self.page.fill('input[name="password"], input[type="password"]', password)
            
            # ç‚¹å‡»ç™»å½•æŒ‰é’®
            self.page.click('button[type="submit"], .login-btn, .submit-btn')
            
            # ç­‰å¾…ç™»å½•å®Œæˆ
            self.page.wait_for_url('**/dashboard**', timeout=10000)
            print("ç™»å½•æˆåŠŸ")
            
            # è®¿é—®ç›®æ ‡é¡µé¢
            self.page.goto(target_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            self.page.wait_for_load_state('networkidle')
            
            # æå–æ•°æ®
            data = self.extract_authenticated_data()
            
            return data
            
        except Exception as e:
            print(f"è®¤è¯çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def extract_authenticated_data(self):
        """æå–éœ€è¦è®¤è¯çš„æ•°æ®"""
        try:
            # ç­‰å¾…å†…å®¹åŠ è½½
            self.page.wait_for_selector('.protected-content, .user-data')
            
            data = []
            
            # æå–ç”¨æˆ·æ•°æ®
            user_elements = self.page.query_selector_all('.user-item, .profile-card')
            
            for element in user_elements:
                user_data = {
                    'name': self.safe_get_text(element, '.name, .username'),
                    'email': self.safe_get_text(element, '.email'),
                    'status': self.safe_get_text(element, '.status'),
                    'last_login': self.safe_get_text(element, '.last-login'),
                    'role': self.safe_get_text(element, '.role, .permission'),
                    'avatar': self.safe_get_attribute(element, 'img', 'src')
                }
                data.append(user_data)
            
            return data
            
        except Exception as e:
            print(f"æå–è®¤è¯æ•°æ®å¤±è´¥: {e}")
            return []
    
    def scrape_with_network_interception(self, url):
        """å¸¦ç½‘ç»œæ‹¦æˆªçš„çˆ¬å–"""
        api_responses = []
        
        # è®¾ç½®ç½‘ç»œæ‹¦æˆª
        def handle_response(response):
            if '/api/' in response.url and response.status == 200:
                try:
                    api_data = response.json()
                    api_responses.append({
                        'url': response.url,
                        'data': api_data,
                        'timestamp': datetime.now().isoformat()
                    })
                except:
                    pass
        
        self.page.on('response', handle_response)
        
        try:
            # è®¿é—®é¡µé¢
            self.page.goto(url)
            
            # ç­‰å¾…APIè°ƒç”¨å®Œæˆ
            self.page.wait_for_load_state('networkidle')
            
            # è§¦å‘æ›´å¤šAPIè°ƒç”¨
            self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            time.sleep(2)
            
            # ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®
            load_more_btn = self.page.query_selector('.load-more, .show-more')
            if load_more_btn:
                load_more_btn.click()
                self.page.wait_for_load_state('networkidle')
            
            return api_responses
            
        except Exception as e:
            print(f"ç½‘ç»œæ‹¦æˆªçˆ¬å–å¤±è´¥: {e}")
            return api_responses
    
    def save_data(self, data, filename, format='json'):
        """ä¿å­˜æ•°æ®"""
        if format == 'json':
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    scraper = PlaywrightScraper(headless=False, browser_type='chromium')
    
    try:
        # çˆ¬å–SPAåº”ç”¨
        print("å¼€å§‹çˆ¬å–SPAåº”ç”¨...")
        spa_data = scraper.scrape_spa_application('https://example-spa.com', max_items=30)
        scraper.save_data(spa_data, 'spa_data.json')
        
        # å¸¦è®¤è¯çš„çˆ¬å–
        print("å¼€å§‹è®¤è¯çˆ¬å–...")
        auth_data = scraper.scrape_with_authentication(
            'https://example.com/login',
            'your_username',
            'your_password',
            'https://example.com/protected-data'
        )
        scraper.save_data(auth_data, 'auth_data.json')
        
        # ç½‘ç»œæ‹¦æˆªçˆ¬å–
        print("å¼€å§‹ç½‘ç»œæ‹¦æˆªçˆ¬å–...")
        api_data = scraper.scrape_with_network_interception('https://example-api.com')
        scraper.save_data(api_data, 'api_responses.json')
        
        print("çˆ¬å–å®Œæˆï¼")
        
    finally:
        scraper.close()
```

#### âœ… ä¼˜åŠ¿
- æ€§èƒ½ä¼˜ç§€ï¼Œæ¯”Seleniumæ›´å¿«
- ç°ä»£åŒ–APIè®¾è®¡
- è‡ªåŠ¨ç­‰å¾…æœºåˆ¶
- ç½‘ç»œæ‹¦æˆªåŠŸèƒ½å¼ºå¤§
- å¤šæµè§ˆå™¨æ”¯æŒå¥½

#### âŒ åŠ£åŠ¿
- ç›¸å¯¹è¾ƒæ–°ï¼Œç”Ÿæ€ä¸å¦‚Selenium
- å­¦ä¹ èµ„æºç›¸å¯¹è¾ƒå°‘
- æŸäº›é«˜çº§åŠŸèƒ½éœ€è¦æ·±å…¥äº†è§£

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- ç°ä»£SPAåº”ç”¨çˆ¬å–
- éœ€è¦é«˜æ€§èƒ½çš„åœºæ™¯
- APIæ•°æ®æ‹¦æˆª
- è‡ªåŠ¨åŒ–æµ‹è¯•
- å¤æ‚äº¤äº’åœºæ™¯

---

## ğŸ”§ ä¸“ä¸šå·¥å…·

### 9. Apify

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **äº‘ç«¯å¹³å°**: å®Œå…¨æ‰˜ç®¡çš„çˆ¬è™«æœåŠ¡
- **é¢„ç½®çˆ¬è™«**: ä¸°å¯Œçš„ç°æˆçˆ¬è™«æ¨¡æ¿
- **å¯è§†åŒ–ç¼–è¾‘**: æ— ä»£ç çˆ¬è™«æ„å»º
- **æ•°æ®å­˜å‚¨**: å†…ç½®æ•°æ®å­˜å‚¨å’Œå¯¼å‡º
- **APIé›†æˆ**: å®Œæ•´çš„REST API

#### ğŸ’° å®šä»·
- **å…è´¹å¥—é¤**: $5å…è´¹é¢åº¦/æœˆ
- **Personal**: $49/æœˆ
- **Team**: $499/æœˆ
- **Enterprise**: å®šåˆ¶ä»·æ ¼

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
from apify_client import ApifyClient
import json
from datetime import datetime

class ApifyScraper:
    """Apifyçˆ¬è™«å®¢æˆ·ç«¯"""
    
    def __init__(self, api_token):
        self.client = ApifyClient(api_token)
    
    def scrape_google_search(self, queries, max_results=100):
        """ä½¿ç”¨Googleæœç´¢çˆ¬è™«"""
        try:
            # è¿è¡ŒGoogleæœç´¢çˆ¬è™«
            run_input = {
                "queries": queries,
                "maxPagesPerQuery": max_results // 10,
                "resultsPerPage": 10,
                "countryCode": "US",
                "languageCode": "en",
                "includeUnfurlData": True
            }
            
            run = self.client.actor("apify/google-search-scraper").call(run_input=run_input)
            
            # è·å–ç»“æœ
            results = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                results.append(item)
            
            return results
            
        except Exception as e:
            print(f"Googleæœç´¢çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def scrape_amazon_products(self, search_terms, max_items=50):
        """ä½¿ç”¨Amazonäº§å“çˆ¬è™«"""
        try:
            run_input = {
                "searchTerms": search_terms,
                "maxItems": max_items,
                "country": "US",
                "includeReviews": True,
                "includeImages": True
            }
            
            run = self.client.actor("apify/amazon-product-scraper").call(run_input=run_input)
            
            products = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                products.append(item)
            
            return products
            
        except Exception as e:
            print(f"Amazonäº§å“çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def scrape_instagram_posts(self, usernames, max_posts=30):
        """ä½¿ç”¨Instagramçˆ¬è™«"""
        try:
            run_input = {
                "usernames": usernames,
                "resultsLimit": max_posts,
                "includeComments": True,
                "includeHashtags": True
            }
            
            run = self.client.actor("apify/instagram-scraper").call(run_input=run_input)
            
            posts = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                posts.append(item)
            
            return posts
            
        except Exception as e:
            print(f"Instagramçˆ¬å–å¤±è´¥: {e}")
            return []
    
    def create_custom_scraper(self, start_urls, page_function):
        """åˆ›å»ºè‡ªå®šä¹‰çˆ¬è™«"""
        try:
            # åˆ›å»ºæ–°çš„Actor
            actor_input = {
                "startUrls": start_urls,
                "pageFunction": page_function,
                "maxRequestsPerCrawl": 100,
                "maxConcurrency": 10
            }
            
            run = self.client.actor("apify/web-scraper").call(run_input=actor_input)
            
            results = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                results.append(item)
            
            return results
            
        except Exception as e:
            print(f"è‡ªå®šä¹‰çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    scraper = ApifyScraper("your-apify-token")
    
    # Googleæœç´¢çˆ¬å–
    search_queries = ["AI technology 2024", "machine learning trends"]
    search_results = scraper.scrape_google_search(search_queries, max_results=50)
    print(f"è·å¾— {len(search_results)} ä¸ªæœç´¢ç»“æœ")
    
    # Amazonäº§å“çˆ¬å–
    amazon_terms = ["laptop", "smartphone"]
    amazon_products = scraper.scrape_amazon_products(amazon_terms, max_items=30)
    print(f"è·å¾— {len(amazon_products)} ä¸ªAmazonäº§å“")
    
    # Instagramå¸–å­çˆ¬å–
    instagram_users = ["tech_influencer", "ai_researcher"]
    instagram_posts = scraper.scrape_instagram_posts(instagram_users, max_posts=20)
    print(f"è·å¾— {len(instagram_posts)} ä¸ªInstagramå¸–å­")
```

#### âœ… ä¼˜åŠ¿
- å®Œå…¨æ‰˜ç®¡ï¼Œæ— éœ€ç»´æŠ¤åŸºç¡€è®¾æ–½
- ä¸°å¯Œçš„é¢„ç½®çˆ¬è™«
- å¼ºå¤§çš„åçˆ¬è™«èƒ½åŠ›
- æ•°æ®å­˜å‚¨å’Œå¤„ç†ä¸€ä½“åŒ–
- è‰¯å¥½çš„ç›‘æ§å’Œæ—¥å¿—

#### âŒ åŠ£åŠ¿
- æˆæœ¬ç›¸å¯¹è¾ƒé«˜
- å®šåˆ¶åŒ–ç¨‹åº¦æœ‰é™
- ä¾èµ–ç¬¬ä¸‰æ–¹å¹³å°
- å­¦ä¹ æ›²çº¿è¾ƒé™¡

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å¤§è§„æ¨¡å•†ä¸šçˆ¬å–
- éœ€è¦é«˜å¯é æ€§çš„é¡¹ç›®
- å›¢é˜Ÿåä½œå¼€å‘
- å¤æ‚çš„åçˆ¬è™«åœºæ™¯

---

### 10. Scrapy Cloud

#### ğŸŒŸ æ ¸å¿ƒç‰¹æ€§
- **Scrapyæ‰˜ç®¡**: ä¸“ä¸ºScrapyè®¾è®¡çš„äº‘å¹³å°
- **è‡ªåŠ¨æ‰©å±•**: æ ¹æ®éœ€æ±‚è‡ªåŠ¨æ‰©å±•èµ„æº
- **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§çˆ¬è™«çŠ¶æ€
- **æ•°æ®ç®¡é“**: çµæ´»çš„æ•°æ®å¤„ç†æµæ°´çº¿
- **ç‰ˆæœ¬ç®¡ç†**: ä»£ç ç‰ˆæœ¬æ§åˆ¶å’Œéƒ¨ç½²

#### ğŸ’° å®šä»·
- **å…è´¹å¥—é¤**: 1ä¸ªå¹¶å‘å•å…ƒ
- **Professional**: $9/æœˆ/å•å…ƒ
- **Business**: å®šåˆ¶ä»·æ ¼

#### ğŸ“ ä»£ç ç¤ºä¾‹
```python
# scrapyé¡¹ç›®ç»“æ„
# myproject/
# â”œâ”€â”€ scrapy.cfg
# â”œâ”€â”€ myproject/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ items.py
# â”‚   â”œâ”€â”€ middlewares.py
# â”‚   â”œâ”€â”€ pipelines.py
# â”‚   â”œâ”€â”€ settings.py
# â”‚   â””â”€â”€ spiders/
# â”‚       â”œâ”€â”€ __init__.py
# â”‚       â””â”€â”€ example_spider.py

# items.py
import scrapy

class ProductItem(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    description = scrapy.Field()
    image_urls = scrapy.Field()
    images = scrapy.Field()
    url = scrapy.Field()
    scraped_at = scrapy.Field()

# example_spider.py
import scrapy
from myproject.items import ProductItem
from datetime import datetime

class ProductSpider(scrapy.Spider):
    name = 'products'
    allowed_domains = ['example-shop.com']
    start_urls = ['https://example-shop.com/products']
    
    custom_settings = {
        'ITEM_PIPELINES': {
            'myproject.pipelines.ValidationPipeline': 300,
            'scrapy.pipelines.images.ImagesPipeline': 400,
            'myproject.pipelines.ScrapyCloudPipeline': 500,
        },
        'IMAGES_STORE': 's3://my-bucket/images/',
        'DOWNLOAD_DELAY': 1,
        'AUTOTHROTTLE_ENABLED': True,
    }
    
    def parse(self, response):
        # æå–äº§å“é“¾æ¥
        product_links = response.css('.product-item a::attr(href)').getall()
        
        for link in product_links:
            yield response.follow(link, self.parse_product)
        
        # å¤„ç†åˆ†é¡µ
        next_page = response.css('.pagination .next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_product(self, response):
        item = ProductItem()
        
        item['name'] = response.css('h1.product-title::text').get()
        item['price'] = response.css('.price .amount::text').re_first(r'\d+\.\d+')
        item['description'] = ' '.join(response.css('.product-description p::text').getall())
        item['image_urls'] = response.css('.product-gallery img::attr(src)').getall()
        item['url'] = response.url
        item['scraped_at'] = datetime.now().isoformat()
        
        yield item

# pipelines.py
import json
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class ValidationPipeline:
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        if not adapter.get('name') or not adapter.get('price'):
            raise DropItem(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {item}")
        
        # ä»·æ ¼æ ¼å¼åŒ–
        if adapter.get('price'):
            try:
                adapter['price'] = float(adapter['price'])
            except ValueError:
                adapter['price'] = None
        
        return item

class ScrapyCloudPipeline:
    def process_item(self, item, spider):
        # å‘é€åˆ°Scrapy Cloudçš„æ•°æ®å­˜å‚¨
        return item

# settings.py
BOT_NAME = 'myproject'

SPIDER_MODULES = ['myproject.spiders']
NEWSPIDER_MODULE = 'myproject.spiders'

# Scrapy Cloudè®¾ç½®
SHUB_JOBKEY = None  # ç”±Scrapy Cloudè‡ªåŠ¨è®¾ç½®

# éƒ¨ç½²åˆ°Scrapy Cloud
# 1. å®‰è£…shubå‘½ä»¤è¡Œå·¥å…·: pip install shub
# 2. ç™»å½•: shub login
# 3. éƒ¨ç½²: shub deploy

# ä½¿ç”¨Scrapy Cloud API
import requests

class ScrapyCloudAPI:
    def __init__(self, api_key, project_id):
        self.api_key = api_key
        self.project_id = project_id
        self.base_url = "https://app.scrapinghub.com/api"
    
    def schedule_spider(self, spider_name, **kwargs):
        """è°ƒåº¦çˆ¬è™«è¿è¡Œ"""
        url = f"{self.base_url}/schedule.json"
        data = {
            'project': self.project_id,
            'spider': spider_name,
            **kwargs
        }
        
        response = requests.post(url, data=data, auth=(self.api_key, ''))
        return response.json()
    
    def get_job_status(self, job_id):
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        url = f"{self.base_url}/jobs/list.json"
        params = {
            'project': self.project_id,
            'job': job_id
        }
        
        response = requests.get(url, params=params, auth=(self.api_key, ''))
        return response.json()
    
    def get_job_items(self, job_id):
        """è·å–ä»»åŠ¡ç»“æœ"""
        url = f"{self.base_url}/items.json"
        params = {
            'project': self.project_id,
            'job': job_id
        }
        
        response = requests.get(url, params=params, auth=(self.api_key, ''))
        
        items = []
        for line in response.text.strip().split('\n'):
            if line:
                items.append(json.loads(line))
        
        return items

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    api = ScrapyCloudAPI('your-api-key', 'your-project-id')
    
    # è°ƒåº¦çˆ¬è™«
    job = api.schedule_spider('products', setting='DOWNLOAD_DELAY=2')
    job_id = job['jobid']
    print(f"ä»»åŠ¡å·²è°ƒåº¦ï¼ŒID: {job_id}")
    
    # æ£€æŸ¥çŠ¶æ€
    import time
    while True:
        status = api.get_job_status(job_id)
        if status['jobs'][0]['state'] == 'finished':
            print("ä»»åŠ¡å®Œæˆ")
            break
        elif status['jobs'][0]['state'] == 'running':
            print("ä»»åŠ¡è¿è¡Œä¸­...")
            time.sleep(30)
        else:
            print(f"ä»»åŠ¡çŠ¶æ€: {status['jobs'][0]['state']}")
            break
    
    # è·å–ç»“æœ
    items = api.get_job_items(job_id)
    print(f"è·å¾— {len(items)} ä¸ªç»“æœ")
```

#### âœ… ä¼˜åŠ¿
- ä¸“ä¸ºScrapyä¼˜åŒ–
- è‡ªåŠ¨æ‰©å±•å’Œè´Ÿè½½å‡è¡¡
- å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—
- ç‰ˆæœ¬æ§åˆ¶å’Œéƒ¨ç½²ç®€å•
- ä¸Scrapyç”Ÿæ€å®Œç¾é›†æˆ

#### âŒ åŠ£åŠ¿
- ä»…æ”¯æŒScrapyæ¡†æ¶
- æˆæœ¬éšè§„æ¨¡å¢é•¿
- å­¦ä¹ æ›²çº¿è¾ƒé™¡
- å®šåˆ¶åŒ–ç¨‹åº¦æœ‰é™

#### ğŸ¯ é€‚ç”¨åœºæ™¯
- å¤§è§„æ¨¡Scrapyé¡¹ç›®
- éœ€è¦é«˜å¯ç”¨æ€§çš„çˆ¬è™«
- å›¢é˜Ÿåä½œå¼€å‘
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

---

## ğŸ“Š å·¥å…·å¯¹æ¯”æ€»ç»“

### æŒ‰ä½¿ç”¨åœºæ™¯åˆ†ç±»

#### ğŸš€ å¿«é€Ÿä¸Šæ‰‹ï¼ˆåˆå­¦è€…ï¼‰
1. **Firecrawl** - AIé©±åŠ¨ï¼Œæœ€ç®€å•
2. **Beautiful Soup + Requests** - è¯­æ³•ç®€æ´
3. **Octoparse** - å¯è§†åŒ–æ“ä½œ

#### âš¡ é«˜æ€§èƒ½éœ€æ±‚
1. **Scrapy** - å¼‚æ­¥å¤„ç†ï¼Œé€Ÿåº¦æœ€å¿«
2. **Playwright** - ç°ä»£åŒ–ï¼Œæ€§èƒ½ä¼˜ç§€
3. **Scrapfly** - äº‘ç«¯å¤„ç†ï¼Œå¹¶å‘èƒ½åŠ›å¼º

#### ğŸ¯ å•†ä¸šåº”ç”¨
1. **Firecrawl** - AIæ™ºèƒ½ï¼Œåçˆ¬è™«èƒ½åŠ›å¼º
2. **Apify** - å®Œå…¨æ‰˜ç®¡ï¼Œå¯é æ€§é«˜
3. **Scrapy Cloud** - ä¼ä¸šçº§éƒ¨ç½²

#### ğŸ”§ æŠ€æœ¯æ·±åº¦
1. **Scrapy** - æœ€çµæ´»ï¼Œå¯æ‰©å±•æ€§æœ€å¼º
2. **Selenium** - åŠŸèƒ½æœ€å…¨é¢
3. **Playwright** - ç°ä»£åŒ–æ¶æ„

### æˆæœ¬å¯¹æ¯”

| å·¥å…· | å…è´¹é¢åº¦ | ä»˜è´¹èµ·ä»· | é€‚åˆè§„æ¨¡ |
|------|----------|----------|----------|
| Firecrawl | 500æ¬¡/æœˆ | $20/æœˆ | ä¸­å°å‹ |
| Scrapy | å®Œå…¨å…è´¹ | - | æ‰€æœ‰è§„æ¨¡ |
| Beautiful Soup | å®Œå…¨å…è´¹ | - | å°å‹ |
| Selenium | å®Œå…¨å…è´¹ | - | æ‰€æœ‰è§„æ¨¡ |
| Playwright | å®Œå…¨å…è´¹ | - | æ‰€æœ‰è§„æ¨¡ |
| Octoparse | æœ‰é™åŠŸèƒ½ | $75/æœˆ | ä¸­å‹ |
| ParseHub | 200é¡µ/æœˆ | $149/æœˆ | ä¸­å‹ |
| Scrapfly | 1000æ¬¡/æœˆ | $15/æœˆ | ä¸­å¤§å‹ |
| Apify | $5é¢åº¦/æœˆ | $49/æœˆ | å¤§å‹ |
| Scrapy Cloud | 1å¹¶å‘å•å…ƒ | $9/æœˆ | å¤§å‹ |

### æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | Firecrawl | Scrapy | Selenium | Playwright | Beautiful Soup |
|------|-----------|--------|----------|------------|----------------|
| å­¦ä¹ éš¾åº¦ | â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­ |
| æ€§èƒ½ | â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­ |
| åçˆ¬è™«èƒ½åŠ› | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­ |
| JavaScriptæ”¯æŒ | â­â­â­â­â­ | â­ | â­â­â­â­â­ | â­â­â­â­â­ | âŒ |
| å¯æ‰©å±•æ€§ | â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­ |
| ç»´æŠ¤æˆæœ¬ | â­â­â­â­â­ | â­â­ | â­â­ | â­â­â­ | â­â­â­â­ |

## ğŸ¯ é€‰æ‹©å»ºè®®

### æ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©

#### æ–°æ‰‹å…¥é—¨é¡¹ç›®
- **æ¨è**: Firecrawl + Beautiful Soup
- **ç†ç”±**: å­¦ä¹ æˆæœ¬ä½ï¼Œå¿«é€Ÿä¸Šæ‰‹

#### ä¸­å°å‹å•†ä¸šé¡¹ç›®
- **æ¨è**: Firecrawl + Scrapy
- **ç†ç”±**: å¹³è¡¡äº†æ˜“ç”¨æ€§å’Œæ€§èƒ½

#### å¤§å‹ä¼ä¸šé¡¹ç›®
- **æ¨è**: Scrapy + Apify/Scrapy Cloud
- **ç†ç”±**: é«˜æ€§èƒ½ï¼Œé«˜å¯é æ€§

#### JavaScripté‡åº¦åº”ç”¨
- **æ¨è**: Playwright + Firecrawl
- **ç†ç”±**: ç°ä»£åŒ–æ¶æ„ï¼Œå¤„ç†èƒ½åŠ›å¼º

#### é¢„ç®—æœ‰é™é¡¹ç›®
- **æ¨è**: Scrapy + Beautiful Soup
- **ç†ç”±**: å®Œå…¨å…è´¹ï¼ŒåŠŸèƒ½å¼ºå¤§

---

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Firecrawlæ–‡æ¡£](https://docs.firecrawl.dev/)
- [Scrapyæ–‡æ¡£](https://docs.scrapy.org/)
- [Seleniumæ–‡æ¡£](https://selenium-python.readthedocs.io/)
- [Playwrightæ–‡æ¡£](https://playwright.dev/python/)
- [Beautiful Soupæ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

### ç¤¾åŒºèµ„æº
- [Scrapyä¸­æ–‡ç¤¾åŒº](https://scrapy-chs.readthedocs.io/)
- [Pythonçˆ¬è™«å­¦ä¹ æŒ‡å—](https://github.com/Kr1s77/awesome-python-login-model)
- [ç½‘ç»œçˆ¬è™«æœ€ä½³å®è·µ](https://github.com/lorien/awesome-web-scraping)

### åœ¨çº¿è¯¾ç¨‹
- [Pythonç½‘ç»œçˆ¬è™«å®æˆ˜](https://www.coursera.org/learn/python-web-scraping)
- [Scrapyæ¡†æ¶æ·±å…¥å­¦ä¹ ](https://www.udemy.com/course/scrapy-tutorial-web-scraping-with-python/)
- [ç°ä»£ç½‘ç»œçˆ¬è™«æŠ€æœ¯](https://www.edx.org/course/web-scraping-with-python)

---

## ğŸ”® æœªæ¥è¶‹åŠ¿

### AIé©±åŠ¨çš„å‘å±•æ–¹å‘
1. **æ™ºèƒ½åŒ–æå–**: æ›´å‡†ç¡®çš„å†…å®¹è¯†åˆ«
2. **è‡ªé€‚åº”çˆ¬å–**: è‡ªåŠ¨é€‚åº”ç½‘ç«™å˜åŒ–
3. **è¯­ä¹‰ç†è§£**: åŸºäºå†…å®¹è¯­ä¹‰çš„æ•°æ®æå–
4. **è‡ªåŠ¨åŒ–é…ç½®**: å‡å°‘äººå·¥é…ç½®éœ€æ±‚

### æŠ€æœ¯å‘å±•è¶‹åŠ¿
1. **æ— æœåŠ¡å™¨æ¶æ„**: æ›´çµæ´»çš„éƒ¨ç½²æ–¹å¼
2. **è¾¹ç¼˜è®¡ç®—**: é™ä½å»¶è¿Ÿï¼Œæé«˜æ•ˆç‡
3. **éšç§ä¿æŠ¤**: æ›´å¼ºçš„æ•°æ®ä¿æŠ¤æœºåˆ¶
4. **å®æ—¶å¤„ç†**: æµå¼æ•°æ®å¤„ç†èƒ½åŠ›

### è¡Œä¸šåº”ç”¨è¶‹åŠ¿
1. **å‚ç›´é¢†åŸŸä¸“ä¸šåŒ–**: é’ˆå¯¹ç‰¹å®šè¡Œä¸šçš„è§£å†³æ–¹æ¡ˆ
2. **æ•°æ®è´¨é‡æå‡**: æ›´é«˜è´¨é‡çš„æ•°æ®æ¸…æ´—å’ŒéªŒè¯
3. **åˆè§„æ€§å¢å¼º**: æ›´å¥½çš„æ³•å¾‹æ³•è§„éµå¾ª
4. **æˆæœ¬ä¼˜åŒ–**: æ›´ç»æµçš„å¤§è§„æ¨¡çˆ¬å–æ–¹æ¡ˆ

---

## ğŸ“ æ€»ç»“

ç½‘ç»œçˆ¬è™«å·¥å…·çš„é€‰æ‹©åº”è¯¥åŸºäºå…·ä½“çš„é¡¹ç›®éœ€æ±‚ã€æŠ€æœ¯æ°´å¹³å’Œé¢„ç®—è€ƒè™‘ã€‚å¯¹äºåˆå­¦è€…ï¼Œå»ºè®®ä»Firecrawlæˆ–Beautiful Soupå¼€å§‹ï¼›å¯¹äºæœ‰ç»éªŒçš„å¼€å‘è€…ï¼ŒScrapyæ˜¯æœ€ä½³é€‰æ‹©ï¼›å¯¹äºä¼ä¸šçº§åº”ç”¨ï¼Œå¯ä»¥è€ƒè™‘Apifyæˆ–Scrapy Cloudç­‰æ‰˜ç®¡æœåŠ¡ã€‚

æ— è®ºé€‰æ‹©å“ªç§å·¥å…·ï¼Œéƒ½è¦æ³¨æ„éµå®ˆç½‘ç«™çš„robots.txtåè®®ï¼Œå°Šé‡æ•°æ®ç‰ˆæƒï¼Œåˆç†æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œç¡®ä¿çˆ¬è™«æ´»åŠ¨çš„åˆæ³•æ€§å’Œé“å¾·æ€§ã€‚

éšç€AIæŠ€æœ¯çš„å‘å±•ï¼ŒåƒFirecrawlè¿™æ ·çš„æ™ºèƒ½åŒ–å·¥å…·å°†æˆä¸ºæœªæ¥çš„ä¸»æµè¶‹åŠ¿ï¼Œå®ƒä»¬èƒ½å¤Ÿå¤§å¤§é™ä½ç½‘ç»œçˆ¬è™«çš„æŠ€æœ¯é—¨æ§›ï¼Œæé«˜æ•°æ®é‡‡é›†çš„æ•ˆç‡å’Œè´¨é‡ã€‚

---

*æœ¬æ–‡æŒç»­æ›´æ–°ï¼Œæ¬¢è¿å…³æ³¨æœ€æ–°çš„ç½‘ç»œçˆ¬è™«æŠ€æœ¯å‘å±•åŠ¨æ€ã€‚*