# 网络爬虫Top 10工具完整指南

## 📋 文章信息

- **作者**: Mendel
- **发布时间**: 2024年12月17日
- **原文链接**: https://www.firecrawl.dev/blog/top-10-tools-for-web-scraping
- **分类**: 工具指南
- **标签**: 网络爬虫, 工具对比, 数据采集, 自动化

## 📝 摘要

本文全面介绍了2024年最优秀的10款网络爬虫工具，涵盖AI驱动工具、无代码平台、Python库和浏览器自动化框架四大类别。每种工具都包含详细的功能介绍、优缺点分析、定价信息和实际代码示例，帮助开发者根据具体需求选择最适合的爬虫解决方案。

## 🎯 主要内容

### 工具分类概览

网络爬虫工具可以分为四大类别：

1. **AI驱动工具** - 智能化数据提取
2. **无代码/低代码平台** - 可视化操作界面
3. **Python库** - 编程灵活性高
4. **浏览器自动化框架** - 处理复杂交互

---

## 🤖 AI驱动工具

### 1. Firecrawl

#### 🌟 核心特性
- **AI驱动提取**: 智能识别和提取结构化数据
- **多格式支持**: Markdown、HTML、JSON等多种输出格式
- **反爬虫处理**: 自动处理验证码和反爬虫机制
- **批量处理**: 支持大规模网站爬取
- **实时API**: RESTful API接口，易于集成

#### 💰 定价
- **免费套餐**: 500次请求/月
- **Starter**: $20/月，5,000次请求
- **Standard**: $50/月，20,000次请求
- **Scale**: $200/月，100,000次请求

#### 📝 代码示例
```python
from firecrawl import FirecrawlApp

# 初始化客户端
app = FirecrawlApp(api_key="your-api-key")

# 单页爬取
result = app.scrape_url(
    url="https://example.com",
    params={
        "formats": ["markdown", "html"],
        "only_main_content": True,
        "include_tags": ["title", "meta", "article"]
    }
)

# 批量爬取
crawl_result = app.crawl_url(
    url="https://example.com",
    params={
        "limit": 100,
        "scrape_options": {
            "formats": ["markdown"]
        }
    }
)

# 智能搜索
search_result = app.search(
    query="AI technology trends 2024",
    limit=10,
    scrape_options={
        "formats": ["markdown"]
    }
)

# 结构化数据提取
extract_result = app.extract(
    urls=["https://example.com/product1", "https://example.com/product2"],
    schema={
        "type": "object",
        "properties": {
            "product_name": {"type": "string"},
            "price": {"type": "number"},
            "description": {"type": "string"},
            "availability": {"type": "boolean"}
        }
    },
    prompt="Extract product information from e-commerce pages"
)

print(f"提取的产品数据: {extract_result}")
```

#### ✅ 优势
- 智能化程度高，减少手动配置
- 强大的反爬虫能力
- 支持多种数据格式
- 云端处理，无需维护基础设施
- 优秀的文档和社区支持

#### ❌ 劣势
- 按使用量付费，大规模使用成本较高
- 依赖网络连接
- 定制化程度相对有限

#### 🎯 适用场景
- 新闻资讯采集
- 电商数据监控
- 学术资料收集
- 竞品分析
- 内容聚合平台

---

### 2. Scrapfly

#### 🌟 核心特性
- **智能代理轮换**: 自动处理IP封禁
- **JavaScript渲染**: 支持SPA应用爬取
- **数据清洗**: 自动去除广告和无关内容
- **监控告警**: 实时监控爬取状态
- **API优先**: 完整的RESTful API

#### 💰 定价
- **免费套餐**: 1,000次请求/月
- **Micro**: $15/月，10,000次请求
- **Startup**: $75/月，100,000次请求
- **Business**: $300/月，500,000次请求

#### 📝 代码示例
```python
import requests
from scrapfly import ScrapeConfig, ScrapflyClient

# 初始化客户端
client = ScrapflyClient(key="your-api-key")

# 基础爬取
config = ScrapeConfig(
    url="https://example.com",
    render_js=True,  # 启用JavaScript渲染
    proxy_pool="public_residential_pool",  # 使用住宅代理
    country="US",  # 指定代理国家
    format="clean_html",  # 清洗HTML
    cache=True,  # 启用缓存
    session="my_session"  # 会话管理
)

result = client.scrape(config)

# 处理结果
print(f"状态码: {result.status_code}")
print(f"内容长度: {len(result.content)}")
print(f"使用的代理: {result.context['proxy_used']}")

# 批量爬取
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]

configs = [
    ScrapeConfig(
        url=url,
        render_js=True,
        proxy_pool="public_datacenter_pool"
    ) for url in urls
]

# 并发爬取
results = client.concurrent_scrape(configs)

for i, result in enumerate(results):
    print(f"页面 {i+1}: {result.status_code}")
    
    # 保存结果
    with open(f"page_{i+1}.html", "w", encoding="utf-8") as f:
        f.write(result.content)
```

#### ✅ 优势
- 强大的代理管理功能
- 优秀的JavaScript渲染支持
- 自动数据清洗
- 详细的使用统计
- 灵活的定价方案

#### ❌ 劣势
- 学习曲线相对陡峭
- 高级功能需要付费
- 文档相对简单

#### 🎯 适用场景
- 大规模数据采集
- 需要代理轮换的项目
- JavaScript重度应用爬取
- 商业数据监控

---

## 🎨 无代码/低代码平台

### 3. Octoparse

#### 🌟 核心特性
- **可视化操作**: 点击式配置，无需编程
- **云端执行**: 24/7自动运行
- **数据导出**: 支持Excel、CSV、JSON等格式
- **定时任务**: 灵活的调度系统
- **模板库**: 预置常用网站模板

#### 💰 定价
- **免费版**: 有限功能，本地运行
- **Standard**: $75/月，云端执行
- **Professional**: $209/月，高级功能
- **Enterprise**: 定制价格

#### 📝 使用示例
```python
# Octoparse主要通过GUI操作，但也提供API
import requests
import json

class OctoparseAPI:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.token = self._get_token()
    
    def _get_token(self):
        """获取访问令牌"""
        login_url = "https://dataapi.octoparse.com/api/token"
        data = {
            "username": self.username,
            "password": self.password,
            "grant_type": "password"
        }
        
        response = requests.post(login_url, data=data)
        if response.status_code == 200:
            return response.json()["access_token"]
        else:
            raise Exception("登录失败")
    
    def get_task_list(self):
        """获取任务列表"""
        url = "https://dataapi.octoparse.com/api/task/gettasklist"
        headers = {"Authorization": f"Bearer {self.token}"}
        
        response = requests.get(url, headers=headers)
        return response.json()
    
    def start_task(self, task_id):
        """启动任务"""
        url = "https://dataapi.octoparse.com/api/task/starttask"
        headers = {"Authorization": f"Bearer {self.token}"}
        data = {"taskId": task_id}
        
        response = requests.post(url, headers=headers, json=data)
        return response.json()
    
    def get_data(self, task_id, size=100, offset=0):
        """获取爬取数据"""
        url = "https://dataapi.octoparse.com/api/alldata/GetDataOfTaskByOffset"
        headers = {"Authorization": f"Bearer {self.token}"}
        params = {
            "taskId": task_id,
            "size": size,
            "offset": offset
        }
        
        response = requests.get(url, headers=headers, params=params)
        return response.json()

# 使用示例
api = OctoparseAPI("your_username", "your_password")

# 获取任务列表
tasks = api.get_task_list()
print(f"任务数量: {len(tasks['data'])}")

# 启动第一个任务
if tasks['data']:
    task_id = tasks['data'][0]['taskId']
    result = api.start_task(task_id)
    print(f"任务启动结果: {result}")
    
    # 获取数据
    data = api.get_data(task_id)
    print(f"获取到 {len(data['data'])} 条数据")
```

#### ✅ 优势
- 零编程基础即可使用
- 丰富的预置模板
- 云端自动执行
- 直观的可视化界面
- 良好的客户支持

#### ❌ 劣势
- 定制化能力有限
- 价格相对较高
- 复杂逻辑处理困难
- 依赖平台稳定性

#### 🎯 适用场景
- 非技术人员数据采集
- 简单的定期数据监控
- 快速原型验证
- 小规模商业应用

---

### 4. ParseHub

#### 🌟 核心特性
- **智能选择器**: 自动识别页面元素
- **条件逻辑**: 支持复杂的爬取逻辑
- **API集成**: 完整的REST API
- **实时监控**: 任务执行状态监控
- **数据清洗**: 内置数据处理功能

#### 💰 定价
- **免费版**: 5个项目，200页/月
- **Standard**: $149/月，20个项目
- **Professional**: $499/月，无限项目
- **Enterprise**: 定制价格

#### 📝 代码示例
```python
import requests
import time
import json

class ParseHubAPI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://www.parsehub.com/api/v2"
    
    def get_projects(self):
        """获取项目列表"""
        url = f"{self.base_url}/projects"
        params = {"api_key": self.api_key}
        
        response = requests.get(url, params=params)
        return response.json()
    
    def run_project(self, project_token, start_url=None, start_template=None):
        """运行项目"""
        url = f"{self.base_url}/projects/{project_token}/run"
        data = {"api_key": self.api_key}
        
        if start_url:
            data["start_url"] = start_url
        if start_template:
            data["start_template"] = start_template
        
        response = requests.post(url, data=data)
        return response.json()
    
    def get_run_status(self, run_token):
        """获取运行状态"""
        url = f"{self.base_url}/runs/{run_token}"
        params = {"api_key": self.api_key}
        
        response = requests.get(url, params=params)
        return response.json()
    
    def get_run_data(self, run_token, format="json"):
        """获取运行数据"""
        url = f"{self.base_url}/runs/{run_token}/data"
        params = {
            "api_key": self.api_key,
            "format": format
        }
        
        response = requests.get(url, params=params)
        if format == "json":
            return response.json()
        else:
            return response.text
    
    def wait_for_completion(self, run_token, max_wait=3600):
        """等待任务完成"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            status = self.get_run_status(run_token)
            
            if status["status"] == "complete":
                return True
            elif status["status"] == "error":
                raise Exception(f"任务执行失败: {status.get('message', '未知错误')}")
            
            print(f"任务状态: {status['status']}, 进度: {status.get('pages_run', 0)}/{status.get('total_pages', '?')}")
            time.sleep(30)  # 等待30秒后再次检查
        
        raise Exception("任务执行超时")

# 使用示例
api = ParseHubAPI("your-api-key")

# 获取项目列表
projects = api.get_projects()
print(f"项目数量: {len(projects['projects'])}")

# 运行第一个项目
if projects['projects']:
    project_token = projects['projects'][0]['token']
    
    # 启动爬取任务
    run_result = api.run_project(
        project_token,
        start_url="https://example.com"
    )
    
    run_token = run_result['run_token']
    print(f"任务已启动，运行令牌: {run_token}")
    
    # 等待任务完成
    try:
        api.wait_for_completion(run_token)
        print("任务执行完成")
        
        # 获取数据
        data = api.get_run_data(run_token)
        print(f"获取到 {len(data)} 条数据")
        
        # 保存数据
        with open("parsehub_data.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"任务执行失败: {e}")
```

#### ✅ 优势
- 强大的可视化编辑器
- 支持复杂的条件逻辑
- 良好的API集成
- 免费版功能相对完整
- 活跃的社区支持

#### ❌ 劣势
- 学习曲线较陡
- 高级功能价格昂贵
- 执行速度相对较慢
- 对中文支持一般

#### 🎯 适用场景
- 复杂网站结构爬取
- 需要条件判断的场景
- API集成需求
- 中等规模数据采集

---

## 🐍 Python库

### 5. Scrapy

#### 🌟 核心特性
- **高性能框架**: 异步处理，速度极快
- **可扩展架构**: 丰富的中间件和扩展
- **内置功能**: 去重、缓存、重试等
- **数据管道**: 灵活的数据处理流水线
- **分布式支持**: 支持分布式爬取

#### 💰 定价
- **完全免费**: 开源项目，无使用限制

#### 📝 代码示例
```python
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
import json

class ProductSpider(scrapy.Spider):
    name = 'products'
    allowed_domains = ['example-shop.com']
    start_urls = ['https://example-shop.com/products']
    
    custom_settings = {
        'DOWNLOAD_DELAY': 1,  # 请求间隔
        'CONCURRENT_REQUESTS': 16,  # 并发请求数
        'AUTOTHROTTLE_ENABLED': True,  # 自动调节
        'ROBOTSTXT_OBEY': True,  # 遵守robots.txt
        'USER_AGENT': 'Mozilla/5.0 (compatible; ProductBot/1.0)',
        
        # 数据管道
        'ITEM_PIPELINES': {
            'myproject.pipelines.ValidationPipeline': 300,
            'myproject.pipelines.DuplicatesPipeline': 400,
            'myproject.pipelines.JsonWriterPipeline': 500,
        },
        
        # 中间件
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
            'myproject.middlewares.RotateUserAgentMiddleware': 400,
            'myproject.middlewares.ProxyMiddleware': 410,
        }
    }
    
    def parse(self, response):
        """解析产品列表页"""
        # 提取产品链接
        product_links = response.css('.product-item a::attr(href)').getall()
        
        for link in product_links:
            yield response.follow(link, self.parse_product)
        
        # 处理分页
        next_page = response.css('.pagination .next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_product(self, response):
        """解析产品详情页"""
        yield {
            'name': response.css('h1.product-title::text').get(),
            'price': response.css('.price .amount::text').re_first(r'\d+\.\d+'),
            'description': response.css('.product-description::text').getall(),
            'images': response.css('.product-gallery img::attr(src)').getall(),
            'availability': response.css('.stock-status::text').get(),
            'rating': response.css('.rating .stars::attr(data-rating)').get(),
            'reviews_count': response.css('.reviews-count::text').re_first(r'\d+'),
            'category': response.css('.breadcrumb a::text').getall()[:-1],
            'sku': response.css('.product-sku::text').get(),
            'url': response.url,
            'scraped_at': scrapy.utils.misc.datetime.now().isoformat()
        }

# 自定义数据管道
class ValidationPipeline:
    """数据验证管道"""
    
    def process_item(self, item, spider):
        if not item.get('name') or not item.get('price'):
            raise scrapy.exceptions.DropItem(f"缺少必要字段: {item}")
        
        # 价格格式化
        if item.get('price'):
            try:
                item['price'] = float(item['price'])
            except ValueError:
                item['price'] = None
        
        return item

class DuplicatesPipeline:
    """去重管道"""
    
    def __init__(self):
        self.ids_seen = set()
    
    def process_item(self, item, spider):
        item_id = item.get('sku') or item.get('url')
        if item_id in self.ids_seen:
            raise scrapy.exceptions.DropItem(f"重复项目: {item_id}")
        else:
            self.ids_seen.add(item_id)
            return item

class JsonWriterPipeline:
    """JSON输出管道"""
    
    def open_spider(self, spider):
        self.file = open(f'{spider.name}_items.json', 'w', encoding='utf-8')
        self.file.write('[')
        self.first_item = True
    
    def close_spider(self, spider):
        self.file.write(']')
        self.file.close()
    
    def process_item(self, item, spider):
        if not self.first_item:
            self.file.write(',')
        else:
            self.first_item = False
        
        line = json.dumps(dict(item), ensure_ascii=False, indent=2)
        self.file.write(line)
        return item

# 自定义中间件
class RotateUserAgentMiddleware:
    """用户代理轮换中间件"""
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
    
    def process_request(self, request, spider):
        import random
        ua = random.choice(self.user_agents)
        request.headers['User-Agent'] = ua
        return None

# 运行爬虫
if __name__ == '__main__':
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (compatible; ProductBot/1.0)',
        'ROBOTSTXT_OBEY': True,
        'DOWNLOAD_DELAY': 1,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 16,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 60,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'LOG_LEVEL': 'INFO'
    })
    
    process.crawl(ProductSpider)
    process.start()
```

#### ✅ 优势
- 性能极高，适合大规模爬取
- 架构灵活，可扩展性强
- 功能完整，内置常用功能
- 社区活跃，文档详细
- 完全免费开源

#### ❌ 劣势
- 学习曲线陡峭
- 配置复杂
- 需要编程基础
- 调试相对困难

#### 🎯 适用场景
- 大规模数据采集
- 复杂爬取逻辑
- 高性能要求
- 定制化需求高
- 长期维护项目

---

### 6. Beautiful Soup + Requests

#### 🌟 核心特性
- **简单易用**: 语法直观，学习成本低
- **强大解析**: 支持多种解析器
- **灵活选择**: CSS选择器和XPath
- **轻量级**: 依赖少，部署简单
- **稳定可靠**: 成熟的库，bug少

#### 💰 定价
- **完全免费**: 开源库，无使用限制

#### 📝 代码示例
```python
import requests
from bs4 import BeautifulSoup
import json
import time
import random
from urllib.parse import urljoin, urlparse
import csv
from datetime import datetime

class SimpleScraper:
    """简单网页爬虫类"""
    
    def __init__(self, delay_range=(1, 3)):
        self.session = requests.Session()
        self.delay_range = delay_range
        
        # 设置请求头
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_page(self, url, **kwargs):
        """获取网页内容"""
        try:
            response = self.session.get(url, **kwargs)
            response.raise_for_status()
            
            # 随机延迟
            delay = random.uniform(*self.delay_range)
            time.sleep(delay)
            
            return response
        except requests.RequestException as e:
            print(f"请求失败 {url}: {e}")
            return None
    
    def parse_html(self, html_content, parser='html.parser'):
        """解析HTML内容"""
        return BeautifulSoup(html_content, parser)
    
    def scrape_news_site(self, base_url, max_pages=5):
        """爬取新闻网站示例"""
        articles = []
        
        for page in range(1, max_pages + 1):
            print(f"正在爬取第 {page} 页...")
            
            # 构建分页URL
            page_url = f"{base_url}?page={page}"
            response = self.get_page(page_url)
            
            if not response:
                continue
            
            soup = self.parse_html(response.text)
            
            # 提取文章链接
            article_links = soup.find_all('a', class_='article-link')
            
            for link in article_links:
                article_url = urljoin(base_url, link.get('href'))
                article_data = self.scrape_article(article_url)
                
                if article_data:
                    articles.append(article_data)
        
        return articles
    
    def scrape_article(self, url):
        """爬取单篇文章"""
        response = self.get_page(url)
        if not response:
            return None
        
        soup = self.parse_html(response.text)
        
        # 提取文章信息
        article = {
            'url': url,
            'title': self.safe_extract(soup, 'h1.article-title', 'text'),
            'author': self.safe_extract(soup, '.author-name', 'text'),
            'publish_date': self.safe_extract(soup, '.publish-date', 'text'),
            'content': self.extract_content(soup),
            'tags': self.extract_tags(soup),
            'images': self.extract_images(soup, url),
            'scraped_at': datetime.now().isoformat()
        }
        
        return article
    
    def safe_extract(self, soup, selector, attr='text'):
        """安全提取元素内容"""
        try:
            element = soup.select_one(selector)
            if element:
                if attr == 'text':
                    return element.get_text(strip=True)
                else:
                    return element.get(attr)
        except Exception as e:
            print(f"提取失败 {selector}: {e}")
        return None
    
    def extract_content(self, soup):
        """提取文章正文"""
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content',
            'article .content'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # 清理内容
                for script in content_div(["script", "style"]):
                    script.decompose()
                
                paragraphs = content_div.find_all('p')
                return '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        return None
    
    def extract_tags(self, soup):
        """提取标签"""
        tag_selectors = [
            '.tags a',
            '.post-tags a',
            '.article-tags .tag'
        ]
        
        for selector in tag_selectors:
            tags = soup.select(selector)
            if tags:
                return [tag.get_text(strip=True) for tag in tags]
        
        return []
    
    def extract_images(self, soup, base_url):
        """提取图片链接"""
        images = []
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                # 转换为绝对URL
                absolute_url = urljoin(base_url, src)
                images.append({
                    'url': absolute_url,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
        
        return images
    
    def scrape_ecommerce_products(self, category_url, max_products=100):
        """爬取电商产品示例"""
        products = []
        page = 1
        
        while len(products) < max_products:
            print(f"正在爬取第 {page} 页产品...")
            
            page_url = f"{category_url}?page={page}"
            response = self.get_page(page_url)
            
            if not response:
                break
            
            soup = self.parse_html(response.text)
            product_links = soup.select('.product-item a')
            
            if not product_links:
                print("没有找到更多产品")
                break
            
            for link in product_links:
                if len(products) >= max_products:
                    break
                
                product_url = urljoin(category_url, link.get('href'))
                product_data = self.scrape_product(product_url)
                
                if product_data:
                    products.append(product_data)
            
            page += 1
        
        return products
    
    def scrape_product(self, url):
        """爬取单个产品"""
        response = self.get_page(url)
        if not response:
            return None
        
        soup = self.parse_html(response.text)
        
        # 提取价格（处理多种格式）
        price_text = self.safe_extract(soup, '.price .amount', 'text')
        price = None
        if price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', price_text.replace(',', ''))
            if price_match:
                price = float(price_match.group())
        
        product = {
            'url': url,
            'name': self.safe_extract(soup, 'h1.product-title', 'text'),
            'price': price,
            'original_price': self.extract_original_price(soup),
            'description': self.safe_extract(soup, '.product-description', 'text'),
            'images': [img['src'] for img in soup.select('.product-gallery img') if img.get('src')],
            'availability': self.safe_extract(soup, '.stock-status', 'text'),
            'rating': self.extract_rating(soup),
            'reviews_count': self.extract_reviews_count(soup),
            'specifications': self.extract_specifications(soup),
            'scraped_at': datetime.now().isoformat()
        }
        
        return product
    
    def extract_original_price(self, soup):
        """提取原价"""
        original_price_text = self.safe_extract(soup, '.price .original-price', 'text')
        if original_price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', original_price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_rating(self, soup):
        """提取评分"""
        rating_element = soup.select_one('.rating .stars')
        if rating_element:
            rating_text = rating_element.get('data-rating') or rating_element.get_text()
            try:
                return float(rating_text)
            except ValueError:
                pass
        return None
    
    def extract_reviews_count(self, soup):
        """提取评论数量"""
        reviews_text = self.safe_extract(soup, '.reviews-count', 'text')
        if reviews_text:
            import re
            count_match = re.search(r'\d+', reviews_text)
            if count_match:
                return int(count_match.group())
        return 0
    
    def extract_specifications(self, soup):
        """提取产品规格"""
        specs = {}
        spec_rows = soup.select('.specifications tr')
        
        for row in spec_rows:
            cells = row.select('td')
            if len(cells) >= 2:
                key = cells[0].get_text(strip=True)
                value = cells[1].get_text(strip=True)
                specs[key] = value
        
        return specs
    
    def save_to_json(self, data, filename):
        """保存数据到JSON文件"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"数据已保存到 {filename}")
    
    def save_to_csv(self, data, filename):
        """保存数据到CSV文件"""
        if not data:
            return
        
        fieldnames = data[0].keys()
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        
        print(f"数据已保存到 {filename}")

# 使用示例
if __name__ == '__main__':
    scraper = SimpleScraper(delay_range=(1, 2))
    
    # 爬取新闻网站
    print("开始爬取新闻...")
    news_articles = scraper.scrape_news_site('https://example-news.com', max_pages=3)
    scraper.save_to_json(news_articles, 'news_articles.json')
    
    # 爬取电商产品
    print("开始爬取产品...")
    products = scraper.scrape_ecommerce_products('https://example-shop.com/category/electronics', max_products=50)
    scraper.save_to_csv(products, 'products.csv')
    
    print(f"爬取完成！获得 {len(news_articles)} 篇文章和 {len(products)} 个产品")
```

#### ✅ 优势
- 学习成本低，适合初学者
- 语法简洁直观
- 文档详细，社区支持好
- 轻量级，依赖少
- 调试容易

#### ❌ 劣势
- 性能相对较低
- 不支持异步处理
- 缺少高级功能
- 大规模爬取效率低

#### 🎯 适用场景
- 小规模数据采集
- 学习和原型开发
- 简单的一次性任务
- 快速验证想法
- 教学演示

---

## 🌐 浏览器自动化框架

### 7. Selenium

#### 🌟 核心特性
- **真实浏览器**: 完全模拟用户行为
- **JavaScript支持**: 处理动态内容
- **多浏览器支持**: Chrome、Firefox、Safari等
- **交互能力**: 点击、输入、滚动等操作
- **截图功能**: 页面截图和元素截图

#### 💰 定价
- **完全免费**: 开源项目，无使用限制

#### 📝 代码示例
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import json
import csv
from datetime import datetime

class SeleniumScraper:
    """Selenium网页爬虫类"""
    
    def __init__(self, headless=True, window_size=(1920, 1080)):
        self.setup_driver(headless, window_size)
        self.wait = WebDriverWait(self.driver, 10)
    
    def setup_driver(self, headless, window_size):
        """设置浏览器驱动"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')
        
        # 基础设置
        chrome_options.add_argument(f'--window-size={window_size[0]},{window_size[1]}')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        
        # 反检测设置
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # 用户代理
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # 执行反检测脚本
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    def get_page(self, url, wait_for_element=None, timeout=10):
        """访问页面并等待加载"""
        try:
            self.driver.get(url)
            
            if wait_for_element:
                self.wait_for_element(wait_for_element, timeout)
            
            return True
        except Exception as e:
            print(f"访问页面失败 {url}: {e}")
            return False
    
    def wait_for_element(self, selector, timeout=10, by=By.CSS_SELECTOR):
        """等待元素出现"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, selector))
            )
            return element
        except TimeoutException:
            print(f"等待元素超时: {selector}")
            return None
    
    def scroll_to_bottom(self, pause_time=1):
        """滚动到页面底部"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # 滚动到底部
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # 等待新内容加载
            time.sleep(pause_time)
            
            # 计算新的滚动高度
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
            
            last_height = new_height
    
    def infinite_scroll(self, max_scrolls=10, pause_time=2):
        """无限滚动加载"""
        for i in range(max_scrolls):
            print(f"滚动第 {i+1} 次...")
            
            # 滚动到底部
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # 等待内容加载
            time.sleep(pause_time)
            
            # 检查是否有"加载更多"按钮
            try:
                load_more_btn = self.driver.find_element(By.CSS_SELECTOR, '.load-more, .show-more, [data-load-more]')
                if load_more_btn.is_displayed():
                    self.driver.execute_script("arguments[0].click();", load_more_btn)
                    time.sleep(pause_time)
            except NoSuchElementException:
                pass
    
    def scrape_social_media_posts(self, profile_url, max_posts=50):
        """爬取社交媒体帖子示例"""
        if not self.get_page(profile_url):
            return []
        
        posts = []
        
        # 等待帖子加载
        self.wait_for_element('.post, .tweet, .feed-item')
        
        # 无限滚动加载更多帖子
        self.infinite_scroll(max_scrolls=10)
        
        # 提取帖子
        post_elements = self.driver.find_elements(By.CSS_SELECTOR, '.post, .tweet, .feed-item')
        
        for i, post_element in enumerate(post_elements[:max_posts]):
            try:
                post_data = self.extract_post_data(post_element)
                if post_data:
                    posts.append(post_data)
                    print(f"已提取第 {i+1} 个帖子")
            except Exception as e:
                print(f"提取帖子失败: {e}")
                continue
        
        return posts
    
    def extract_post_data(self, post_element):
        """提取单个帖子数据"""
        try:
            # 滚动到元素位置
            self.driver.execute_script("arguments[0].scrollIntoView(true);", post_element)
            time.sleep(0.5)
            
            post_data = {
                'text': self.safe_get_text(post_element, '.post-text, .tweet-text, .content'),
                'author': self.safe_get_text(post_element, '.author, .username, .user-name'),
                'timestamp': self.safe_get_text(post_element, '.timestamp, .time, .date'),
                'likes': self.extract_count(post_element, '.likes, .like-count'),
                'shares': self.extract_count(post_element, '.shares, .retweet-count'),
                'comments': self.extract_count(post_element, '.comments, .reply-count'),
                'images': self.extract_images(post_element),
                'links': self.extract_links(post_element),
                'hashtags': self.extract_hashtags(post_element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return post_data
            
        except Exception as e:
            print(f"提取帖子数据失败: {e}")
            return None
    
    def safe_get_text(self, parent_element, selector):
        """安全获取元素文本"""
        try:
            element = parent_element.find_element(By.CSS_SELECTOR, selector)
            return element.text.strip()
        except NoSuchElementException:
            return None
    
    def extract_count(self, parent_element, selector):
        """提取数量（点赞、分享等）"""
        text = self.safe_get_text(parent_element, selector)
        if text:
            import re
            # 处理K、M等单位
            if 'K' in text.upper():
                number = re.search(r'([\d.]+)K', text.upper())
                if number:
                    return int(float(number.group(1)) * 1000)
            elif 'M' in text.upper():
                number = re.search(r'([\d.]+)M', text.upper())
                if number:
                    return int(float(number.group(1)) * 1000000)
            else:
                number = re.search(r'\d+', text)
                if number:
                    return int(number.group())
        return 0
    
    def extract_images(self, parent_element):
        """提取图片链接"""
        try:
            img_elements = parent_element.find_elements(By.CSS_SELECTOR, 'img')
            return [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]
        except:
            return []
    
    def extract_links(self, parent_element):
        """提取链接"""
        try:
            link_elements = parent_element.find_elements(By.CSS_SELECTOR, 'a')
            return [link.get_attribute('href') for link in link_elements if link.get_attribute('href')]
        except:
            return []
    
    def extract_hashtags(self, parent_element):
        """提取话题标签"""
        text = self.safe_get_text(parent_element, '.post-text, .tweet-text, .content')
        if text:
            import re
            hashtags = re.findall(r'#\w+', text)
            return hashtags
        return []
    
    def scrape_ecommerce_with_filters(self, base_url, filters=None, max_products=100):
        """爬取电商网站（带筛选功能）"""
        if not self.get_page(base_url):
            return []
        
        # 应用筛选条件
        if filters:
            self.apply_filters(filters)
        
        products = []
        page = 1
        
        while len(products) < max_products:
            print(f"正在爬取第 {page} 页...")
            
            # 等待产品加载
            self.wait_for_element('.product-item, .product-card')
            
            # 滚动加载更多产品
            self.scroll_to_bottom()
            
            # 提取当前页面的产品
            product_elements = self.driver.find_elements(By.CSS_SELECTOR, '.product-item, .product-card')
            
            current_page_products = []
            for product_element in product_elements[len(products):]:
                if len(products) >= max_products:
                    break
                
                product_data = self.extract_product_data(product_element)
                if product_data:
                    products.append(product_data)
                    current_page_products.append(product_data)
            
            if not current_page_products:
                print("没有找到新产品，停止爬取")
                break
            
            # 尝试点击下一页
            if not self.go_to_next_page():
                print("没有下一页，爬取完成")
                break
            
            page += 1
        
        return products
    
    def apply_filters(self, filters):
        """应用筛选条件"""
        for filter_type, filter_value in filters.items():
            try:
                if filter_type == 'price_range':
                    # 设置价格范围
                    min_price, max_price = filter_value
                    
                    min_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name="min_price"], .price-min')
                    min_input.clear()
                    min_input.send_keys(str(min_price))
                    
                    max_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name="max_price"], .price-max')
                    max_input.clear()
                    max_input.send_keys(str(max_price))
                    
                elif filter_type == 'category':
                    # 选择分类
                    category_element = self.driver.find_element(By.XPATH, f"//a[contains(text(), '{filter_value}')]")
                    category_element.click()
                    
                elif filter_type == 'brand':
                    # 选择品牌
                    brand_checkbox = self.driver.find_element(By.XPATH, f"//input[@value='{filter_value}' or following-sibling::text()[contains(., '{filter_value}')]]")
                    if not brand_checkbox.is_selected():
                        brand_checkbox.click()
                
                time.sleep(1)  # 等待筛选生效
                
            except NoSuchElementException:
                print(f"未找到筛选条件: {filter_type} = {filter_value}")
    
    def extract_product_data(self, product_element):
        """提取产品数据"""
        try:
            # 滚动到产品位置
            self.driver.execute_script("arguments[0].scrollIntoView(true);", product_element)
            time.sleep(0.3)
            
            product_data = {
                'name': self.safe_get_text(product_element, '.product-name, .title, h3'),
                'price': self.extract_price(product_element),
                'original_price': self.extract_original_price(product_element),
                'rating': self.extract_rating(product_element),
                'reviews_count': self.extract_reviews_count(product_element),
                'image': self.safe_get_attribute(product_element, 'img', 'src'),
                'link': self.safe_get_attribute(product_element, 'a', 'href'),
                'availability': self.safe_get_text(product_element, '.stock, .availability'),
                'discount': self.extract_discount(product_element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return product_data
            
        except Exception as e:
            print(f"提取产品数据失败: {e}")
            return None
    
    def safe_get_attribute(self, parent_element, selector, attribute):
        """安全获取元素属性"""
        try:
            element = parent_element.find_element(By.CSS_SELECTOR, selector)
            return element.get_attribute(attribute)
        except NoSuchElementException:
            return None
    
    def extract_price(self, product_element):
        """提取价格"""
        price_text = self.safe_get_text(product_element, '.price, .current-price, .sale-price')
        if price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_original_price(self, product_element):
        """提取原价"""
        original_price_text = self.safe_get_text(product_element, '.original-price, .regular-price, .was-price')
        if original_price_text:
            import re
            price_match = re.search(r'[\d,]+\.\d+', original_price_text.replace(',', ''))
            if price_match:
                return float(price_match.group())
        return None
    
    def extract_rating(self, product_element):
        """提取评分"""
        # 尝试多种评分格式
        rating_element = product_element.find_elements(By.CSS_SELECTOR, '.rating, .stars, .score')
        
        for element in rating_element:
            # 检查data属性
            rating = element.get_attribute('data-rating') or element.get_attribute('data-score')
            if rating:
                try:
                    return float(rating)
                except ValueError:
                    pass
            
            # 检查文本内容
            rating_text = element.text
            if rating_text:
                import re
                rating_match = re.search(r'([\d.]+)', rating_text)
                if rating_match:
                    try:
                        return float(rating_match.group(1))
                    except ValueError:
                        pass
        
        return None
    
    def extract_reviews_count(self, product_element):
        """提取评论数量"""
        reviews_text = self.safe_get_text(product_element, '.reviews, .review-count, .comments')
        if reviews_text:
            import re
            count_match = re.search(r'\d+', reviews_text)
            if count_match:
                return int(count_match.group())
        return 0
    
    def extract_discount(self, product_element):
        """提取折扣信息"""
        discount_text = self.safe_get_text(product_element, '.discount, .sale, .off')
        if discount_text:
            import re
            discount_match = re.search(r'(\d+)%', discount_text)
            if discount_match:
                return int(discount_match.group(1))
        return None
    
    def go_to_next_page(self):
        """转到下一页"""
        try:
            # 尝试多种下一页按钮选择器
            next_selectors = [
                '.next-page',
                '.pagination .next',
                'a[aria-label="Next"]',
                '.pager .next',
                '[data-page="next"]'
            ]
            
            for selector in next_selectors:
                try:
                    next_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if next_button.is_enabled() and next_button.is_displayed():
                        self.driver.execute_script("arguments[0].click();", next_button)
                        time.sleep(2)
                        return True
                except NoSuchElementException:
                    continue
            
            return False
            
        except Exception as e:
            print(f"转到下一页失败: {e}")
            return False
    
    def take_screenshot(self, filename=None):
        """截图"""
        if not filename:
            filename = f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        
        self.driver.save_screenshot(filename)
        print(f"截图已保存: {filename}")
        return filename
    
    def save_data(self, data, filename, format='json'):
        """保存数据"""
        if format == 'json':
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        elif format == 'csv':
            if data:
                fieldnames = data[0].keys()
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
        
        print(f"数据已保存到 {filename}")
    
    def close(self):
        """关闭浏览器"""
        self.driver.quit()

# 使用示例
if __name__ == '__main__':
    scraper = SeleniumScraper(headless=False)  # 显示浏览器窗口用于调试
    
    try:
        # 爬取社交媒体帖子
        print("开始爬取社交媒体帖子...")
        posts = scraper.scrape_social_media_posts('https://example-social.com/profile', max_posts=20)
        scraper.save_data(posts, 'social_posts.json')
        
        # 爬取电商产品（带筛选）
        print("开始爬取电商产品...")
        filters = {
            'price_range': (100, 500),
            'brand': 'Apple',
            'category': 'Electronics'
        }
        products = scraper.scrape_ecommerce_with_filters(
            'https://example-shop.com/search',
            filters=filters,
            max_products=30
        )
        scraper.save_data(products, 'filtered_products.csv', format='csv')
        
        print(f"爬取完成！获得 {len(posts)} 个帖子和 {len(products)} 个产品")
        
    finally:
        scraper.close()
```

#### ✅ 优势
- 完全模拟真实用户行为
- 支持复杂的JavaScript应用
- 可以处理各种交互操作
- 支持多种浏览器
- 功能强大，扩展性好

#### ❌ 劣势
- 资源消耗大，速度慢
- 容易被检测
- 维护成本高
- 稳定性相对较差

#### 🎯 适用场景
- 重度JavaScript应用
- 需要用户交互的网站
- 复杂的表单提交
- 需要截图的场景
- 测试和调试

---

### 8. Playwright

#### 🌟 核心特性
- **现代化设计**: 专为现代Web应用设计
- **多浏览器支持**: Chrome、Firefox、Safari、Edge
- **高性能**: 比Selenium更快更稳定
- **自动等待**: 智能等待元素加载
- **网络拦截**: 可以拦截和修改网络请求

#### 💰 定价
- **完全免费**: 开源项目，无使用限制

#### 📝 代码示例
```python
from playwright.sync_api import sync_playwright
import json
import time
from datetime import datetime

class PlaywrightScraper:
    """Playwright网页爬虫类"""
    
    def __init__(self, headless=True, browser_type='chromium'):
        self.playwright = sync_playwright().start()
        self.browser_type = browser_type
        self.headless = headless
        self.browser = None
        self.context = None
        self.page = None
        
        self.setup_browser()
    
    def setup_browser(self):
        """设置浏览器"""
        # 选择浏览器类型
        if self.browser_type == 'chromium':
            self.browser = self.playwright.chromium.launch(headless=self.headless)
        elif self.browser_type == 'firefox':
            self.browser = self.playwright.firefox.launch(headless=self.headless)
        elif self.browser_type == 'webkit':
            self.browser = self.playwright.webkit.launch(headless=self.headless)
        
        # 创建浏览器上下文
        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        
        # 创建页面
        self.page = self.context.new_page()
        
        # 设置默认超时
        self.page.set_default_timeout(30000)
    
    def scrape_spa_application(self, url, max_items=50):
        """爬取SPA应用示例"""
        try:
            print(f"正在访问: {url}")
            self.page.goto(url)
            
            # 等待应用加载
            self.page.wait_for_selector('.app-container, #app, .main-content')
            
            items = []
            
            # 模拟无限滚动
            while len(items) < max_items:
                # 等待内容加载
                self.page.wait_for_selector('.item, .card, .post', timeout=5000)
                
                # 获取当前页面的所有项目
                current_items = self.page.query_selector_all('.item, .card, .post')
                
                # 提取新项目
                for item_element in current_items[len(items):]:
                    if len(items) >= max_items:
                        break
                    
                    item_data = self.extract_item_data(item_element)
                    if item_data:
                        items.append(item_data)
                        print(f"已提取第 {len(items)} 个项目")
                
                # 滚动加载更多
                self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                
                # 等待新内容加载
                try:
                    self.page.wait_for_function(
                        f'document.querySelectorAll(".item, .card, .post").length > {len(current_items)}',
                        timeout=5000
                    )
                except:
                    print("没有更多内容加载")
                    break
            
            return items
            
        except Exception as e:
            print(f"爬取SPA应用失败: {e}")
            return []
    
    def extract_item_data(self, element):
        """提取项目数据"""
        try:
            # 滚动到元素位置
            element.scroll_into_view_if_needed()
            
            item_data = {
                'title': self.safe_get_text(element, '.title, h2, h3'),
                'description': self.safe_get_text(element, '.description, .summary, p'),
                'link': self.safe_get_attribute(element, 'a', 'href'),
                'image': self.safe_get_attribute(element, 'img', 'src'),
                'date': self.safe_get_text(element, '.date, .time, .timestamp'),
                'author': self.safe_get_text(element, '.author, .user, .creator'),
                'tags': self.extract_tags(element),
                'scraped_at': datetime.now().isoformat()
            }
            
            return item_data
            
        except Exception as e:
            print(f"提取项目数据失败: {e}")
            return None
    
    def safe_get_text(self, element, selector):
        """安全获取文本内容"""
        try:
            target = element.query_selector(selector)
            return target.text_content().strip() if target else None
        except:
            return None
    
    def safe_get_attribute(self, element, selector, attribute):
        """安全获取属性值"""
        try:
            target = element.query_selector(selector)
            return target.get_attribute(attribute) if target else None
        except:
            return None
    
    def extract_tags(self, element):
        """提取标签"""
        try:
            tag_elements = element.query_selector_all('.tag, .label, .category')
            return [tag.text_content().strip() for tag in tag_elements]
        except:
            return []
    
    def scrape_with_authentication(self, login_url, username, password, target_url):
        """带认证的爬取"""
        try:
            # 访问登录页面
            print("正在登录...")
            self.page.goto(login_url)
            
            # 填写登录表单
            self.page.fill('input[name="username"], input[type="email"]', username)
            self.page.fill('input[name="password"], input[type="password"]', password)
            
            # 点击登录按钮
            self.page.click('button[type="submit"], .login-btn, .submit-btn')
            
            # 等待登录完成
            self.page.wait_for_url('**/dashboard**', timeout=10000)
            print("登录成功")
            
            # 访问目标页面
            self.page.goto(target_url)
            
            # 等待页面加载
            self.page.wait_for_load_state('networkidle')
            
            # 提取数据
            data = self.extract_authenticated_data()
            
            return data
            
        except Exception as e:
            print(f"认证爬取失败: {e}")
            return []
    
    def extract_authenticated_data(self):
        """提取需要认证的数据"""
        try:
            # 等待内容加载
            self.page.wait_for_selector('.protected-content, .user-data')
            
            data = []
            
            # 提取用户数据
            user_elements = self.page.query_selector_all('.user-item, .profile-card')
            
            for element in user_elements:
                user_data = {
                    'name': self.safe_get_text(element, '.name, .username'),
                    'email': self.safe_get_text(element, '.email'),
                    'status': self.safe_get_text(element, '.status'),
                    'last_login': self.safe_get_text(element, '.last-login'),
                    'role': self.safe_get_text(element, '.role, .permission'),
                    'avatar': self.safe_get_attribute(element, 'img', 'src')
                }
                data.append(user_data)
            
            return data
            
        except Exception as e:
            print(f"提取认证数据失败: {e}")
            return []
    
    def scrape_with_network_interception(self, url):
        """带网络拦截的爬取"""
        api_responses = []
        
        # 设置网络拦截
        def handle_response(response):
            if '/api/' in response.url and response.status == 200:
                try:
                    api_data = response.json()
                    api_responses.append({
                        'url': response.url,
                        'data': api_data,
                        'timestamp': datetime.now().isoformat()
                    })
                except:
                    pass
        
        self.page.on('response', handle_response)
        
        try:
            # 访问页面
            self.page.goto(url)
            
            # 等待API调用完成
            self.page.wait_for_load_state('networkidle')
            
            # 触发更多API调用
            self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            time.sleep(2)
            
            # 点击加载更多按钮
            load_more_btn = self.page.query_selector('.load-more, .show-more')
            if load_more_btn:
                load_more_btn.click()
                self.page.wait_for_load_state('networkidle')
            
            return api_responses
            
        except Exception as e:
            print(f"网络拦截爬取失败: {e}")
            return api_responses
    
    def save_data(self, data, filename, format='json'):
        """保存数据"""
        if format == 'json':
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"数据已保存到 {filename}")
    
    def close(self):
        """关闭浏览器"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()

# 使用示例
if __name__ == '__main__':
    scraper = PlaywrightScraper(headless=False, browser_type='chromium')
    
    try:
        # 爬取SPA应用
        print("开始爬取SPA应用...")
        spa_data = scraper.scrape_spa_application('https://example-spa.com', max_items=30)
        scraper.save_data(spa_data, 'spa_data.json')
        
        # 带认证的爬取
        print("开始认证爬取...")
        auth_data = scraper.scrape_with_authentication(
            'https://example.com/login',
            'your_username',
            'your_password',
            'https://example.com/protected-data'
        )
        scraper.save_data(auth_data, 'auth_data.json')
        
        # 网络拦截爬取
        print("开始网络拦截爬取...")
        api_data = scraper.scrape_with_network_interception('https://example-api.com')
        scraper.save_data(api_data, 'api_responses.json')
        
        print("爬取完成！")
        
    finally:
        scraper.close()
```

#### ✅ 优势
- 性能优秀，比Selenium更快
- 现代化API设计
- 自动等待机制
- 网络拦截功能强大
- 多浏览器支持好

#### ❌ 劣势
- 相对较新，生态不如Selenium
- 学习资源相对较少
- 某些高级功能需要深入了解

#### 🎯 适用场景
- 现代SPA应用爬取
- 需要高性能的场景
- API数据拦截
- 自动化测试
- 复杂交互场景

---

## 🔧 专业工具

### 9. Apify

#### 🌟 核心特性
- **云端平台**: 完全托管的爬虫服务
- **预置爬虫**: 丰富的现成爬虫模板
- **可视化编辑**: 无代码爬虫构建
- **数据存储**: 内置数据存储和导出
- **API集成**: 完整的REST API

#### 💰 定价
- **免费套餐**: $5免费额度/月
- **Personal**: $49/月
- **Team**: $499/月
- **Enterprise**: 定制价格

#### 📝 代码示例
```python
from apify_client import ApifyClient
import json
from datetime import datetime

class ApifyScraper:
    """Apify爬虫客户端"""
    
    def __init__(self, api_token):
        self.client = ApifyClient(api_token)
    
    def scrape_google_search(self, queries, max_results=100):
        """使用Google搜索爬虫"""
        try:
            # 运行Google搜索爬虫
            run_input = {
                "queries": queries,
                "maxPagesPerQuery": max_results // 10,
                "resultsPerPage": 10,
                "countryCode": "US",
                "languageCode": "en",
                "includeUnfurlData": True
            }
            
            run = self.client.actor("apify/google-search-scraper").call(run_input=run_input)
            
            # 获取结果
            results = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                results.append(item)
            
            return results
            
        except Exception as e:
            print(f"Google搜索爬取失败: {e}")
            return []
    
    def scrape_amazon_products(self, search_terms, max_items=50):
        """使用Amazon产品爬虫"""
        try:
            run_input = {
                "searchTerms": search_terms,
                "maxItems": max_items,
                "country": "US",
                "includeReviews": True,
                "includeImages": True
            }
            
            run = self.client.actor("apify/amazon-product-scraper").call(run_input=run_input)
            
            products = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                products.append(item)
            
            return products
            
        except Exception as e:
            print(f"Amazon产品爬取失败: {e}")
            return []
    
    def scrape_instagram_posts(self, usernames, max_posts=30):
        """使用Instagram爬虫"""
        try:
            run_input = {
                "usernames": usernames,
                "resultsLimit": max_posts,
                "includeComments": True,
                "includeHashtags": True
            }
            
            run = self.client.actor("apify/instagram-scraper").call(run_input=run_input)
            
            posts = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                posts.append(item)
            
            return posts
            
        except Exception as e:
            print(f"Instagram爬取失败: {e}")
            return []
    
    def create_custom_scraper(self, start_urls, page_function):
        """创建自定义爬虫"""
        try:
            # 创建新的Actor
            actor_input = {
                "startUrls": start_urls,
                "pageFunction": page_function,
                "maxRequestsPerCrawl": 100,
                "maxConcurrency": 10
            }
            
            run = self.client.actor("apify/web-scraper").call(run_input=actor_input)
            
            results = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                results.append(item)
            
            return results
            
        except Exception as e:
            print(f"自定义爬虫运行失败: {e}")
            return []

# 使用示例
if __name__ == '__main__':
    scraper = ApifyScraper("your-apify-token")
    
    # Google搜索爬取
    search_queries = ["AI technology 2024", "machine learning trends"]
    search_results = scraper.scrape_google_search(search_queries, max_results=50)
    print(f"获得 {len(search_results)} 个搜索结果")
    
    # Amazon产品爬取
    amazon_terms = ["laptop", "smartphone"]
    amazon_products = scraper.scrape_amazon_products(amazon_terms, max_items=30)
    print(f"获得 {len(amazon_products)} 个Amazon产品")
    
    # Instagram帖子爬取
    instagram_users = ["tech_influencer", "ai_researcher"]
    instagram_posts = scraper.scrape_instagram_posts(instagram_users, max_posts=20)
    print(f"获得 {len(instagram_posts)} 个Instagram帖子")
```

#### ✅ 优势
- 完全托管，无需维护基础设施
- 丰富的预置爬虫
- 强大的反爬虫能力
- 数据存储和处理一体化
- 良好的监控和日志

#### ❌ 劣势
- 成本相对较高
- 定制化程度有限
- 依赖第三方平台
- 学习曲线较陡

#### 🎯 适用场景
- 大规模商业爬取
- 需要高可靠性的项目
- 团队协作开发
- 复杂的反爬虫场景

---

### 10. Scrapy Cloud

#### 🌟 核心特性
- **Scrapy托管**: 专为Scrapy设计的云平台
- **自动扩展**: 根据需求自动扩展资源
- **监控告警**: 实时监控爬虫状态
- **数据管道**: 灵活的数据处理流水线
- **版本管理**: 代码版本控制和部署

#### 💰 定价
- **免费套餐**: 1个并发单元
- **Professional**: $9/月/单元
- **Business**: 定制价格

#### 📝 代码示例
```python
# scrapy项目结构
# myproject/
# ├── scrapy.cfg
# ├── myproject/
# │   ├── __init__.py
# │   ├── items.py
# │   ├── middlewares.py
# │   ├── pipelines.py
# │   ├── settings.py
# │   └── spiders/
# │       ├── __init__.py
# │       └── example_spider.py

# items.py
import scrapy

class ProductItem(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    description = scrapy.Field()
    image_urls = scrapy.Field()
    images = scrapy.Field()
    url = scrapy.Field()
    scraped_at = scrapy.Field()

# example_spider.py
import scrapy
from myproject.items import ProductItem
from datetime import datetime

class ProductSpider(scrapy.Spider):
    name = 'products'
    allowed_domains = ['example-shop.com']
    start_urls = ['https://example-shop.com/products']
    
    custom_settings = {
        'ITEM_PIPELINES': {
            'myproject.pipelines.ValidationPipeline': 300,
            'scrapy.pipelines.images.ImagesPipeline': 400,
            'myproject.pipelines.ScrapyCloudPipeline': 500,
        },
        'IMAGES_STORE': 's3://my-bucket/images/',
        'DOWNLOAD_DELAY': 1,
        'AUTOTHROTTLE_ENABLED': True,
    }
    
    def parse(self, response):
        # 提取产品链接
        product_links = response.css('.product-item a::attr(href)').getall()
        
        for link in product_links:
            yield response.follow(link, self.parse_product)
        
        # 处理分页
        next_page = response.css('.pagination .next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_product(self, response):
        item = ProductItem()
        
        item['name'] = response.css('h1.product-title::text').get()
        item['price'] = response.css('.price .amount::text').re_first(r'\d+\.\d+')
        item['description'] = ' '.join(response.css('.product-description p::text').getall())
        item['image_urls'] = response.css('.product-gallery img::attr(src)').getall()
        item['url'] = response.url
        item['scraped_at'] = datetime.now().isoformat()
        
        yield item

# pipelines.py
import json
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class ValidationPipeline:
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        if not adapter.get('name') or not adapter.get('price'):
            raise DropItem(f"缺少必要字段: {item}")
        
        # 价格格式化
        if adapter.get('price'):
            try:
                adapter['price'] = float(adapter['price'])
            except ValueError:
                adapter['price'] = None
        
        return item

class ScrapyCloudPipeline:
    def process_item(self, item, spider):
        # 发送到Scrapy Cloud的数据存储
        return item

# settings.py
BOT_NAME = 'myproject'

SPIDER_MODULES = ['myproject.spiders']
NEWSPIDER_MODULE = 'myproject.spiders'

# Scrapy Cloud设置
SHUB_JOBKEY = None  # 由Scrapy Cloud自动设置

# 部署到Scrapy Cloud
# 1. 安装shub命令行工具: pip install shub
# 2. 登录: shub login
# 3. 部署: shub deploy

# 使用Scrapy Cloud API
import requests

class ScrapyCloudAPI:
    def __init__(self, api_key, project_id):
        self.api_key = api_key
        self.project_id = project_id
        self.base_url = "https://app.scrapinghub.com/api"
    
    def schedule_spider(self, spider_name, **kwargs):
        """调度爬虫运行"""
        url = f"{self.base_url}/schedule.json"
        data = {
            'project': self.project_id,
            'spider': spider_name,
            **kwargs
        }
        
        response = requests.post(url, data=data, auth=(self.api_key, ''))
        return response.json()
    
    def get_job_status(self, job_id):
        """获取任务状态"""
        url = f"{self.base_url}/jobs/list.json"
        params = {
            'project': self.project_id,
            'job': job_id
        }
        
        response = requests.get(url, params=params, auth=(self.api_key, ''))
        return response.json()
    
    def get_job_items(self, job_id):
        """获取任务结果"""
        url = f"{self.base_url}/items.json"
        params = {
            'project': self.project_id,
            'job': job_id
        }
        
        response = requests.get(url, params=params, auth=(self.api_key, ''))
        
        items = []
        for line in response.text.strip().split('\n'):
            if line:
                items.append(json.loads(line))
        
        return items

# 使用示例
if __name__ == '__main__':
    api = ScrapyCloudAPI('your-api-key', 'your-project-id')
    
    # 调度爬虫
    job = api.schedule_spider('products', setting='DOWNLOAD_DELAY=2')
    job_id = job['jobid']
    print(f"任务已调度，ID: {job_id}")
    
    # 检查状态
    import time
    while True:
        status = api.get_job_status(job_id)
        if status['jobs'][0]['state'] == 'finished':
            print("任务完成")
            break
        elif status['jobs'][0]['state'] == 'running':
            print("任务运行中...")
            time.sleep(30)
        else:
            print(f"任务状态: {status['jobs'][0]['state']}")
            break
    
    # 获取结果
    items = api.get_job_items(job_id)
    print(f"获得 {len(items)} 个结果")
```

#### ✅ 优势
- 专为Scrapy优化
- 自动扩展和负载均衡
- 完整的监控和日志
- 版本控制和部署简单
- 与Scrapy生态完美集成

#### ❌ 劣势
- 仅支持Scrapy框架
- 成本随规模增长
- 学习曲线较陡
- 定制化程度有限

#### 🎯 适用场景
- 大规模Scrapy项目
- 需要高可用性的爬虫
- 团队协作开发
- 生产环境部署

---

## 📊 工具对比总结

### 按使用场景分类

#### 🚀 快速上手（初学者）
1. **Firecrawl** - AI驱动，最简单
2. **Beautiful Soup + Requests** - 语法简洁
3. **Octoparse** - 可视化操作

#### ⚡ 高性能需求
1. **Scrapy** - 异步处理，速度最快
2. **Playwright** - 现代化，性能优秀
3. **Scrapfly** - 云端处理，并发能力强

#### 🎯 商业应用
1. **Firecrawl** - AI智能，反爬虫能力强
2. **Apify** - 完全托管，可靠性高
3. **Scrapy Cloud** - 企业级部署

#### 🔧 技术深度
1. **Scrapy** - 最灵活，可扩展性最强
2. **Selenium** - 功能最全面
3. **Playwright** - 现代化架构

### 成本对比

| 工具 | 免费额度 | 付费起价 | 适合规模 |
|------|----------|----------|----------|
| Firecrawl | 500次/月 | $20/月 | 中小型 |
| Scrapy | 完全免费 | - | 所有规模 |
| Beautiful Soup | 完全免费 | - | 小型 |
| Selenium | 完全免费 | - | 所有规模 |
| Playwright | 完全免费 | - | 所有规模 |
| Octoparse | 有限功能 | $75/月 | 中型 |
| ParseHub | 200页/月 | $149/月 | 中型 |
| Scrapfly | 1000次/月 | $15/月 | 中大型 |
| Apify | $5额度/月 | $49/月 | 大型 |
| Scrapy Cloud | 1并发单元 | $9/月 | 大型 |

### 技术特性对比

| 特性 | Firecrawl | Scrapy | Selenium | Playwright | Beautiful Soup |
|------|-----------|--------|----------|------------|----------------|
| 学习难度 | ⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |
| 性能 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| 反爬虫能力 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐ |
| JavaScript支持 | ⭐⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ |
| 可扩展性 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| 维护成本 | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |

## 🎯 选择建议

### 根据项目需求选择

#### 新手入门项目
- **推荐**: Firecrawl + Beautiful Soup
- **理由**: 学习成本低，快速上手

#### 中小型商业项目
- **推荐**: Firecrawl + Scrapy
- **理由**: 平衡了易用性和性能

#### 大型企业项目
- **推荐**: Scrapy + Apify/Scrapy Cloud
- **理由**: 高性能，高可靠性

#### JavaScript重度应用
- **推荐**: Playwright + Firecrawl
- **理由**: 现代化架构，处理能力强

#### 预算有限项目
- **推荐**: Scrapy + Beautiful Soup
- **理由**: 完全免费，功能强大

---

## 📚 学习资源

### 官方文档
- [Firecrawl文档](https://docs.firecrawl.dev/)
- [Scrapy文档](https://docs.scrapy.org/)
- [Selenium文档](https://selenium-python.readthedocs.io/)
- [Playwright文档](https://playwright.dev/python/)
- [Beautiful Soup文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

### 社区资源
- [Scrapy中文社区](https://scrapy-chs.readthedocs.io/)
- [Python爬虫学习指南](https://github.com/Kr1s77/awesome-python-login-model)
- [网络爬虫最佳实践](https://github.com/lorien/awesome-web-scraping)

### 在线课程
- [Python网络爬虫实战](https://www.coursera.org/learn/python-web-scraping)
- [Scrapy框架深入学习](https://www.udemy.com/course/scrapy-tutorial-web-scraping-with-python/)
- [现代网络爬虫技术](https://www.edx.org/course/web-scraping-with-python)

---

## 🔮 未来趋势

### AI驱动的发展方向
1. **智能化提取**: 更准确的内容识别
2. **自适应爬取**: 自动适应网站变化
3. **语义理解**: 基于内容语义的数据提取
4. **自动化配置**: 减少人工配置需求

### 技术发展趋势
1. **无服务器架构**: 更灵活的部署方式
2. **边缘计算**: 降低延迟，提高效率
3. **隐私保护**: 更强的数据保护机制
4. **实时处理**: 流式数据处理能力

### 行业应用趋势
1. **垂直领域专业化**: 针对特定行业的解决方案
2. **数据质量提升**: 更高质量的数据清洗和验证
3. **合规性增强**: 更好的法律法规遵循
4. **成本优化**: 更经济的大规模爬取方案

---

## 📝 总结

网络爬虫工具的选择应该基于具体的项目需求、技术水平和预算考虑。对于初学者，建议从Firecrawl或Beautiful Soup开始；对于有经验的开发者，Scrapy是最佳选择；对于企业级应用，可以考虑Apify或Scrapy Cloud等托管服务。

无论选择哪种工具，都要注意遵守网站的robots.txt协议，尊重数据版权，合理控制爬取频率，确保爬虫活动的合法性和道德性。

随着AI技术的发展，像Firecrawl这样的智能化工具将成为未来的主流趋势，它们能够大大降低网络爬虫的技术门槛，提高数据采集的效率和质量。

---

*本文持续更新，欢迎关注最新的网络爬虫技术发展动态。*