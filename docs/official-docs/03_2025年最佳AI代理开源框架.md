# The Best Open Source Frameworks For Building AI Agents in 2025

**ä½œè€…**: Abid Ali Awan  
**å‘å¸ƒæ—¶é—´**: Aug 15, 2025  
**åŸæ–‡é“¾æ¥**: https://www.firecrawl.dev/blog/best-open-source-frameworks-building-ai-agents-2025  
**åˆ†ç±»**: AI Agents, Open Source, Framework Comparison

## æ‘˜è¦

æœ¬æ–‡æ·±å…¥åˆ†æäº†2025å¹´æ„å»ºAIä»£ç†çš„æœ€ä½³å¼€æºæ¡†æ¶ï¼Œæ¶µç›–äº†ä»ç®€å•èŠå¤©æœºå™¨äººåˆ°å¤æ‚ä¼ä¸šçº§ä»£ç†ç³»ç»Ÿçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« åŸºäºFirecrawl FIRE-1æ•°æ®æ”¶é›†ä»£ç†çš„å®é™…æµ‹è¯•å’Œè¯„ä¼°ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æƒå¨çš„æ¡†æ¶é€‰æ‹©æŒ‡å—ã€‚

## AIä»£ç†å¸‚åœºç°çŠ¶

### å¸‚åœºå¢é•¿è¶‹åŠ¿

æ ¹æ®æœ€æ–°çš„å¸‚åœºç ”ç©¶æ•°æ®ï¼š

- **å¸‚åœºè§„æ¨¡**: 2025å¹´AIä»£ç†å¸‚åœºé¢„è®¡è¾¾åˆ°420äº¿ç¾å…ƒ
- **å¢é•¿ç‡**: å¹´å¤åˆå¢é•¿ç‡(CAGR)è¾¾åˆ°35.8%
- **ä¼ä¸šé‡‡ç”¨**: 78%çš„ä¼ä¸šè®¡åˆ’åœ¨2025å¹´éƒ¨ç½²AIä»£ç†
- **å¼€æºè¶‹åŠ¿**: å¼€æºæ¡†æ¶å æ®65%çš„å¸‚åœºä»½é¢

### æŠ€æœ¯å‘å±•æ–¹å‘

1. **å¤šæ¨¡æ€èƒ½åŠ›**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»¼åˆå¤„ç†
2. **è‡ªä¸»å†³ç­–**: æ›´å¼ºçš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›
3. **å·¥å…·é›†æˆ**: ä¸å¤–éƒ¨APIå’ŒæœåŠ¡çš„æ— ç¼é›†æˆ
4. **å¯è§£é‡Šæ€§**: æä¾›å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦
5. **å®‰å…¨æ€§**: å¢å¼ºçš„å®‰å…¨æ§åˆ¶å’Œéšç§ä¿æŠ¤

## è¯„ä¼°æ–¹æ³•è®º

### Firecrawl FIRE-1 æ•°æ®æ”¶é›†ä»£ç†

æˆ‘ä»¬ä½¿ç”¨Firecrawlå¼€å‘çš„FIRE-1ä»£ç†ç³»ç»Ÿå¯¹å„ä¸ªæ¡†æ¶è¿›è¡Œäº†å…¨é¢æµ‹è¯•ï¼š

```python
class FIRE1Agent:
    """Firecrawl FIRE-1 æ•°æ®æ”¶é›†ä»£ç†"""
    
    def __init__(self, framework_name):
        self.framework = framework_name
        self.test_scenarios = [
            "web_scraping_task",
            "data_analysis_task", 
            "api_integration_task",
            "multi_step_workflow",
            "error_handling_test"
        ]
        self.metrics = {
            "performance": 0,
            "ease_of_use": 0,
            "scalability": 0,
            "community_support": 0,
            "documentation_quality": 0
        }
    
    def evaluate_framework(self):
        """è¯„ä¼°æ¡†æ¶æ€§èƒ½"""
        results = {}
        
        for scenario in self.test_scenarios:
            start_time = time.time()
            success_rate = self._run_test_scenario(scenario)
            execution_time = time.time() - start_time
            
            results[scenario] = {
                "success_rate": success_rate,
                "execution_time": execution_time,
                "memory_usage": self._measure_memory_usage(),
                "error_count": self._count_errors()
            }
        
        return results
```

### è¯„ä¼°æ ‡å‡†

æˆ‘ä»¬åŸºäºä»¥ä¸‹äº”ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š

1. **æ€§èƒ½è¡¨ç°** (25%)
   - å“åº”é€Ÿåº¦
   - å†…å­˜ä½¿ç”¨æ•ˆç‡
   - å¹¶å‘å¤„ç†èƒ½åŠ›
   - é”™è¯¯ç‡

2. **æ˜“ç”¨æ€§** (20%)
   - å­¦ä¹ æ›²çº¿
   - APIè®¾è®¡è´¨é‡
   - å¼€å‘ä½“éªŒ
   - è°ƒè¯•å·¥å…·

3. **å¯æ‰©å±•æ€§** (20%)
   - æ¶æ„çµæ´»æ€§
   - æ’ä»¶ç³»ç»Ÿ
   - åˆ†å¸ƒå¼æ”¯æŒ
   - è´Ÿè½½å¤„ç†èƒ½åŠ›

4. **ç¤¾åŒºæ”¯æŒ** (20%)
   - æ´»è·ƒåº¦
   - è´¡çŒ®è€…æ•°é‡
   - é—®é¢˜å“åº”é€Ÿåº¦
   - ç”Ÿæ€ç³»ç»Ÿä¸°å¯Œåº¦

5. **æ–‡æ¡£è´¨é‡** (15%)
   - å®Œæ•´æ€§
   - å‡†ç¡®æ€§
   - ç¤ºä¾‹è´¨é‡
   - æ›´æ–°é¢‘ç‡

## Top 6 å¼€æºAIä»£ç†æ¡†æ¶

### 1. LangChain ğŸ†

**ç»¼åˆè¯„åˆ†**: 9.2/10

#### æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å—åŒ–è®¾è®¡**: é«˜åº¦æ¨¡å—åŒ–çš„ç»„ä»¶æ¶æ„
- **ä¸°å¯Œçš„é›†æˆ**: æ”¯æŒ100+ç§LLMå’Œå·¥å…·
- **å¼ºå¤§çš„é“¾å¼æ“ä½œ**: å¤æ‚çš„å·¥ä½œæµç¼–æ’
- **æ´»è·ƒçš„ç¤¾åŒº**: æœ€å¤§çš„AIä»£ç†å¼€å‘ç¤¾åŒº

#### ä»£ç ç¤ºä¾‹

```python
from langchain.agents import create_openai_functions_agent
from langchain.tools import Tool
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from firecrawl import FirecrawlApp

# åˆ›å»ºFirecrawlå·¥å…·
def firecrawl_scrape(url: str) -> str:
    """ä½¿ç”¨FirecrawlæŠ“å–ç½‘é¡µå†…å®¹"""
    app = FirecrawlApp(api_key="your-api-key")
    result = app.scrape_url(url)
    return result['content']

firecrawl_tool = Tool(
    name="firecrawl_scraper",
    description="æŠ“å–ç½‘é¡µå†…å®¹å¹¶è¿”å›markdownæ ¼å¼",
    func=firecrawl_scrape
)

# åˆ›å»ºä»£ç†
llm = ChatOpenAI(model="gpt-4")

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æ”¶é›†ä»£ç†ï¼Œä½¿ç”¨Firecrawlå·¥å…·æŠ“å–å’Œåˆ†æç½‘é¡µå†…å®¹ã€‚"),
    ("user", "{input}"),
    ("assistant", "{agent_scratchpad}")
])

agent = create_openai_functions_agent(
    llm=llm,
    tools=[firecrawl_tool],
    prompt=prompt
)

# æ‰§è¡Œä»»åŠ¡
from langchain.agents import AgentExecutor

agent_executor = AgentExecutor(
    agent=agent,
    tools=[firecrawl_tool],
    verbose=True
)

result = agent_executor.invoke({
    "input": "è¯·æŠ“å–https://example.comçš„å†…å®¹å¹¶æ€»ç»“ä¸»è¦ä¿¡æ¯"
})
```

#### ä¼˜åŠ¿

- âœ… æœ€æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿ
- âœ… ä¼˜ç§€çš„æ–‡æ¡£å’Œæ•™ç¨‹
- âœ… å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ
- âœ… ä¸°å¯Œçš„é¢„æ„å»ºç»„ä»¶
- âœ… ä¼ä¸šçº§åŠŸèƒ½æ”¯æŒ

#### åŠ£åŠ¿

- âŒ å­¦ä¹ æ›²çº¿è¾ƒé™¡å³­
- âŒ æŸäº›åŠŸèƒ½è¿‡äºå¤æ‚
- âŒ ç‰ˆæœ¬æ›´æ–°é¢‘ç¹å¯èƒ½å¯¼è‡´å…¼å®¹æ€§é—®é¢˜

#### é€‚ç”¨åœºæ™¯

- å¤æ‚çš„ä¼ä¸šçº§AIåº”ç”¨
- éœ€è¦å¤šç§å·¥å…·é›†æˆçš„é¡¹ç›®
- æœ‰ç»éªŒçš„å¼€å‘å›¢é˜Ÿ
- é•¿æœŸç»´æŠ¤çš„äº§å“

### 2. CrewAI ğŸš€

**ç»¼åˆè¯„åˆ†**: 8.8/10

#### æ ¸å¿ƒç‰¹æ€§

- **å¤šä»£ç†åä½œ**: ä¸“æ³¨äºä»£ç†å›¢é˜Ÿåä½œ
- **è§’è‰²å®šä¹‰**: æ¸…æ™°çš„ä»£ç†è§’è‰²å’ŒèŒè´£åˆ†å·¥
- **ä»»åŠ¡ç¼–æ’**: æ™ºèƒ½çš„ä»»åŠ¡åˆ†é…å’Œæ‰§è¡Œ
- **ç®€æ´API**: ç›´è§‚æ˜“ç”¨çš„æ¥å£è®¾è®¡

#### ä»£ç ç¤ºä¾‹

```python
from crewai import Agent, Task, Crew
from crewai_tools import tool
from firecrawl import FirecrawlApp

# å®šä¹‰Firecrawlå·¥å…·
@tool("Firecrawlç½‘é¡µæŠ“å–å·¥å…·")
def firecrawl_scrape_tool(url: str) -> str:
    """ä½¿ç”¨FirecrawlæŠ“å–æŒ‡å®šURLçš„å†…å®¹"""
    app = FirecrawlApp(api_key="your-api-key")
    result = app.scrape_url(url, params={'formats': ['markdown']})
    return result['markdown']

@tool("Firecrawlæ‰¹é‡çˆ¬å–å·¥å…·")
def firecrawl_crawl_tool(url: str, max_pages: int = 10) -> str:
    """ä½¿ç”¨Firecrawlçˆ¬å–æ•´ä¸ªç½‘ç«™"""
    app = FirecrawlApp(api_key="your-api-key")
    crawl_result = app.crawl_url(url, params={
        'limit': max_pages,
        'scrapeOptions': {'formats': ['markdown']}
    })
    return str(crawl_result)

# åˆ›å»ºä¸“ä¸šä»£ç†å›¢é˜Ÿ
data_collector = Agent(
    role='æ•°æ®æ”¶é›†ä¸“å®¶',
    goal='é«˜æ•ˆå‡†ç¡®åœ°æ”¶é›†ç½‘ç»œæ•°æ®',
    backstory="""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æ•°æ®æ”¶é›†ä¸“å®¶ï¼Œ
    æ“…é•¿ä½¿ç”¨Firecrawlç­‰å·¥å…·ä»å„ç§ç½‘ç«™æŠ“å–é«˜è´¨é‡æ•°æ®ã€‚
    ä½ æ³¨é‡æ•°æ®çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚""",
    tools=[firecrawl_scrape_tool, firecrawl_crawl_tool],
    verbose=True
)

data_analyst = Agent(
    role='æ•°æ®åˆ†æå¸ˆ',
    goal='åˆ†æå’Œæ€»ç»“æ”¶é›†åˆ°çš„æ•°æ®',
    backstory="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œ
    èƒ½å¤Ÿä»åŸå§‹æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿå’Œæ¨¡å¼ã€‚
    ä½ æ“…é•¿æ•°æ®æ¸…æ´—ã€åˆ†æå’Œå¯è§†åŒ–ã€‚""",
    verbose=True
)

report_writer = Agent(
    role='æŠ¥å‘Šæ’°å†™å‘˜',
    goal='åˆ›å»ºæ¸…æ™°ä¸“ä¸šçš„åˆ†ææŠ¥å‘Š',
    backstory="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯å†™ä½œä¸“å®¶ï¼Œ
    èƒ½å¤Ÿå°†å¤æ‚çš„æ•°æ®åˆ†æç»“æœè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æŠ¥å‘Šã€‚
    ä½ çš„æŠ¥å‘Šç»“æ„æ¸…æ™°ã€å†…å®¹å‡†ç¡®ã€‚""",
    verbose=True
)

# å®šä¹‰ä»»åŠ¡
collection_task = Task(
    description="""ä½¿ç”¨Firecrawlå·¥å…·æ”¶é›†ä»¥ä¸‹ç½‘ç«™çš„æ•°æ®ï¼š
    1. https://example.com
    2. https://another-site.com
    
    ç¡®ä¿æ”¶é›†å®Œæ•´çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€é“¾æ¥å’Œå…ƒæ•°æ®ã€‚""",
    agent=data_collector,
    expected_output="åŒ…å«æ‰€æœ‰ç½‘ç«™å†…å®¹çš„ç»“æ„åŒ–æ•°æ®é›†åˆ"
)

analysis_task = Task(
    description="""åˆ†ææ”¶é›†åˆ°çš„æ•°æ®ï¼Œè¯†åˆ«ï¼š
    1. ä¸»è¦ä¸»é¢˜å’Œè¶‹åŠ¿
    2. å…³é”®ä¿¡æ¯ç‚¹
    3. æ•°æ®è´¨é‡è¯„ä¼°
    4. æ½œåœ¨çš„æ´å¯Ÿ""",
    agent=data_analyst,
    expected_output="è¯¦ç»†çš„æ•°æ®åˆ†æç»“æœå’Œå‘ç°"
)

report_task = Task(
    description="""åŸºäºæ•°æ®åˆ†æç»“æœåˆ›å»ºç»¼åˆæŠ¥å‘Šï¼š
    1. æ‰§è¡Œæ‘˜è¦
    2. è¯¦ç»†å‘ç°
    3. æ•°æ®å¯è§†åŒ–å»ºè®®
    4. åç»­è¡ŒåŠ¨å»ºè®®""",
    agent=report_writer,
    expected_output="ä¸“ä¸šçš„markdownæ ¼å¼åˆ†ææŠ¥å‘Š"
)

# åˆ›å»ºä»£ç†å›¢é˜Ÿ
crew = Crew(
    agents=[data_collector, data_analyst, report_writer],
    tasks=[collection_task, analysis_task, report_task],
    verbose=2
)

# æ‰§è¡Œä»»åŠ¡
result = crew.kickoff()
print(result)
```

#### ä¼˜åŠ¿

- âœ… å¤šä»£ç†åä½œèƒ½åŠ›å¼º
- âœ… ç›´è§‚çš„è§’è‰²å®šä¹‰
- âœ… ä¼˜ç§€çš„ä»»åŠ¡ç¼–æ’
- âœ… å¿«é€Ÿä¸Šæ‰‹
- âœ… æ´»è·ƒçš„å¼€å‘å›¢é˜Ÿ

#### åŠ£åŠ¿

- âŒ ç›¸å¯¹è¾ƒæ–°ï¼Œç”Ÿæ€ç³»ç»Ÿè¿˜åœ¨å‘å±•
- âŒ é«˜çº§åŠŸèƒ½æœ‰é™
- âŒ å¤§è§„æ¨¡éƒ¨ç½²ç»éªŒä¸è¶³

#### é€‚ç”¨åœºæ™¯

- éœ€è¦å¤šä»£ç†åä½œçš„é¡¹ç›®
- ä¸­å°å‹å›¢é˜Ÿå¿«é€ŸåŸå‹å¼€å‘
- ä»»åŠ¡å¯¼å‘çš„AIåº”ç”¨
- æ•™è‚²å’Œå­¦ä¹ é¡¹ç›®

### 3. AutoGen ğŸ¤–

**ç»¼åˆè¯„åˆ†**: 8.6/10

#### æ ¸å¿ƒç‰¹æ€§

- **å¯¹è¯å¼AI**: ä¸“æ³¨äºå¤šä»£ç†å¯¹è¯ç³»ç»Ÿ
- **Microsoftæ”¯æŒ**: å¾®è½¯ç ”ç©¶é™¢å¼€å‘
- **çµæ´»çš„ä»£ç†é…ç½®**: é«˜åº¦å¯å®šåˆ¶çš„ä»£ç†è¡Œä¸º
- **ç¾¤ç»„å¯¹è¯**: æ”¯æŒå¤æ‚çš„å¤šæ–¹å¯¹è¯

#### ä»£ç ç¤ºä¾‹

```python
import autogen
from firecrawl import FirecrawlApp

# é…ç½®LLM
config_list = [
    {
        "model": "gpt-4",
        "api_key": "your-openai-key"
    }
]

llm_config = {
    "config_list": config_list,
    "temperature": 0.1
}

# åˆ›å»ºFirecrawlå‡½æ•°
def firecrawl_function(url: str, action: str = "scrape") -> str:
    """Firecrawlç½‘é¡µå¤„ç†å‡½æ•°"""
    app = FirecrawlApp(api_key="your-firecrawl-key")
    
    if action == "scrape":
        result = app.scrape_url(url)
        return result['content']
    elif action == "crawl":
        result = app.crawl_url(url, params={'limit': 5})
        return str(result)
    elif action == "search":
        result = app.search(url)  # urlä½œä¸ºæœç´¢æŸ¥è¯¢
        return str(result)
    
    return "ä¸æ”¯æŒçš„æ“ä½œ"

# åˆ›å»ºä¸“ä¸šä»£ç†
user_proxy = autogen.UserProxyAgent(
    name="ç”¨æˆ·ä»£ç†",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": "web_scraping",
        "use_docker": False
    },
    function_map={"firecrawl_function": firecrawl_function}
)

web_scraper = autogen.AssistantAgent(
    name="ç½‘é¡µæŠ“å–ä¸“å®¶",
    llm_config=llm_config,
    system_message="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç½‘é¡µæŠ“å–ä¸“å®¶ã€‚
    ä½ å¯ä»¥ä½¿ç”¨firecrawl_functionæ¥æŠ“å–ç½‘é¡µå†…å®¹ã€çˆ¬å–æ•´ä¸ªç½‘ç«™æˆ–æœç´¢ç›¸å…³å†…å®¹ã€‚
    
    å¯ç”¨æ“ä½œï¼š
    - scrape: æŠ“å–å•ä¸ªç½‘é¡µ
    - crawl: çˆ¬å–æ•´ä¸ªç½‘ç«™
    - search: æœç´¢ç›¸å…³å†…å®¹
    
    è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ“ä½œï¼Œå¹¶æä¾›æ¸…æ™°çš„ç»“æœåˆ†æã€‚"""
)

data_analyst = autogen.AssistantAgent(
    name="æ•°æ®åˆ†æå¸ˆ",
    llm_config=llm_config,
    system_message="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚
    ä½ è´Ÿè´£åˆ†æç½‘é¡µæŠ“å–çš„æ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œ
    è¯†åˆ«æ¨¡å¼å’Œè¶‹åŠ¿ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚
    
    ä½ çš„åˆ†æåº”è¯¥åŒ…æ‹¬ï¼š
    1. æ•°æ®æ¦‚è§ˆ
    2. å…³é”®å‘ç°
    3. è¶‹åŠ¿åˆ†æ
    4. å»ºè®®å’Œåç»­æ­¥éª¤"""
)

# æ³¨å†Œå‡½æ•°
autogen.register_function(
    firecrawl_function,
    caller=web_scraper,
    executor=user_proxy,
    description="ä½¿ç”¨FirecrawlæŠ“å–ç½‘é¡µå†…å®¹ã€çˆ¬å–ç½‘ç«™æˆ–æœç´¢ä¿¡æ¯"
)

# åˆ›å»ºç¾¤ç»„èŠå¤©
groupchat = autogen.GroupChat(
    agents=[user_proxy, web_scraper, data_analyst],
    messages=[],
    max_round=20
)

manager = autogen.GroupChatManager(
    groupchat=groupchat,
    llm_config=llm_config
)

# å¯åŠ¨å¯¹è¯
user_proxy.initiate_chat(
    manager,
    message="""è¯·å¸®æˆ‘åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™ https://competitor.com çš„å†…å®¹ç»“æ„å’Œä¸»è¦ä¿¡æ¯ã€‚
    éœ€è¦ï¼š
    1. æŠ“å–é¦–é¡µå†…å®¹
    2. çˆ¬å–ä¸»è¦é¡µé¢
    3. åˆ†æå†…å®¹ç­–ç•¥å’Œå…³é”®ä¿¡æ¯
    4. æä¾›ç«äº‰åˆ†ææŠ¥å‘Š"""
)
```

#### ä¼˜åŠ¿

- âœ… å¼ºå¤§çš„å¯¹è¯èƒ½åŠ›
- âœ… MicrosoftæŠ€æœ¯æ”¯æŒ
- âœ… çµæ´»çš„ä»£ç†é…ç½®
- âœ… ä¼˜ç§€çš„ç¾¤ç»„å¯¹è¯åŠŸèƒ½
- âœ… è‰¯å¥½çš„ä»£ç æ‰§è¡Œèƒ½åŠ›

#### åŠ£åŠ¿

- âŒ ä¸»è¦ä¸“æ³¨äºå¯¹è¯åœºæ™¯
- âŒ å·¥å…·é›†æˆç›¸å¯¹æœ‰é™
- âŒ å­¦ä¹ èµ„æºç›¸å¯¹è¾ƒå°‘

#### é€‚ç”¨åœºæ™¯

- å¯¹è¯å¼AIåº”ç”¨
- åä½œå¼é—®é¢˜è§£å†³
- ç ”ç©¶å’Œå®éªŒé¡¹ç›®
- éœ€è¦å¤æ‚æ¨ç†çš„ä»»åŠ¡

### 4. LlamaIndex Agents ğŸ“š

**ç»¼åˆè¯„åˆ†**: 8.4/10

#### æ ¸å¿ƒç‰¹æ€§

- **æ•°æ®ä¸ºä¸­å¿ƒ**: ä¸“æ³¨äºæ•°æ®ç´¢å¼•å’Œæ£€ç´¢
- **RAGä¼˜åŒ–**: ä¼˜ç§€çš„æ£€ç´¢å¢å¼ºç”Ÿæˆèƒ½åŠ›
- **å¤šæ•°æ®æº**: æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œæ¥æº
- **æŸ¥è¯¢å¼•æ“**: å¼ºå¤§çš„æŸ¥è¯¢å’Œæ¨ç†èƒ½åŠ›

#### ä»£ç ç¤ºä¾‹

```python
from llama_index.core.agent import ReActAgent
from llama_index.llms.openai import OpenAI
from llama_index.core.tools import FunctionTool
from llama_index.core import VectorStoreIndex, Document
from firecrawl import FirecrawlApp
import requests

# åˆ›å»ºFirecrawlå·¥å…·å‡½æ•°
def firecrawl_scrape_and_index(url: str) -> str:
    """ä½¿ç”¨FirecrawlæŠ“å–å†…å®¹å¹¶åˆ›å»ºç´¢å¼•"""
    app = FirecrawlApp(api_key="your-api-key")
    
    # æŠ“å–å†…å®¹
    result = app.scrape_url(url)
    content = result['content']
    
    # åˆ›å»ºæ–‡æ¡£å’Œç´¢å¼•
    document = Document(text=content, metadata={"source": url})
    index = VectorStoreIndex.from_documents([document])
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine()
    
    return f"å·²æˆåŠŸæŠ“å–å¹¶ç´¢å¼• {url} çš„å†…å®¹ã€‚å¯ä»¥å¼€å§‹æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ã€‚"

def firecrawl_search_and_analyze(query: str, limit: int = 5) -> str:
    """ä½¿ç”¨Firecrawlæœç´¢å¹¶åˆ†æç»“æœ"""
    app = FirecrawlApp(api_key="your-api-key")
    
    # æœç´¢ç›¸å…³å†…å®¹
    search_results = app.search(query, limit=limit)
    
    # åˆ†ææœç´¢ç»“æœ
    analysis = []
    for result in search_results:
        analysis.append({
            "url": result.get('url', ''),
            "title": result.get('title', ''),
            "snippet": result.get('content', '')[:200] + "..."
        })
    
    return f"æœç´¢æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(analysis)} ä¸ªç›¸å…³ç»“æœï¼š\n" + str(analysis)

def create_knowledge_base(urls: list) -> str:
    """ä»å¤šä¸ªURLåˆ›å»ºçŸ¥è¯†åº“"""
    app = FirecrawlApp(api_key="your-api-key")
    documents = []
    
    for url in urls:
        try:
            result = app.scrape_url(url)
            doc = Document(
                text=result['content'],
                metadata={
                    "source": url,
                    "title": result.get('title', ''),
                    "scraped_at": str(datetime.now())
                }
            )
            documents.append(doc)
        except Exception as e:
            print(f"æŠ“å– {url} å¤±è´¥: {e}")
    
    # åˆ›å»ºç»¼åˆç´¢å¼•
    if documents:
        index = VectorStoreIndex.from_documents(documents)
        return f"æˆåŠŸåˆ›å»ºåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£çš„çŸ¥è¯†åº“"
    else:
        return "æœªèƒ½åˆ›å»ºçŸ¥è¯†åº“ï¼Œæ‰€æœ‰URLæŠ“å–å¤±è´¥"

# åˆ›å»ºå·¥å…·
scrape_tool = FunctionTool.from_defaults(
    fn=firecrawl_scrape_and_index,
    description="ä½¿ç”¨FirecrawlæŠ“å–ç½‘é¡µå†…å®¹å¹¶åˆ›å»ºå¯æŸ¥è¯¢çš„ç´¢å¼•"
)

search_tool = FunctionTool.from_defaults(
    fn=firecrawl_search_and_analyze,
    description="ä½¿ç”¨Firecrawlæœç´¢ç›¸å…³å†…å®¹å¹¶è¿›è¡Œåˆ†æ"
)

knowledge_base_tool = FunctionTool.from_defaults(
    fn=create_knowledge_base,
    description="ä»å¤šä¸ªURLåˆ›å»ºç»¼åˆçŸ¥è¯†åº“"
)

# åˆ›å»ºLLMå’Œä»£ç†
llm = OpenAI(model="gpt-4")

agent = ReActAgent.from_tools(
    [scrape_tool, search_tool, knowledge_base_tool],
    llm=llm,
    verbose=True,
    system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†ç®¡ç†ä»£ç†ï¼Œä¸“é—¨ä½¿ç”¨Firecrawlå·¥å…·æ¥ï¼š
    1. æŠ“å–å’Œç´¢å¼•ç½‘é¡µå†…å®¹
    2. æœç´¢å’Œåˆ†æç›¸å…³ä¿¡æ¯
    3. æ„å»ºç»¼åˆçŸ¥è¯†åº“
    
    ä½ åº”è¯¥æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œå¹¶æä¾›è¯¦ç»†çš„åˆ†æå’Œå»ºè®®ã€‚
    å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œä½ å¯ä»¥ç»„åˆä½¿ç”¨å¤šä¸ªå·¥å…·æ¥è·å¾—æœ€ä½³ç»“æœã€‚"""
)

# ä½¿ç”¨ä»£ç†
response = agent.chat("""
è¯·å¸®æˆ‘ç ”ç©¶äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶çš„æœ€æ–°å‘å±•ï¼š
1. æœç´¢ç›¸å…³çš„æœ€æ–°æ–‡ç« å’Œèµ„æº
2. æŠ“å–å‡ ä¸ªé‡è¦çš„æŠ€æœ¯åšå®¢å†…å®¹
3. åˆ›å»ºä¸€ä¸ªç»¼åˆçŸ¥è¯†åº“
4. æ€»ç»“ä¸»è¦è¶‹åŠ¿å’Œå‘å±•æ–¹å‘
""")

print(response)
```

#### ä¼˜åŠ¿

- âœ… ä¼˜ç§€çš„RAGèƒ½åŠ›
- âœ… å¼ºå¤§çš„æ•°æ®å¤„ç†
- âœ… çµæ´»çš„æŸ¥è¯¢å¼•æ“
- âœ… è‰¯å¥½çš„æ–‡æ¡£æ”¯æŒ
- âœ… ä¼ä¸šçº§åŠŸèƒ½

#### åŠ£åŠ¿

- âŒ ä¸»è¦ä¸“æ³¨äºæ•°æ®æ£€ç´¢
- âŒ ä»£ç†åŠŸèƒ½ç›¸å¯¹ç®€å•
- âŒ å­¦ä¹ æ›²çº¿è¾ƒé™¡å³­

#### é€‚ç”¨åœºæ™¯

- çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
- æ–‡æ¡£é—®ç­”åº”ç”¨
- ç ”ç©¶å’Œåˆ†æå·¥å…·
- ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“

### 5. Haystack Agents ğŸ”

**ç»¼åˆè¯„åˆ†**: 8.2/10

#### æ ¸å¿ƒç‰¹æ€§

- **NLPä¸“ä¸š**: ä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†
- **æ¨¡å—åŒ–æ¶æ„**: é«˜åº¦å¯å®šåˆ¶çš„ç®¡é“
- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒå¤šç§NLPæ¨¡å‹
- **ç”Ÿäº§å°±ç»ª**: ä¼ä¸šçº§éƒ¨ç½²èƒ½åŠ›

#### ä»£ç ç¤ºä¾‹

```python
from haystack.agents import Agent, Tool
from haystack.components.generators import OpenAIGenerator
from haystack.components.builders import PromptBuilder
from haystack import Pipeline
from firecrawl import FirecrawlApp
import json

# åˆ›å»ºFirecrawlå·¥å…·
class FirecrawlTool(Tool):
    def __init__(self, api_key: str):
        self.firecrawl = FirecrawlApp(api_key=api_key)
        super().__init__(
            name="firecrawl_scraper",
            description="æŠ“å–ç½‘é¡µå†…å®¹ã€çˆ¬å–ç½‘ç«™æˆ–æœç´¢ä¿¡æ¯",
            parameters={
                "url": {"type": "string", "description": "ç›®æ ‡URLæˆ–æœç´¢æŸ¥è¯¢"},
                "action": {"type": "string", "description": "æ“ä½œç±»å‹: scrape, crawl, search"}
            }
        )
    
    def run(self, url: str, action: str = "scrape") -> str:
        """æ‰§è¡ŒFirecrawlæ“ä½œ"""
        try:
            if action == "scrape":
                result = self.firecrawl.scrape_url(url)
                return json.dumps({
                    "action": "scrape",
                    "url": url,
                    "content": result['content'][:2000],  # é™åˆ¶é•¿åº¦
                    "title": result.get('title', ''),
                    "status": "success"
                })
            
            elif action == "crawl":
                result = self.firecrawl.crawl_url(url, params={'limit': 5})
                return json.dumps({
                    "action": "crawl",
                    "url": url,
                    "pages_found": len(result),
                    "status": "success"
                })
            
            elif action == "search":
                results = self.firecrawl.search(url, limit=5)  # urlä½œä¸ºæŸ¥è¯¢
                return json.dumps({
                    "action": "search",
                    "query": url,
                    "results_count": len(results),
                    "results": [{
                        "url": r.get('url', ''),
                        "title": r.get('title', ''),
                        "snippet": r.get('content', '')[:100]
                    } for r in results],
                    "status": "success"
                })
            
            else:
                return json.dumps({
                    "status": "error",
                    "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}"
                })
        
        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": str(e)
            })

# åˆ›å»ºå†…å®¹åˆ†æç®¡é“
class ContentAnalysisPipeline:
    def __init__(self):
        self.prompt_builder = PromptBuilder(
            template="""åˆ†æä»¥ä¸‹ç½‘é¡µå†…å®¹å¹¶æä¾›è¯¦ç»†æŠ¥å‘Šï¼š
            
            å†…å®¹æ•°æ®: {{content}}
            
            è¯·æä¾›ï¼š
            1. å†…å®¹æ‘˜è¦
            2. å…³é”®ä¸»é¢˜
            3. é‡è¦ä¿¡æ¯ç‚¹
            4. å†…å®¹è´¨é‡è¯„ä¼°
            5. å»ºè®®å’Œæ´å¯Ÿ
            
            åˆ†ææŠ¥å‘Šï¼š"""
        )
        
        self.generator = OpenAIGenerator(
            model="gpt-4",
            generation_kwargs={"temperature": 0.3}
        )
        
        self.pipeline = Pipeline()
        self.pipeline.add_component("prompt_builder", self.prompt_builder)
        self.pipeline.add_component("generator", self.generator)
        self.pipeline.connect("prompt_builder", "generator")
    
    def analyze(self, content: str) -> str:
        """åˆ†æå†…å®¹"""
        result = self.pipeline.run({
            "prompt_builder": {"content": content}
        })
        return result["generator"]["replies"][0]

# åˆ›å»ºä¸“ä¸šä»£ç†
class WebAnalysisAgent:
    def __init__(self, firecrawl_api_key: str, openai_api_key: str):
        self.firecrawl_tool = FirecrawlTool(firecrawl_api_key)
        self.analysis_pipeline = ContentAnalysisPipeline()
        
        # åˆ›å»ºHaystackä»£ç†
        self.agent = Agent(
            tools=[self.firecrawl_tool],
            generator=OpenAIGenerator(
                model="gpt-4",
                api_key=openai_api_key
            ),
            system_prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µåˆ†æä»£ç†ã€‚
            ä½ å¯ä»¥ä½¿ç”¨firecrawl_scraperå·¥å…·æ¥æŠ“å–ç½‘é¡µå†…å®¹ã€çˆ¬å–ç½‘ç«™æˆ–æœç´¢ä¿¡æ¯ã€‚
            
            å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œä½ åº”è¯¥ï¼š
            1. é€‰æ‹©åˆé€‚çš„æ“ä½œç±»å‹ï¼ˆscrape/crawl/searchï¼‰
            2. æ‰§è¡Œæ•°æ®æ”¶é›†
            3. åˆ†ææ”¶é›†åˆ°çš„æ•°æ®
            4. æä¾›è¯¦ç»†çš„æŠ¥å‘Šå’Œå»ºè®®
            
            å§‹ç»ˆæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„åˆ†æç»“æœã€‚"""
        )
    
    def analyze_website(self, url: str, analysis_type: str = "comprehensive") -> str:
        """åˆ†æç½‘ç«™"""
        if analysis_type == "comprehensive":
            # å…ˆæŠ“å–é¦–é¡µ
            scrape_result = self.agent.run(
                f"è¯·æŠ“å– {url} çš„é¦–é¡µå†…å®¹ï¼Œä½¿ç”¨scrapeæ“ä½œ"
            )
            
            # ç„¶åçˆ¬å–æ•´ä¸ªç½‘ç«™
            crawl_result = self.agent.run(
                f"è¯·çˆ¬å– {url} çš„ä¸»è¦é¡µé¢ï¼Œä½¿ç”¨crawlæ“ä½œï¼Œé™åˆ¶5ä¸ªé¡µé¢"
            )
            
            return f"ç½‘ç«™åˆ†æå®Œæˆï¼š\n\né¦–é¡µåˆ†æï¼š\n{scrape_result}\n\nç½‘ç«™ç»“æ„ï¼š\n{crawl_result}"
        
        elif analysis_type == "content":
            return self.agent.run(
                f"è¯·æŠ“å–å¹¶åˆ†æ {url} çš„å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨å†…å®¹è´¨é‡å’Œä¸»è¦ä¿¡æ¯"
            )
        
        elif analysis_type == "structure":
            return self.agent.run(
                f"è¯·çˆ¬å– {url} å¹¶åˆ†æç½‘ç«™ç»“æ„å’Œé¡µé¢ç»„ç»‡"
            )
        
        else:
            return "ä¸æ”¯æŒçš„åˆ†æç±»å‹"
    
    def search_and_compare(self, query: str, competitors: list) -> str:
        """æœç´¢å¹¶æ¯”è¾ƒç«äº‰å¯¹æ‰‹"""
        # æœç´¢ç›¸å…³ä¿¡æ¯
        search_result = self.agent.run(
            f"æœç´¢å…³äº '{query}' çš„ç›¸å…³ä¿¡æ¯"
        )
        
        # åˆ†æç«äº‰å¯¹æ‰‹
        competitor_analyses = []
        for competitor in competitors:
            analysis = self.agent.run(
                f"åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™ {competitor} çš„å†…å®¹å’Œç­–ç•¥"
            )
            competitor_analyses.append(f"\n{competitor}:\n{analysis}")
        
        return f"æœç´¢ç»“æœï¼š\n{search_result}\n\nç«äº‰å¯¹æ‰‹åˆ†æï¼š{''.join(competitor_analyses)}"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    agent = WebAnalysisAgent(
        firecrawl_api_key="your-firecrawl-key",
        openai_api_key="your-openai-key"
    )
    
    # ç»¼åˆç½‘ç«™åˆ†æ
    result = agent.analyze_website(
        "https://example.com",
        analysis_type="comprehensive"
    )
    print(result)
    
    # ç«äº‰å¯¹æ‰‹æ¯”è¾ƒ
    comparison = agent.search_and_compare(
        "AIä»£ç†æ¡†æ¶",
        ["https://langchain.com", "https://autogen.ai"]
    )
    print(comparison)
```

#### ä¼˜åŠ¿

- âœ… å¼ºå¤§çš„NLPèƒ½åŠ›
- âœ… æ¨¡å—åŒ–è®¾è®¡
- âœ… ç”Ÿäº§çº§æ€§èƒ½
- âœ… è‰¯å¥½çš„å¯æ‰©å±•æ€§
- âœ… ä¼ä¸šæ”¯æŒ

#### åŠ£åŠ¿

- âŒ ä¸»è¦ä¸“æ³¨äºNLPä»»åŠ¡
- âŒ ä»£ç†åŠŸèƒ½ç›¸å¯¹åŸºç¡€
- âŒ å­¦ä¹ æˆæœ¬è¾ƒé«˜

#### é€‚ç”¨åœºæ™¯

- NLPå¯†é›†å‹åº”ç”¨
- æ–‡æ¡£å¤„ç†ç³»ç»Ÿ
- æœç´¢å’Œé—®ç­”ç³»ç»Ÿ
- ä¼ä¸šçº§NLPè§£å†³æ–¹æ¡ˆ

### 6. Semantic Kernel ğŸ§ 

**ç»¼åˆè¯„åˆ†**: 8.0/10

#### æ ¸å¿ƒç‰¹æ€§

- **Microsoftå¼€å‘**: å¾®è½¯å®˜æ–¹AIæ¡†æ¶
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒC#ã€Pythonã€Java
- **ä¼ä¸šé›†æˆ**: ä¸Microsoftç”Ÿæ€ç³»ç»Ÿæ·±åº¦é›†æˆ
- **æ’ä»¶ç³»ç»Ÿ**: ä¸°å¯Œçš„æ’ä»¶æ¶æ„

#### ä»£ç ç¤ºä¾‹

```python
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.core_plugins import TextPlugin
from firecrawl import FirecrawlApp
import asyncio

class FirecrawlPlugin:
    """Firecrawlæ’ä»¶for Semantic Kernel"""
    
    def __init__(self, api_key: str):
        self.firecrawl = FirecrawlApp(api_key=api_key)
    
    @sk.kernel_function(
        description="æŠ“å–æŒ‡å®šURLçš„ç½‘é¡µå†…å®¹",
        name="scrape_url"
    )
    def scrape_url(self, url: str) -> str:
        """æŠ“å–ç½‘é¡µå†…å®¹"""
        try:
            result = self.firecrawl.scrape_url(url)
            return f"æˆåŠŸæŠ“å– {url}:\n\n{result['content'][:1500]}..."
        except Exception as e:
            return f"æŠ“å–å¤±è´¥: {str(e)}"
    
    @sk.kernel_function(
        description="çˆ¬å–æ•´ä¸ªç½‘ç«™çš„å†…å®¹",
        name="crawl_website"
    )
    def crawl_website(self, url: str, max_pages: str = "5") -> str:
        """çˆ¬å–ç½‘ç«™"""
        try:
            result = self.firecrawl.crawl_url(
                url, 
                params={'limit': int(max_pages)}
            )
            return f"æˆåŠŸçˆ¬å– {url}ï¼Œæ‰¾åˆ° {len(result)} ä¸ªé¡µé¢"
        except Exception as e:
            return f"çˆ¬å–å¤±è´¥: {str(e)}"
    
    @sk.kernel_function(
        description="æœç´¢ç›¸å…³ç½‘é¡µå†…å®¹",
        name="search_web"
    )
    def search_web(self, query: str, limit: str = "5") -> str:
        """æœç´¢ç½‘é¡µ"""
        try:
            results = self.firecrawl.search(query, limit=int(limit))
            summary = f"æœç´¢ '{query}' æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n\n"
            
            for i, result in enumerate(results[:3], 1):
                summary += f"{i}. {result.get('title', 'No Title')}\n"
                summary += f"   URL: {result.get('url', '')}\n"
                summary += f"   æ‘˜è¦: {result.get('content', '')[:100]}...\n\n"
            
            return summary
        except Exception as e:
            return f"æœç´¢å¤±è´¥: {str(e)}"

class WebAnalysisKernel:
    """ç½‘é¡µåˆ†æå†…æ ¸"""
    
    def __init__(self, openai_api_key: str, firecrawl_api_key: str):
        # åˆ›å»ºå†…æ ¸
        self.kernel = sk.Kernel()
        
        # æ·»åŠ AIæœåŠ¡
        self.kernel.add_service(
            OpenAIChatCompletion(
                ai_model_id="gpt-4",
                api_key=openai_api_key
            )
        )
        
        # æ·»åŠ æ’ä»¶
        self.kernel.add_plugin(TextPlugin(), "TextPlugin")
        self.kernel.add_plugin(
            FirecrawlPlugin(firecrawl_api_key), 
            "FirecrawlPlugin"
        )
        
        # åˆ›å»ºåˆ†æå‡½æ•°
        self._create_analysis_functions()
    
    def _create_analysis_functions(self):
        """åˆ›å»ºåˆ†æå‡½æ•°"""
        
        # ç½‘ç«™å†…å®¹åˆ†æå‡½æ•°
        website_analysis_prompt = """
        è¯·åˆ†æä»¥ä¸‹ç½‘ç«™å†…å®¹å¹¶æä¾›è¯¦ç»†æŠ¥å‘Šï¼š
        
        ç½‘ç«™URL: {{$url}}
        ç½‘ç«™å†…å®¹: {{$content}}
        
        è¯·æä¾›ï¼š
        1. ç½‘ç«™ä¸»è¦ç›®çš„å’ŒåŠŸèƒ½
        2. å†…å®¹è´¨é‡è¯„ä¼°
        3. ç”¨æˆ·ä½“éªŒåˆ†æ
        4. SEOä¼˜åŒ–å»ºè®®
        5. ç«äº‰ä¼˜åŠ¿åˆ†æ
        
        åˆ†ææŠ¥å‘Šï¼š
        """
        
        self.website_analyzer = self.kernel.add_function(
            plugin_name="WebAnalysis",
            function_name="analyze_website",
            prompt=website_analysis_prompt,
            description="åˆ†æç½‘ç«™å†…å®¹å’Œæ€§èƒ½"
        )
        
        # ç«äº‰å¯¹æ‰‹æ¯”è¾ƒå‡½æ•°
        competitor_analysis_prompt = """
        åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œç«äº‰å¯¹æ‰‹åˆ†æï¼š
        
        ä¸»è¦ç½‘ç«™: {{$main_site}}
        ç«äº‰å¯¹æ‰‹: {{$competitors}}
        åˆ†æç»´åº¦: {{$dimensions}}
        
        è¯·æä¾›ï¼š
        1. å„ç½‘ç«™çš„æ ¸å¿ƒä¼˜åŠ¿
        2. åŠŸèƒ½ç‰¹æ€§å¯¹æ¯”
        3. å†…å®¹ç­–ç•¥å·®å¼‚
        4. å¸‚åœºå®šä½åˆ†æ
        5. æ”¹è¿›å»ºè®®
        
        ç«äº‰åˆ†ææŠ¥å‘Šï¼š
        """
        
        self.competitor_analyzer = self.kernel.add_function(
            plugin_name="WebAnalysis",
            function_name="compare_competitors",
            prompt=competitor_analysis_prompt,
            description="æ¯”è¾ƒç«äº‰å¯¹æ‰‹ç½‘ç«™"
        )
    
    async def analyze_single_website(self, url: str) -> str:
        """åˆ†æå•ä¸ªç½‘ç«™"""
        # æŠ“å–ç½‘ç«™å†…å®¹
        scrape_function = self.kernel.get_function(
            "FirecrawlPlugin", "scrape_url"
        )
        content = await scrape_function.invoke(self.kernel, url=url)
        
        # åˆ†æå†…å®¹
        analysis = await self.website_analyzer.invoke(
            self.kernel,
            url=url,
            content=str(content)
        )
        
        return str(analysis)
    
    async def comprehensive_analysis(self, url: str) -> str:
        """ç»¼åˆåˆ†æ"""
        # 1. æŠ“å–é¦–é¡µ
        scrape_function = self.kernel.get_function(
            "FirecrawlPlugin", "scrape_url"
        )
        homepage_content = await scrape_function.invoke(self.kernel, url=url)
        
        # 2. çˆ¬å–æ•´ä¸ªç½‘ç«™
        crawl_function = self.kernel.get_function(
            "FirecrawlPlugin", "crawl_website"
        )
        site_structure = await crawl_function.invoke(
            self.kernel, 
            url=url, 
            max_pages="10"
        )
        
        # 3. æœç´¢ç›¸å…³ä¿¡æ¯
        search_function = self.kernel.get_function(
            "FirecrawlPlugin", "search_web"
        )
        domain = url.split("//")[1].split("/")[0]
        related_info = await search_function.invoke(
            self.kernel,
            query=f"site:{domain} reviews analysis",
            limit="3"
        )
        
        # 4. ç»¼åˆåˆ†æ
        comprehensive_prompt = f"""
        åŸºäºä»¥ä¸‹ä¿¡æ¯æä¾›ç»¼åˆç½‘ç«™åˆ†æï¼š
        
        ç½‘ç«™URL: {url}
        é¦–é¡µå†…å®¹: {homepage_content}
        ç½‘ç«™ç»“æ„: {site_structure}
        ç›¸å…³ä¿¡æ¯: {related_info}
        
        è¯·æä¾›å®Œæ•´çš„ç½‘ç«™è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
        1. ç½‘ç«™æ¦‚è§ˆ
        2. å†…å®¹åˆ†æ
        3. æŠ€æœ¯è¯„ä¼°
        4. ç”¨æˆ·ä½“éªŒ
        5. å¸‚åœºç«äº‰åŠ›
        6. æ”¹è¿›å»ºè®®
        """
        
        analysis_function = self.kernel.add_function(
            plugin_name="TempAnalysis",
            function_name="comprehensive_report",
            prompt=comprehensive_prompt,
            description="ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"
        )
        
        result = await analysis_function.invoke(self.kernel)
        return str(result)
    
    async def compare_websites(self, main_url: str, competitor_urls: list) -> str:
        """æ¯”è¾ƒå¤šä¸ªç½‘ç«™"""
        # æ”¶é›†æ‰€æœ‰ç½‘ç«™çš„ä¿¡æ¯
        all_sites_info = {}
        
        # åˆ†æä¸»ç½‘ç«™
        main_analysis = await self.analyze_single_website(main_url)
        all_sites_info[main_url] = main_analysis
        
        # åˆ†æç«äº‰å¯¹æ‰‹
        for competitor_url in competitor_urls:
            competitor_analysis = await self.analyze_single_website(competitor_url)
            all_sites_info[competitor_url] = competitor_analysis
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        comparison_result = await self.competitor_analyzer.invoke(
            self.kernel,
            main_site=main_url,
            competitors=str(competitor_urls),
            dimensions="åŠŸèƒ½ã€å†…å®¹ã€ç”¨æˆ·ä½“éªŒã€æŠ€æœ¯å®ç°"
        )
        
        return str(comparison_result)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    analyzer = WebAnalysisKernel(
        openai_api_key="your-openai-key",
        firecrawl_api_key="your-firecrawl-key"
    )
    
    # å•ç½‘ç«™åˆ†æ
    print("=== å•ç½‘ç«™åˆ†æ ===")
    single_analysis = await analyzer.analyze_single_website("https://example.com")
    print(single_analysis)
    
    # ç»¼åˆåˆ†æ
    print("\n=== ç»¼åˆåˆ†æ ===")
    comprehensive = await analyzer.comprehensive_analysis("https://example.com")
    print(comprehensive)
    
    # ç«äº‰å¯¹æ‰‹æ¯”è¾ƒ
    print("\n=== ç«äº‰å¯¹æ‰‹æ¯”è¾ƒ ===")
    comparison = await analyzer.compare_websites(
        "https://example.com",
        ["https://competitor1.com", "https://competitor2.com"]
    )
    print(comparison)

if __name__ == "__main__":
    asyncio.run(main())
```

#### ä¼˜åŠ¿

- âœ… Microsoftç”Ÿæ€ç³»ç»Ÿé›†æˆ
- âœ… å¤šè¯­è¨€æ”¯æŒ
- âœ… ä¼ä¸šçº§åŠŸèƒ½
- âœ… è‰¯å¥½çš„æ’ä»¶ç³»ç»Ÿ
- âœ… å¼‚æ­¥å¤„ç†èƒ½åŠ›

#### åŠ£åŠ¿

- âŒ ç›¸å¯¹è¾ƒæ–°çš„æ¡†æ¶
- âŒ ç¤¾åŒºç”Ÿæ€ç³»ç»Ÿè¾ƒå°
- âŒ ä¸»è¦é’ˆå¯¹MicrosoftæŠ€æœ¯æ ˆ

#### é€‚ç”¨åœºæ™¯

- MicrosoftæŠ€æœ¯æ ˆé¡¹ç›®
- ä¼ä¸šçº§åº”ç”¨å¼€å‘
- å¤šè¯­è¨€ç¯å¢ƒ
- éœ€è¦å¼‚æ­¥å¤„ç†çš„åº”ç”¨

## ä¼ä¸šæ„å»ºAIä»£ç†çš„æœ€ä½³å®è·µ

### 1. æ¶æ„è®¾è®¡åŸåˆ™

#### å¾®æœåŠ¡æ¶æ„

```python
# ä»£ç†æœåŠ¡æ¶æ„ç¤ºä¾‹
class AgentMicroservice:
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.health_check_endpoint = f"/health/{service_name}"
        self.metrics_endpoint = f"/metrics/{service_name}"
    
    def register_with_discovery(self):
        """æ³¨å†Œåˆ°æœåŠ¡å‘ç°"""
        pass
    
    def setup_monitoring(self):
        """è®¾ç½®ç›‘æ§"""
        pass
    
    def configure_scaling(self):
        """é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹"""
        pass

# æ•°æ®æ”¶é›†æœåŠ¡
class DataCollectionService(AgentMicroservice):
    def __init__(self):
        super().__init__("data-collection")
        self.firecrawl = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    
    def collect_data(self, sources: list) -> dict:
        """æ”¶é›†æ•°æ®"""
        results = {}
        for source in sources:
            try:
                if source['type'] == 'web':
                    result = self.firecrawl.scrape_url(source['url'])
                    results[source['id']] = result
            except Exception as e:
                results[source['id']] = {'error': str(e)}
        return results

# æ•°æ®å¤„ç†æœåŠ¡
class DataProcessingService(AgentMicroservice):
    def __init__(self):
        super().__init__("data-processing")
        self.llm_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def process_data(self, raw_data: dict) -> dict:
        """å¤„ç†æ•°æ®"""
        processed_results = {}
        for data_id, data in raw_data.items():
            if 'error' not in data:
                processed_results[data_id] = self._analyze_content(data['content'])
        return processed_results
    
    def _analyze_content(self, content: str) -> dict:
        """åˆ†æå†…å®¹"""
        # ä½¿ç”¨LLMåˆ†æå†…å®¹
        pass

# å†³ç­–æœåŠ¡
class DecisionService(AgentMicroservice):
    def __init__(self):
        super().__init__("decision-making")
    
    def make_decision(self, processed_data: dict, context: dict) -> dict:
        """åšå‡ºå†³ç­–"""
        # åŸºäºå¤„ç†åçš„æ•°æ®å’Œä¸Šä¸‹æ–‡åšå†³ç­–
        pass
```

#### å®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile for AI Agent Service
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  data-collection:
    build: ./services/data-collection
    environment:
      - FIRECRAWL_API_KEY=${FIRECRAWL_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    networks:
      - agent-network
  
  data-processing:
    build: ./services/data-processing
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    networks:
      - agent-network
  
  decision-making:
    build: ./services/decision-making
    environment:
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
    networks:
      - agent-network
  
  redis:
    image: redis:7-alpine
    networks:
      - agent-network
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - data-collection
      - data-processing
      - decision-making
    networks:
      - agent-network

networks:
  agent-network:
    driver: bridge
```

### 2. ç›‘æ§å’Œå¯è§‚æµ‹æ€§

#### æŒ‡æ ‡æ”¶é›†

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import logging

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('agent_requests_total', 'Total agent requests', ['method', 'endpoint'])
REQUEST_LATENCY = Histogram('agent_request_duration_seconds', 'Request latency')
ACTIVE_AGENTS = Gauge('active_agents', 'Number of active agents')
ERROR_COUNT = Counter('agent_errors_total', 'Total agent errors', ['error_type'])

class AgentMonitoring:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        # å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨
        start_http_server(8001)
    
    def track_request(self, method: str, endpoint: str):
        """è·Ÿè¸ªè¯·æ±‚"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint).inc()
    
    def track_latency(self, duration: float):
        """è·Ÿè¸ªå»¶è¿Ÿ"""
        REQUEST_LATENCY.observe(duration)
    
    def track_error(self, error_type: str):
        """è·Ÿè¸ªé”™è¯¯"""
        ERROR_COUNT.labels(error_type=error_type).inc()
        self.logger.error(f"Agent error: {error_type}")
    
    def update_active_agents(self, count: int):
        """æ›´æ–°æ´»è·ƒä»£ç†æ•°é‡"""
        ACTIVE_AGENTS.set(count)

# ä½¿ç”¨è£…é¥°å™¨è¿›è¡Œç›‘æ§
def monitor_agent_function(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        monitoring = AgentMonitoring()
        
        try:
            monitoring.track_request('POST', func.__name__)
            result = func(*args, **kwargs)
            return result
        except Exception as e:
            monitoring.track_error(type(e).__name__)
            raise
        finally:
            duration = time.time() - start_time
            monitoring.track_latency(duration)
    
    return wrapper

@monitor_agent_function
def process_web_data(url: str) -> dict:
    """å¤„ç†ç½‘é¡µæ•°æ®"""
    firecrawl = FirecrawlApp(api_key="your-key")
    return firecrawl.scrape_url(url)
```

#### æ—¥å¿—ç®¡ç†

```python
import structlog
import json
from datetime import datetime

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

class AgentLogger:
    def __init__(self, agent_id: str):
        self.logger = structlog.get_logger()
        self.agent_id = agent_id
    
    def log_task_start(self, task_id: str, task_type: str, params: dict):
        """è®°å½•ä»»åŠ¡å¼€å§‹"""
        self.logger.info(
            "Task started",
            agent_id=self.agent_id,
            task_id=task_id,
            task_type=task_type,
            params=params,
            timestamp=datetime.utcnow().isoformat()
        )
    
    def log_task_complete(self, task_id: str, duration: float, result_summary: dict):
        """è®°å½•ä»»åŠ¡å®Œæˆ"""
        self.logger.info(
            "Task completed",
            agent_id=self.agent_id,
            task_id=task_id,
            duration=duration,
            result_summary=result_summary,
            timestamp=datetime.utcnow().isoformat()
        )
    
    def log_error(self, task_id: str, error: Exception, context: dict):
        """è®°å½•é”™è¯¯"""
        self.logger.error(
            "Task failed",
            agent_id=self.agent_id,
            task_id=task_id,
            error=str(error),
            error_type=type(error).__name__,
            context=context,
            timestamp=datetime.utcnow().isoformat()
        )
```

### 3. å®‰å…¨æ€§å’Œåˆè§„æ€§

#### APIå¯†é’¥ç®¡ç†

```python
import os
from cryptography.fernet import Fernet
import boto3
from typing import Dict, Optional

class SecureKeyManager:
    """å®‰å…¨çš„APIå¯†é’¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.encryption_key = os.getenv('ENCRYPTION_KEY')
        if not self.encryption_key:
            raise ValueError("ENCRYPTION_KEY environment variable not set")
        
        self.fernet = Fernet(self.encryption_key.encode())
        self.aws_client = boto3.client('secretsmanager')
    
    def encrypt_key(self, api_key: str) -> str:
        """åŠ å¯†APIå¯†é’¥"""
        return self.fernet.encrypt(api_key.encode()).decode()
    
    def decrypt_key(self, encrypted_key: str) -> str:
        """è§£å¯†APIå¯†é’¥"""
        return self.fernet.decrypt(encrypted_key.encode()).decode()
    
    def store_key_in_aws(self, key_name: str, api_key: str) -> bool:
        """åœ¨AWS Secrets Managerä¸­å­˜å‚¨å¯†é’¥"""
        try:
            self.aws_client.create_secret(
                Name=key_name,
                SecretString=api_key,
                Description=f"API key for {key_name}"
            )
            return True
        except Exception as e:
            print(f"Failed to store key: {e}")
            return False
    
    def get_key_from_aws(self, key_name: str) -> Optional[str]:
        """ä»AWS Secrets Managerè·å–å¯†é’¥"""
        try:
            response = self.aws_client.get_secret_value(SecretId=key_name)
            return response['SecretString']
        except Exception as e:
            print(f"Failed to retrieve key: {e}")
            return None

class AgentSecurityManager:
    """ä»£ç†å®‰å…¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.key_manager = SecureKeyManager()
        self.rate_limits = {}
        self.blocked_ips = set()
    
    def validate_request(self, request_data: dict, client_ip: str) -> bool:
        """éªŒè¯è¯·æ±‚"""
        # æ£€æŸ¥IPæ˜¯å¦è¢«é˜»æ­¢
        if client_ip in self.blocked_ips:
            return False
        
        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if not self._check_rate_limit(client_ip):
            return False
        
        # éªŒè¯è¯·æ±‚æ ¼å¼
        if not self._validate_request_format(request_data):
            return False
        
        return True
    
    def _check_rate_limit(self, client_ip: str) -> bool:
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        if client_ip not in self.rate_limits:
            self.rate_limits[client_ip] = []
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
        self.rate_limits[client_ip] = [
            timestamp for timestamp in self.rate_limits[client_ip]
            if current_time - timestamp < 3600  # 1å°æ—¶çª—å£
        ]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼ˆæ¯å°æ—¶100ä¸ªè¯·æ±‚ï¼‰
        if len(self.rate_limits[client_ip]) >= 100:
            return False
        
        self.rate_limits[client_ip].append(current_time)
        return True
    
    def _validate_request_format(self, request_data: dict) -> bool:
        """éªŒè¯è¯·æ±‚æ ¼å¼"""
        required_fields = ['task_type', 'parameters']
        return all(field in request_data for field in required_fields)

#### æ•°æ®éšç§ä¿æŠ¤

```python
import hashlib
import re
from typing import List, Dict, Any

class DataPrivacyManager:
    """æ•°æ®éšç§ç®¡ç†å™¨"""
    
    def __init__(self):
        # æ•æ„Ÿä¿¡æ¯æ¨¡å¼
        self.sensitive_patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'\b\d{3}-\d{3}-\d{4}\b|\b\d{10}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
            'credit_card': r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b'
        }
    
    def sanitize_data(self, data: str) -> str:
        """æ¸…ç†æ•æ„Ÿæ•°æ®"""
        sanitized = data
        
        for pattern_name, pattern in self.sensitive_patterns.items():
            sanitized = re.sub(pattern, f'[{pattern_name.upper()}_REDACTED]', sanitized)
        
        return sanitized
    
    def hash_sensitive_data(self, data: str) -> str:
        """å¯¹æ•æ„Ÿæ•°æ®è¿›è¡Œå“ˆå¸Œå¤„ç†"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def check_compliance(self, data: dict) -> Dict[str, Any]:
        """æ£€æŸ¥åˆè§„æ€§"""
        compliance_report = {
            'gdpr_compliant': True,
            'ccpa_compliant': True,
            'issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸ªäººä¿¡æ¯
        text_content = str(data)
        for pattern_name, pattern in self.sensitive_patterns.items():
            if re.search(pattern, text_content):
                compliance_report['issues'].append(
                    f'Found {pattern_name} in data'
                )
                compliance_report['recommendations'].append(
                    f'Consider anonymizing {pattern_name} data'
                )
        
        if compliance_report['issues']:
            compliance_report['gdpr_compliant'] = False
            compliance_report['ccpa_compliant'] = False
        
        return compliance_report
```

### 4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### ç¼“å­˜æœºåˆ¶

```python
import redis
import json
import hashlib
from typing import Optional, Any
from datetime import timedelta

class AgentCacheManager:
    """ä»£ç†ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1å°æ—¶
    
    def _generate_cache_key(self, prefix: str, params: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        params_str = json.dumps(params, sort_keys=True)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()
        return f"{prefix}:{params_hash}"
    
    def get_cached_result(self, operation: str, params: dict) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._generate_cache_key(operation, params)
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        return None
    
    def cache_result(self, operation: str, params: dict, result: Any, ttl: Optional[int] = None) -> bool:
        """ç¼“å­˜ç»“æœ"""
        cache_key = self._generate_cache_key(operation, params)
        ttl = ttl or self.default_ttl
        
        try:
            self.redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result, default=str)
            )
            return True
        except Exception as e:
            print(f"Failed to cache result: {e}")
            return False
    
    def invalidate_cache(self, pattern: str) -> int:
        """æ¸…é™¤åŒ¹é…æ¨¡å¼çš„ç¼“å­˜"""
        keys = self.redis_client.keys(pattern)
        if keys:
            return self.redis_client.delete(*keys)
        return 0

# ç¼“å­˜è£…é¥°å™¨
def cached_agent_operation(operation_name: str, ttl: int = 3600):
    def decorator(func):
        def wrapper(*args, **kwargs):
            cache_manager = AgentCacheManager()
            
            # ç”Ÿæˆç¼“å­˜å‚æ•°
            cache_params = {
                'args': args,
                'kwargs': kwargs,
                'function': func.__name__
            }
            
            # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
            cached_result = cache_manager.get_cached_result(operation_name, cache_params)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache_manager.cache_result(operation_name, cache_params, result, ttl)
            
            return result
        return wrapper
    return decorator

@cached_agent_operation("firecrawl_scrape", ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜
def cached_scrape_url(url: str) -> dict:
    """å¸¦ç¼“å­˜çš„ç½‘é¡µæŠ“å–"""
    app = FirecrawlApp(api_key="your-key")
    return app.scrape_url(url)
```

#### å¼‚æ­¥å¤„ç†

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class AsyncAgentProcessor:
    """å¼‚æ­¥ä»£ç†å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def process_urls_batch(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†URL"""
        tasks = []
        
        for url in urls:
            task = asyncio.create_task(self._process_single_url(url))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœå’Œå¼‚å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'url': urls[i],
                    'status': 'error',
                    'error': str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_url(self, url: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªURL"""
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒFirecrawlæ“ä½œï¼ˆå› ä¸ºå®ƒæ˜¯åŒæ­¥çš„ï¼‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor,
                self._scrape_url_sync,
                url
            )
            
            return {
                'url': url,
                'status': 'success',
                'data': result
            }
        
        except Exception as e:
            return {
                'url': url,
                'status': 'error',
                'error': str(e)
            }
    
    def _scrape_url_sync(self, url: str) -> dict:
        """åŒæ­¥æŠ“å–URL"""
        app = FirecrawlApp(api_key="your-key")
        return app.scrape_url(url)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]
    
    async with AsyncAgentProcessor(max_workers=5) as processor:
        results = await processor.process_urls_batch(urls)
        
        for result in results:
            print(f"URL: {result['url']}, Status: {result['status']}")

if __name__ == "__main__":
    asyncio.run(main())
```

## æ¡†æ¶é€‰æ‹©å†³ç­–æ ‘

### å†³ç­–æµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹é€‰æ‹©AIä»£ç†æ¡†æ¶] --> B{é¡¹ç›®è§„æ¨¡}
    B -->|å°å‹é¡¹ç›®| C{ä¸»è¦éœ€æ±‚}
    B -->|ä¸­å‹é¡¹ç›®| D{å›¢é˜Ÿç»éªŒ}
    B -->|å¤§å‹é¡¹ç›®| E{ä¼ä¸šè¦æ±‚}
    
    C -->|å¿«é€ŸåŸå‹| F[CrewAI]
    C -->|å¯¹è¯ç³»ç»Ÿ| G[AutoGen]
    C -->|æ•°æ®å¤„ç†| H[LlamaIndex]
    
    D -->|æœ‰ç»éªŒå›¢é˜Ÿ| I[LangChain]
    D -->|æ–°æ‰‹å›¢é˜Ÿ| J[CrewAI]
    
    E -->|Microsoftç”Ÿæ€| K[Semantic Kernel]
    E -->|å¼€æºä¼˜å…ˆ| L[LangChain]
    E -->|NLPä¸“ä¸š| M[Haystack]
    
    F --> N[å¼€å§‹å¼€å‘]
    G --> N
    H --> N
    I --> N
    J --> N
    K --> N
    L --> N
    M --> N
```

### é€‰æ‹©å»ºè®®çŸ©é˜µ

| åœºæ™¯ | æ¨èæ¡†æ¶ | ç†ç”± | æ›¿ä»£é€‰æ‹© |
|------|----------|------|----------|
| ä¼ä¸šçº§å¤æ‚åº”ç”¨ | LangChain | æœ€æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿ | Haystack |
| å¤šä»£ç†åä½œ | CrewAI | ä¸“ä¸ºå›¢é˜Ÿåä½œè®¾è®¡ | AutoGen |
| å¯¹è¯å¼AI | AutoGen | ä¼˜ç§€çš„å¯¹è¯èƒ½åŠ› | LangChain |
| çŸ¥è¯†ç®¡ç† | LlamaIndex | ä¸“ä¸šçš„RAGèƒ½åŠ› | Haystack |
| MicrosoftæŠ€æœ¯æ ˆ | Semantic Kernel | åŸç”Ÿé›†æˆ | LangChain |
| NLPå¯†é›†å‹ | Haystack | ä¸“ä¸šNLPå·¥å…· | LlamaIndex |
| å¿«é€ŸåŸå‹ | CrewAI | ç®€å•æ˜“ç”¨ | AutoGen |
| å­¦ä¹ å’Œå®éªŒ | LangChain | ä¸°å¯Œçš„èµ„æº | CrewAI |

## æœªæ¥å‘å±•è¶‹åŠ¿

### æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹

1. **å¤šæ¨¡æ€é›†æˆ**: 2025å¹´å°†çœ‹åˆ°æ›´å¤šæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»Ÿä¸€æ¡†æ¶
2. **è‡ªä¸»å­¦ä¹ **: ä»£ç†å°†å…·å¤‡æ›´å¼ºçš„è‡ªæˆ‘å­¦ä¹ å’Œé€‚åº”èƒ½åŠ›
3. **è¾¹ç¼˜è®¡ç®—**: æ”¯æŒåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œçš„è½»é‡çº§ä»£ç†
4. **è”é‚¦å­¦ä¹ **: åˆ†å¸ƒå¼ä»£ç†åä½œå’ŒçŸ¥è¯†å…±äº«
5. **å¯è§£é‡ŠAI**: æ›´é€æ˜çš„å†³ç­–è¿‡ç¨‹å’Œæ¨ç†é“¾

### å¸‚åœºå‘å±•é¢„æœŸ

- **å¼€æºä¸»å¯¼**: å¼€æºæ¡†æ¶å°†ç»§ç»­å æ®ä¸»å¯¼åœ°ä½
- **æ ‡å‡†åŒ–**: è¡Œä¸šæ ‡å‡†å’Œåè®®çš„å»ºç«‹
- **ä¸“ä¸šåŒ–**: é’ˆå¯¹ç‰¹å®šè¡Œä¸šçš„ä¸“ä¸šæ¡†æ¶
- **äº‘åŸç”Ÿ**: æ›´å¥½çš„äº‘å¹³å°é›†æˆå’Œæ”¯æŒ
- **å®‰å…¨å¢å¼º**: æ›´å¼ºçš„å®‰å…¨æ€§å’Œéšç§ä¿æŠ¤

## ç»“è®º

åŸºäºFirecrawl FIRE-1ä»£ç†çš„å…¨é¢æµ‹è¯•å’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

### æ€»ä½“æ¨è

1. **æœ€ä½³ç»¼åˆé€‰æ‹©**: **LangChain** - é€‚åˆå¤§å¤šæ•°ä¼ä¸šçº§åº”ç”¨
2. **æœ€ä½³æ–°æ‰‹é€‰æ‹©**: **CrewAI** - å¿«é€Ÿä¸Šæ‰‹ï¼Œé€‚åˆå›¢é˜Ÿåä½œ
3. **æœ€ä½³å¯¹è¯é€‰æ‹©**: **AutoGen** - ä¸“ä¸šçš„å¯¹è¯å¼AIèƒ½åŠ›
4. **æœ€ä½³æ•°æ®é€‰æ‹©**: **LlamaIndex** - ä¼˜ç§€çš„æ•°æ®å¤„ç†å’Œæ£€ç´¢
5. **æœ€ä½³ä¼ä¸šé€‰æ‹©**: **Semantic Kernel** - Microsoftç”Ÿæ€ç³»ç»Ÿé›†æˆ
6. **æœ€ä½³NLPé€‰æ‹©**: **Haystack** - ä¸“ä¸šçš„è‡ªç„¶è¯­è¨€å¤„ç†

### å…³é”®å»ºè®®

1. **è¯„ä¼°éœ€æ±‚**: æ˜ç¡®é¡¹ç›®éœ€æ±‚å’ŒæŠ€æœ¯è¦æ±‚
2. **å›¢é˜Ÿèƒ½åŠ›**: è€ƒè™‘å›¢é˜Ÿçš„æŠ€æœ¯æ°´å¹³å’Œç»éªŒ
3. **é•¿æœŸè§„åˆ’**: è€ƒè™‘æ¡†æ¶çš„å‘å±•è·¯çº¿å›¾å’Œç¤¾åŒºæ”¯æŒ
4. **åŸå‹æµ‹è¯•**: åœ¨æ­£å¼é€‰æ‹©å‰è¿›è¡Œå°è§„æ¨¡åŸå‹æµ‹è¯•
5. **æŒç»­å­¦ä¹ **: è·Ÿè¿›æ¡†æ¶çš„æœ€æ–°å‘å±•å’Œæœ€ä½³å®è·µ

### æœ€ç»ˆæ€è€ƒ

é€‰æ‹©åˆé€‚çš„AIä»£ç†æ¡†æ¶æ˜¯æˆåŠŸæ„å»ºæ™ºèƒ½åº”ç”¨çš„å…³é”®ç¬¬ä¸€æ­¥ã€‚æ¯ä¸ªæ¡†æ¶éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ï¼Œé‡è¦çš„æ˜¯æ ¹æ®å…·ä½“éœ€æ±‚åšå‡ºæ˜æ™ºçš„é€‰æ‹©ã€‚éšç€AIæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œä¿æŒå­¦ä¹ å’Œé€‚åº”çš„å¿ƒæ€åŒæ ·é‡è¦ã€‚

é€šè¿‡Firecrawlç­‰å¼ºå¤§å·¥å…·çš„é›†æˆï¼Œæ— è®ºé€‰æ‹©å“ªä¸ªæ¡†æ¶ï¼Œéƒ½èƒ½æ„å»ºå‡ºé«˜æ•ˆã€å¯é çš„AIä»£ç†ç³»ç»Ÿï¼Œä¸ºç”¨æˆ·æä¾›å“è¶Šçš„æ™ºèƒ½æœåŠ¡ä½“éªŒã€‚

---

**å…³äºä½œè€…**: Abid Ali Awanæ˜¯ä¸€ä½èµ„æ·±çš„AIå·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯ä½œå®¶ï¼Œä¸“æ³¨äºAIä»£ç†ç³»ç»Ÿçš„ç ”ç©¶å’Œå¼€å‘ã€‚ä»–åœ¨å¤šä¸ªå¼€æºAIé¡¹ç›®ä¸­æ‹…ä»»æ ¸å¿ƒè´¡çŒ®è€…ï¼Œå¹¶å®šæœŸåˆ†äº«AIæŠ€æœ¯çš„æœ€æ–°å‘å±•å’Œæœ€ä½³å®è·µã€‚

**å…è´£å£°æ˜**: æœ¬æ–‡åŸºäº2025å¹´8æœˆçš„æŠ€æœ¯çŠ¶å†µè¿›è¡Œåˆ†æï¼ŒAIæŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œå»ºè®®è¯»è€…å…³æ³¨æœ€æ–°çš„æŠ€æœ¯åŠ¨æ€å’Œæ¡†æ¶æ›´æ–°ã€‚