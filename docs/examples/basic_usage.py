#!/usr/bin/env python3
"""
Firecrawlæ•°æ®é‡‡é›†å™¨ - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨Firecrawlæ•°æ®é‡‡é›†å™¨è¿›è¡ŒåŸºæœ¬çš„ç½‘é¡µå†…å®¹é‡‡é›†ã€‚
"""

import asyncio
import json
from typing import Dict, List, Optional
import requests
from datetime import datetime

class FirecrawlCollectorExample:
    """Firecrawlé‡‡é›†å™¨ä½¿ç”¨ç¤ºä¾‹"""
    
    def __init__(self, api_base: str = "http://localhost:8000", api_key: str = None):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨
        
        Args:
            api_base: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
        """
        self.api_base = api_base.rstrip('/')
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}" if api_key else "",
            "Content-Type": "application/json"
        }
    
    def health_check(self) -> Dict:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_base}/health", timeout=10)
            return response.json()
        except Exception as e:
            return {"error": str(e), "status": "unhealthy"}
    
    def crawl_single_url(self, url: str, options: Dict = None) -> Dict:
        """
        é‡‡é›†å•ä¸ªURL
        
        Args:
            url: è¦é‡‡é›†çš„URL
            options: é‡‡é›†é€‰é¡¹
            
        Returns:
            é‡‡é›†ç»“æœ
        """
        if options is None:
            options = {
                "formats": ["markdown"],
                "onlyMainContent": True,
                "includeTags": ["h1", "h2", "h3", "p", "a"],
                "excludeTags": ["script", "style", "nav", "footer"]
            }
        
        payload = {
            "url": url,
            "options": options
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/api/v1/crawl/url",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def crawl_batch_urls(self, urls: List[str], options: Dict = None) -> Dict:
        """
        æ‰¹é‡é‡‡é›†URLs
        
        Args:
            urls: URLåˆ—è¡¨
            options: é‡‡é›†é€‰é¡¹
            
        Returns:
            æ‰¹é‡é‡‡é›†ç»“æœ
        """
        if options is None:
            options = {
                "formats": ["markdown"],
                "onlyMainContent": True
            }
        
        payload = {
            "urls": urls,
            "options": options
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/api/v1/crawl/batch",
                headers=self.headers,
                json=payload,
                timeout=60
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def get_job_status(self, job_id: str) -> Dict:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        
        Args:
            job_id: ä»»åŠ¡ID
            
        Returns:
            ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        try:
            response = requests.get(
                f"{self.api_base}/api/v1/jobs/{job_id}",
                headers=self.headers,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def wait_for_completion(self, job_id: str, timeout: int = 300) -> Dict:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆ
        
        Args:
            job_id: ä»»åŠ¡ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æœ€ç»ˆä»»åŠ¡çŠ¶æ€
        """
        start_time = datetime.now()
        
        while (datetime.now() - start_time).seconds < timeout:
            status = self.get_job_status(job_id)
            
            if not status.get("success", True):
                return status
            
            job_status = status.get("status", "unknown")
            
            if job_status == "completed":
                return status
            elif job_status == "failed":
                return status
            elif job_status == "running":
                print(f"ä»»åŠ¡ {job_id} æ­£åœ¨è¿è¡Œä¸­...")
                import time
                time.sleep(2)
            else:
                print(f"æœªçŸ¥çŠ¶æ€: {job_status}")
                import time
                time.sleep(2)
        
        return {"error": "ä»»åŠ¡è¶…æ—¶", "success": False}
    
    def search_data(self, query: str, limit: int = 10) -> Dict:
        """
        æœç´¢é‡‡é›†æ•°æ®
        
        Args:
            query: æœç´¢å…³é”®è¯
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            æœç´¢ç»“æœ
        """
        params = {
            "query": query,
            "limit": limit
        }
        
        try:
            response = requests.get(
                f"{self.api_base}/api/v1/data/search",
                headers=self.headers,
                params=params,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºåŸºæœ¬ç”¨æ³•"""
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    collector = FirecrawlCollectorExample(
        api_base="http://localhost:8000",
        api_key="your_api_key_here"  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    )
    
    print("ğŸ”¥ Firecrawlæ•°æ®é‡‡é›†å™¨ - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. å¥åº·æ£€æŸ¥
    print("\n1. æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€...")
    health = collector.health_check()
    print(f"æœåŠ¡çŠ¶æ€: {health.get('status', 'unknown')}")
    
    if health.get("status") != "healthy":
        print("âŒ æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨")
        return
    
    # 2. å•é¡µé¢é‡‡é›†ç¤ºä¾‹
    print("\n2. å•é¡µé¢é‡‡é›†ç¤ºä¾‹...")
    test_url = "https://example.com"
    
    print(f"æ­£åœ¨é‡‡é›†: {test_url}")
    result = collector.crawl_single_url(test_url)
    
    if result.get("success"):
        job_id = result.get("job_id")
        print(f"âœ… ä»»åŠ¡å·²åˆ›å»ºï¼ŒID: {job_id}")
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        print("ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
        final_status = collector.wait_for_completion(job_id)
        
        if final_status.get("status") == "completed":
            data = final_status.get("result", {}).get("data", {})
            print(f"âœ… é‡‡é›†å®Œæˆï¼")
            print(f"æ ‡é¢˜: {data.get('title', 'N/A')}")
            print(f"å†…å®¹é•¿åº¦: {len(data.get('content', ''))}")
        else:
            print(f"âŒ ä»»åŠ¡å¤±è´¥: {final_status.get('error', 'Unknown error')}")
    else:
        print(f"âŒ é‡‡é›†å¤±è´¥: {result.get('error', 'Unknown error')}")
    
    # 3. æ‰¹é‡é‡‡é›†ç¤ºä¾‹
    print("\n3. æ‰¹é‡é‡‡é›†ç¤ºä¾‹...")
    test_urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml"
    ]
    
    print(f"æ­£åœ¨æ‰¹é‡é‡‡é›† {len(test_urls)} ä¸ªURL...")
    batch_result = collector.crawl_batch_urls(test_urls)
    
    if batch_result.get("success"):
        job_id = batch_result.get("job_id")
        print(f"âœ… æ‰¹é‡ä»»åŠ¡å·²åˆ›å»ºï¼ŒID: {job_id}")
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        print("ç­‰å¾…æ‰¹é‡ä»»åŠ¡å®Œæˆ...")
        final_status = collector.wait_for_completion(job_id)
        
        if final_status.get("status") == "completed":
            results = final_status.get("result", {}).get("results", [])
            success_count = sum(1 for r in results if r.get("status") == "success")
            print(f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(results)}")
            
            for i, result in enumerate(results):
                status = result.get("status", "unknown")
                url = result.get("url", "unknown")
                print(f"  {i+1}. {url} - {status}")
        else:
            print(f"âŒ æ‰¹é‡ä»»åŠ¡å¤±è´¥: {final_status.get('error', 'Unknown error')}")
    else:
        print(f"âŒ æ‰¹é‡é‡‡é›†å¤±è´¥: {batch_result.get('error', 'Unknown error')}")
    
    # 4. æ•°æ®æœç´¢ç¤ºä¾‹
    print("\n4. æ•°æ®æœç´¢ç¤ºä¾‹...")
    search_result = collector.search_data("example", limit=5)
    
    if search_result.get("success"):
        data = search_result.get("data", [])
        print(f"âœ… æ‰¾åˆ° {len(data)} æ¡ç›¸å…³æ•°æ®")
        
        for i, item in enumerate(data[:3]):  # åªæ˜¾ç¤ºå‰3æ¡
            title = item.get("title", "N/A")
            url = item.get("url", "N/A")
            print(f"  {i+1}. {title} - {url}")
    else:
        print(f"âŒ æœç´¢å¤±è´¥: {search_result.get('error', 'Unknown error')}")
    
    print("\nğŸ‰ ç¤ºä¾‹æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()
