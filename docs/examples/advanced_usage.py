#!/usr/bin/env python3
"""
Firecrawlæ•°æ®é‡‡é›†å™¨ - é«˜çº§ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨Firecrawlæ•°æ®é‡‡é›†å™¨è¿›è¡Œé«˜çº§åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç›‘æ§ã€æ•°æ®å¤„ç†ç­‰ã€‚
"""

import asyncio
import json
import time
from typing import Dict, List, Optional
import requests
from datetime import datetime, timedelta

class AdvancedFirecrawlCollector:
    """é«˜çº§Firecrawlé‡‡é›†å™¨ç¤ºä¾‹"""
    
    def __init__(self, api_base: str = "http://localhost:8000", api_key: str = None):
        self.api_base = api_base.rstrip('/')
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}" if api_key else "",
            "Content-Type": "application/json"
        }
    
    def create_monitor(self, url: str, name: str, schedule: str = "0 */6 * * *") -> Dict:
        """
        åˆ›å»ºç›‘æ§ä»»åŠ¡
        
        Args:
            url: ç›‘æ§çš„URL
            name: ç›‘æ§ä»»åŠ¡åç§°
            schedule: Cronè¡¨è¾¾å¼
            
        Returns:
            åˆ›å»ºç»“æœ
        """
        payload = {
            "url": url,
            "name": name,
            "schedule": schedule,
            "options": {
                "formats": ["markdown"],
                "onlyMainContent": True,
                "includeTags": ["h1", "h2", "h3", "p", "a", "img"]
            },
            "notifications": {
                "email": "admin@example.com",
                "webhook": "https://hooks.slack.com/services/..."
            }
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/api/v1/monitor/create",
                headers=self.headers,
                json=payload,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def get_monitors(self) -> Dict:
        """è·å–æ‰€æœ‰ç›‘æ§ä»»åŠ¡"""
        try:
            response = requests.get(
                f"{self.api_base}/api/v1/monitor/list",
                headers=self.headers,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def update_monitor(self, monitor_id: str, updates: Dict) -> Dict:
        """
        æ›´æ–°ç›‘æ§ä»»åŠ¡
        
        Args:
            monitor_id: ç›‘æ§ä»»åŠ¡ID
            updates: æ›´æ–°å†…å®¹
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        try:
            response = requests.put(
                f"{self.api_base}/api/v1/monitor/{monitor_id}",
                headers=self.headers,
                json=updates,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def delete_monitor(self, monitor_id: str) -> Dict:
        """åˆ é™¤ç›‘æ§ä»»åŠ¡"""
        try:
            response = requests.delete(
                f"{self.api_base}/api/v1/monitor/{monitor_id}",
                headers=self.headers,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def get_data_history(self, url: str = None, limit: int = 10) -> Dict:
        """
        è·å–é‡‡é›†å†å²
        
        Args:
            url: ç‰¹å®šURLçš„å†å²ï¼ˆå¯é€‰ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            å†å²æ•°æ®
        """
        params = {"limit": limit}
        if url:
            params["url"] = url
        
        try:
            response = requests.get(
                f"{self.api_base}/api/v1/data/history",
                headers=self.headers,
                params=params,
                timeout=10
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}
    
    def analyze_content_changes(self, url: str, days: int = 7) -> Dict:
        """
        åˆ†æå†…å®¹å˜åŒ–
        
        Args:
            url: è¦åˆ†æçš„URL
            days: åˆ†æå¤©æ•°
            
        Returns:
            å˜åŒ–åˆ†æç»“æœ
        """
        # è·å–å†å²æ•°æ®
        history = self.get_data_history(url, limit=100)
        
        if not history.get("success"):
            return history
        
        data_list = history.get("data", [])
        
        if len(data_list) < 2:
            return {"message": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æå˜åŒ–", "changes": []}
        
        # æŒ‰æ—¶é—´æ’åº
        data_list.sort(key=lambda x: x.get("timestamp", ""))
        
        changes = []
        for i in range(1, len(data_list)):
            current = data_list[i]
            previous = data_list[i-1]
            
            # ç®€å•çš„å˜åŒ–æ£€æµ‹ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
            current_content = current.get("content", "")
            previous_content = previous.get("content", "")
            
            if current_content != previous_content:
                change = {
                    "timestamp": current.get("timestamp"),
                    "change_type": "content_updated",
                    "content_length_change": len(current_content) - len(previous_content),
                    "title_change": current.get("title") != previous.get("title")
                }
                changes.append(change)
        
        return {
            "success": True,
            "url": url,
            "analysis_period": f"{days} days",
            "total_changes": len(changes),
            "changes": changes
        }
    
    def export_data(self, format: str = "json", limit: int = 100) -> Dict:
        """
        å¯¼å‡ºæ•°æ®
        
        Args:
            format: å¯¼å‡ºæ ¼å¼ (json, csv, markdown)
            limit: å¯¼å‡ºæ•°é‡é™åˆ¶
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        # è·å–æ•°æ®
        history = self.get_data_history(limit=limit)
        
        if not history.get("success"):
            return history
        
        data = history.get("data", [])
        
        if format == "json":
            return {
                "success": True,
                "format": "json",
                "data": data,
                "count": len(data)
            }
        elif format == "csv":
            # ç®€å•çš„CSVè½¬æ¢
            if not data:
                return {"success": True, "format": "csv", "data": "", "count": 0}
            
            csv_lines = ["url,title,timestamp,content_length"]
            for item in data:
                csv_line = f'"{item.get("url", "")}","{item.get("title", "")}","{item.get("timestamp", "")}",{len(item.get("content", ""))}'
                csv_lines.append(csv_line)
            
            return {
                "success": True,
                "format": "csv",
                "data": "\n".join(csv_lines),
                "count": len(data)
            }
        elif format == "markdown":
            # ç®€å•çš„Markdownè½¬æ¢
            if not data:
                return {"success": True, "format": "markdown", "data": "", "count": 0}
            
            md_lines = ["# Firecrawlæ•°æ®å¯¼å‡º", f"å¯¼å‡ºæ—¶é—´: {datetime.now().isoformat()}", f"æ•°æ®æ¡æ•°: {len(data)}", ""]
            
            for i, item in enumerate(data, 1):
                md_lines.extend([
                    f"## {i}. {item.get('title', 'Untitled')}",
                    f"**URL**: {item.get('url', 'N/A')}",
                    f"**æ—¶é—´**: {item.get('timestamp', 'N/A')}",
                    f"**å†…å®¹é•¿åº¦**: {len(item.get('content', ''))}",
                    "",
                    "### å†…å®¹é¢„è§ˆ",
                    item.get('content', '')[:500] + "..." if len(item.get('content', '')) > 500 else item.get('content', ''),
                    "",
                    "---",
                    ""
                ])
            
            return {
                "success": True,
                "format": "markdown",
                "data": "\n".join(md_lines),
                "count": len(data)
            }
        else:
            return {"error": f"ä¸æ”¯æŒçš„æ ¼å¼: {format}", "success": False}
    
    def crawl_with_retry(self, url: str, max_retries: int = 3, delay: int = 5) -> Dict:
        """
        å¸¦é‡è¯•çš„é‡‡é›†
        
        Args:
            url: è¦é‡‡é›†çš„URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            
        Returns:
            é‡‡é›†ç»“æœ
        """
        for attempt in range(max_retries):
            print(f"å°è¯•é‡‡é›† {url} (ç¬¬ {attempt + 1} æ¬¡)")
            
            result = self.crawl_single_url(url)
            
            if result.get("success"):
                return result
            
            if attempt < max_retries - 1:
                print(f"é‡‡é›†å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•...")
                time.sleep(delay)
            else:
                print(f"é‡‡é›†å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        
        return {"error": "è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°", "success": False}
    
    def crawl_single_url(self, url: str, options: Dict = None) -> Dict:
        """å•é¡µé¢é‡‡é›†ï¼ˆåŸºç¡€æ–¹æ³•ï¼‰"""
        if options is None:
            options = {
                "formats": ["markdown"],
                "onlyMainContent": True
            }
        
        payload = {
            "url": url,
            "options": options
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/api/v1/crawl/url",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            return response.json()
        except Exception as e:
            return {"error": str(e), "success": False}

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºé«˜çº§ç”¨æ³•"""
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    collector = AdvancedFirecrawlCollector(
        api_base="http://localhost:8000",
        api_key="your_api_key_here"  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    )
    
    print("ğŸ”¥ Firecrawlæ•°æ®é‡‡é›†å™¨ - é«˜çº§ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆ›å»ºç›‘æ§ä»»åŠ¡
    print("\n1. åˆ›å»ºç›‘æ§ä»»åŠ¡...")
    monitor_result = collector.create_monitor(
        url="https://example.com",
        name="Example Site Monitor",
        schedule="0 */6 * * *"  # æ¯6å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    )
    
    if monitor_result.get("success"):
        monitor_id = monitor_result.get("monitor_id")
        print(f"âœ… ç›‘æ§ä»»åŠ¡å·²åˆ›å»ºï¼ŒID: {monitor_id}")
    else:
        print(f"âŒ åˆ›å»ºç›‘æ§ä»»åŠ¡å¤±è´¥: {monitor_result.get('error', 'Unknown error')}")
        monitor_id = None
    
    # 2. è·å–ç›‘æ§ä»»åŠ¡åˆ—è¡¨
    print("\n2. è·å–ç›‘æ§ä»»åŠ¡åˆ—è¡¨...")
    monitors = collector.get_monitors()
    
    if monitors.get("success"):
        monitor_list = monitors.get("data", [])
        print(f"âœ… æ‰¾åˆ° {len(monitor_list)} ä¸ªç›‘æ§ä»»åŠ¡")
        
        for monitor in monitor_list:
            name = monitor.get("name", "Unnamed")
            url = monitor.get("url", "N/A")
            status = monitor.get("enabled", False)
            print(f"  - {name}: {url} ({'å¯ç”¨' if status else 'ç¦ç”¨'})")
    else:
        print(f"âŒ è·å–ç›‘æ§ä»»åŠ¡å¤±è´¥: {monitors.get('error', 'Unknown error')}")
    
    # 3. å¸¦é‡è¯•çš„é‡‡é›†
    print("\n3. å¸¦é‡è¯•çš„é‡‡é›†ç¤ºä¾‹...")
    test_url = "https://httpbin.org/html"
    
    result = collector.crawl_with_retry(test_url, max_retries=3, delay=2)
    
    if result.get("success"):
        print(f"âœ… é‡‡é›†æˆåŠŸï¼ä»»åŠ¡ID: {result.get('job_id')}")
    else:
        print(f"âŒ é‡‡é›†å¤±è´¥: {result.get('error', 'Unknown error')}")
    
    # 4. è·å–é‡‡é›†å†å²
    print("\n4. è·å–é‡‡é›†å†å²...")
    history = collector.get_data_history(limit=5)
    
    if history.get("success"):
        data_list = history.get("data", [])
        print(f"âœ… æ‰¾åˆ° {len(data_list)} æ¡å†å²è®°å½•")
        
        for i, item in enumerate(data_list[:3], 1):
            title = item.get("title", "Untitled")
            url = item.get("url", "N/A")
            timestamp = item.get("timestamp", "N/A")
            print(f"  {i}. {title} - {url} ({timestamp})")
    else:
        print(f"âŒ è·å–å†å²å¤±è´¥: {history.get('error', 'Unknown error')}")
    
    # 5. å†…å®¹å˜åŒ–åˆ†æ
    print("\n5. å†…å®¹å˜åŒ–åˆ†æ...")
    analysis_url = "https://example.com"
    
    changes = collector.analyze_content_changes(analysis_url, days=7)
    
    if changes.get("success"):
        total_changes = changes.get("total_changes", 0)
        print(f"âœ… åˆ†æå®Œæˆï¼å‘ç° {total_changes} æ¬¡å˜åŒ–")
        
        change_list = changes.get("changes", [])
        for change in change_list[:3]:  # åªæ˜¾ç¤ºå‰3æ¬¡å˜åŒ–
            timestamp = change.get("timestamp", "N/A")
            change_type = change.get("change_type", "unknown")
            print(f"  - {timestamp}: {change_type}")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {changes.get('error', 'Unknown error')}")
    
    # 6. æ•°æ®å¯¼å‡º
    print("\n6. æ•°æ®å¯¼å‡ºç¤ºä¾‹...")
    
    # å¯¼å‡ºä¸ºJSON
    json_export = collector.export_data(format="json", limit=5)
    if json_export.get("success"):
        print(f"âœ… JSONå¯¼å‡ºæˆåŠŸï¼ŒåŒ…å« {json_export.get('count', 0)} æ¡æ•°æ®")
    
    # å¯¼å‡ºä¸ºMarkdown
    md_export = collector.export_data(format="markdown", limit=3)
    if md_export.get("success"):
        print(f"âœ… Markdownå¯¼å‡ºæˆåŠŸï¼ŒåŒ…å« {md_export.get('count', 0)} æ¡æ•°æ®")
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open("firecrawl_export.md", "w", encoding="utf-8") as f:
            f.write(md_export.get("data", ""))
        print("  ğŸ“„ å·²ä¿å­˜åˆ° firecrawl_export.md")
    
    # 7. æ¸…ç†ç›‘æ§ä»»åŠ¡ï¼ˆå¦‚æœåˆ›å»ºäº†çš„è¯ï¼‰
    if monitor_id:
        print(f"\n7. æ¸…ç†ç›‘æ§ä»»åŠ¡ {monitor_id}...")
        delete_result = collector.delete_monitor(monitor_id)
        
        if delete_result.get("success"):
            print("âœ… ç›‘æ§ä»»åŠ¡å·²åˆ é™¤")
        else:
            print(f"âŒ åˆ é™¤ç›‘æ§ä»»åŠ¡å¤±è´¥: {delete_result.get('error', 'Unknown error')}")
    
    print("\nğŸ‰ é«˜çº§ç¤ºä¾‹æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()
