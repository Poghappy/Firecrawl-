#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»Šæ—¥å¤´æ¡æ–°é—»æ‰¹é‡é‡‡é›†å™¨
ä½¿ç”¨Firecrawl v2 APIè¿›è¡Œé«˜æ•ˆçš„æ–°é—»å†…å®¹é‡‡é›†
"""

import requests
import json
import time
from datetime import datetime
from typing import List, Dict, Optional

class ToutiaoBatchScraper:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.firecrawl.dev/v2"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def search_news(self, query: str, limit: int = 10) -> List[Dict]:
        """æœç´¢æ–°é—»è·å–URLåˆ—è¡¨"""
        print(f"ğŸ” æœç´¢æ–°é—»: {query}")
        
        search_payload = {
            "query": query,
            "limit": limit,
            "scrapeOptions": {
                "formats": ["markdown"],
                "onlyMainContent": True,
                "waitFor": 3000
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/search", 
                json=search_payload, 
                headers=self.headers
            )
            
            if response.status_code == 200:
                data = response.json()
                urls = []
                
                for item in data.get('data', []):
                    urls.append({
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'description': item.get('description')
                    })
                
                print(f"âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(urls)} æ¡æ–°é—»")
                return urls
            else:
                print(f"âŒ æœç´¢å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            return []
    
    def batch_scrape_news(self, urls: List[str], **options) -> Dict:
        """æ‰¹é‡æŠ“å–æ–°é—»å†…å®¹"""
        print(f"ğŸ“„ å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} æ¡æ–°é—»...")
        
        # é»˜è®¤é…ç½®
        default_options = {
            "maxConcurrency": 3,  # æ§åˆ¶å¹¶å‘æ•°ï¼Œé¿å…APIé™åˆ¶
            "ignoreInvalidURLs": True,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 5000,  # ç­‰å¾…JSæ¸²æŸ“
            "timeout": 30,
            "removeBase64Images": True,
            "blockAds": True,
            "storeInCache": True,
            "location": {
                "country": "CN",  # ä¸­å›½
                "languages": ["zh-CN"]  # ä¸­æ–‡
            }
        }
        
        # åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰é€‰é¡¹
        default_options.update(options)
        
        payload = {
            "urls": urls,
            **default_options
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/batch/scrape",
                json=payload,
                headers=self.headers
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… æ‰¹é‡æŠ“å–ä»»åŠ¡å·²æäº¤")
                print(f"ä»»åŠ¡ID: {result.get('id', 'æœªçŸ¥')}")
                print(f"çŠ¶æ€: {result.get('status', 'æœªçŸ¥')}")
                return result
            else:
                print(f"âŒ æ‰¹é‡æŠ“å–å¤±è´¥: {response.status_code} - {response.text}")
                return {}
                
        except Exception as e:
            print(f"âŒ æ‰¹é‡æŠ“å–é”™è¯¯: {e}")
            return {}
    
    def check_batch_status(self, task_id: str) -> Dict:
        """æ£€æŸ¥æ‰¹é‡æŠ“å–ä»»åŠ¡çŠ¶æ€"""
        try:
            response = requests.get(
                f"{self.base_url}/batch/scrape/{task_id}",
                headers=self.headers
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ çŠ¶æ€æŸ¥è¯¢é”™è¯¯: {e}")
            return {}
    
    def get_batch_results(self, task_id: str) -> List[Dict]:
        """è·å–æ‰¹é‡æŠ“å–ç»“æœ"""
        print(f"ğŸ“Š è·å–ä»»åŠ¡ {task_id} çš„ç»“æœ...")
        
        status = self.check_batch_status(task_id)
        
        if status.get('status') == 'completed':
            results = []
            for item in status.get('data', []):
                if item.get('success'):
                    results.append({
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'content': item.get('markdown', ''),
                        'metadata': item.get('metadata', {})
                    })
            
            print(f"âœ… è·å–åˆ° {len(results)} æ¡æˆåŠŸæŠ“å–çš„æ–°é—»")
            return results
        else:
            print(f"â³ ä»»åŠ¡çŠ¶æ€: {status.get('status', 'æœªçŸ¥')}")
            return []
    
    def scrape_toutiao_news(self, queries: List[str], limit_per_query: int = 5) -> List[Dict]:
        """å®Œæ•´çš„ä»Šæ—¥å¤´æ¡æ–°é—»é‡‡é›†æµç¨‹"""
        print("ğŸš€ å¼€å§‹ä»Šæ—¥å¤´æ¡æ–°é—»é‡‡é›†æµç¨‹")
        print("=" * 50)
        
        all_news = []
        
        for query in queries:
            print(f"\nğŸ“° å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. æœç´¢æ–°é—»
            news_urls = self.search_news(query, limit_per_query)
            
            if not news_urls:
                continue
            
            # 2. æå–URLåˆ—è¡¨
            urls = [item['url'] for item in news_urls if item['url']]
            
            if not urls:
                continue
            
            # 3. æ‰¹é‡æŠ“å–
            batch_result = self.batch_scrape_news(urls)
            
            if not batch_result.get('id'):
                continue
            
            task_id = batch_result['id']
            
            # 4. ç­‰å¾…ä»»åŠ¡å®Œæˆ
            print("â³ ç­‰å¾…æ‰¹é‡æŠ“å–å®Œæˆ...")
            max_wait = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            wait_time = 0
            
            while wait_time < max_wait:
                status = self.check_batch_status(task_id)
                
                if status.get('status') == 'completed':
                    break
                elif status.get('status') == 'failed':
                    print("âŒ æ‰¹é‡æŠ“å–ä»»åŠ¡å¤±è´¥")
                    break
                
                time.sleep(10)  # ç­‰å¾…10ç§’
                wait_time += 10
                print(f"â³ ç­‰å¾…ä¸­... ({wait_time}s)")
            
            # 5. è·å–ç»“æœ
            results = self.get_batch_results(task_id)
            
            # 6. åˆå¹¶åˆ°æ€»ç»“æœ
            for result in results:
                # æ·»åŠ æœç´¢æ—¶çš„æ ‡é¢˜å’Œæè¿°
                for news_info in news_urls:
                    if news_info['url'] == result['url']:
                        result['search_title'] = news_info['title']
                        result['search_description'] = news_info['description']
                        break
                
                all_news.append(result)
        
        print(f"\nğŸ‰ é‡‡é›†å®Œæˆ! æ€»å…±è·å¾— {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def save_results(self, results: List[Dict], filename: str = None):
        """ä¿å­˜é‡‡é›†ç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"toutiao_news_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    
    # ä½¿ç”¨æ‚¨çš„APIå¯†é’¥
    API_KEY = "fc-0a2c801f433d4718bcd8189f2742edf4"
    
    # åˆ›å»ºé‡‡é›†å™¨å®ä¾‹
    scraper = ToutiaoBatchScraper(API_KEY)
    
    # å®šä¹‰æœç´¢æŸ¥è¯¢
    queries = [
        "äººå·¥æ™ºèƒ½ æœ€æ–°æ–°é—»",
        "ç§‘æŠ€èµ„è®¯ å¤´æ¡",
        "AIæŠ€æœ¯å‘å±•"
    ]
    
    print("ğŸ”¥ ä»Šæ—¥å¤´æ¡æ–°é—»æ‰¹é‡é‡‡é›†å™¨")
    print("=" * 50)
    
    try:
        # æ‰§è¡Œé‡‡é›†
        results = scraper.scrape_toutiao_news(queries, limit_per_query=3)
        
        if results:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"\nğŸ“Š é‡‡é›†ç»“æœæ‘˜è¦:")
            for i, news in enumerate(results[:5], 1):
                print(f"{i}. {news.get('search_title', news.get('title', 'æ— æ ‡é¢˜'))}")
                print(f"   URL: {news.get('url', 'æ— URL')}")
                print(f"   å†…å®¹é•¿åº¦: {len(news.get('content', ''))} å­—ç¬¦")
                print()
            
            # ä¿å­˜ç»“æœ
            scraper.save_results(results)
            
        else:
            print("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ–°é—»")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        print(f"âŒ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
