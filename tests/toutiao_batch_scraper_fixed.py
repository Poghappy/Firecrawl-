#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»Šæ—¥å¤´æ¡æ–°é—»æ‰¹é‡é‡‡é›†å™¨ (ä¿®å¤ç‰ˆ)
ä½¿ç”¨Firecrawl v2 APIè¿›è¡Œé«˜æ•ˆçš„æ–°é—»å†…å®¹é‡‡é›†

ä¿®å¤å†…å®¹:
1. ä¿®å¤APIå“åº”ç»“æ„è§£æé”™è¯¯
2. ä¿®å¤é…ç½®å‚æ•°å†²çª
3. ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†APIå¯†é’¥
4. å¢å¼ºé”™è¯¯å¤„ç†å’Œè¾“å…¥éªŒè¯
5. å®Œå–„æ—¥å¿—è®°å½•

ä½œè€…: AIä»£ç å®¡æŸ¥åŠ©æ‰‹
ä¿®å¤æ—¶é—´: 2025-01-22
ç‰ˆæœ¬: v2.0 (ä¿®å¤ç‰ˆ)
"""

import requests
import json
import time
import os
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class ToutiaoBatchScraper:
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–é‡‡é›†å™¨
        
        Args:
            api_key: Firecrawl APIå¯†é’¥ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        # ä¿®å¤ï¼šä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.api_key = api_key or os.getenv('FIRECRAWL_API_KEY')
        if not self.api_key:
            raise ValueError("FIRECRAWL_API_KEY environment variable is required")
        
        self.base_url = "https://api.firecrawl.dev/v2"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
    
    def search_news(self, query: str, limit: int = 10) -> List[Dict]:
        """æœç´¢æ–°é—»è·å–URLåˆ—è¡¨
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: æ–°é—»URLåˆ—è¡¨
        """
        self.logger.info(f"æœç´¢æ–°é—»: {query}")
        
        # è¾“å…¥éªŒè¯
        if not query or not isinstance(query, str):
            self.logger.error("æŸ¥è¯¢å‚æ•°æ— æ•ˆ")
            return []
        
        if not isinstance(limit, int) or limit <= 0:
            self.logger.error("é™åˆ¶å‚æ•°æ— æ•ˆ")
            return []
        
        search_payload = {
            "query": query,
            "limit": limit,
            "scrapeOptions": {
                "formats": ["markdown"],
                "onlyMainContent": True,
                "waitFor": 3000
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/search", 
                json=search_payload, 
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                urls = []
                
                # ä¿®å¤ï¼šæ­£ç¡®è§£æAPIå“åº”ç»“æ„
                if 'data' in data and 'web' in data['data']:
                    for item in data['data']['web']:
                        if item.get('url'):  # ç¡®ä¿URLå­˜åœ¨
                            urls.append({
                                'url': item.get('url'),
                                'title': item.get('title', ''),
                                'description': item.get('description', '')
                            })
                
                self.logger.info(f"æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(urls)} æ¡æ–°é—»")
                return urls
            else:
                self.logger.error(f"æœç´¢å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.error(f"æœç´¢æœªçŸ¥é”™è¯¯: {e}")
            return []
    
    def batch_scrape_news(self, urls: List[str], **options) -> Dict:
        """æ‰¹é‡æŠ“å–æ–°é—»å†…å®¹
        
        Args:
            urls: URLåˆ—è¡¨
            **options: è‡ªå®šä¹‰é…ç½®é€‰é¡¹
            
        Returns:
            Dict: æ‰¹é‡æŠ“å–ä»»åŠ¡ç»“æœ
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} æ¡æ–°é—»...")
        
        # è¾“å…¥éªŒè¯
        if not urls or not isinstance(urls, list):
            self.logger.error("URLåˆ—è¡¨æ— æ•ˆ")
            return {}
        
        # ä¿®å¤ï¼šæ­£ç¡®çš„é…ç½®å‚æ•°
        default_options = {
            "maxConcurrency": 1,  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…APIé™åˆ¶
            "ignoreInvalidURLs": True,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 2000,      # 2ç§’
            "timeout": 60000,     # 60ç§’ (waitForçš„30å€)
            "removeBase64Images": True,
            "blockAds": True,
            "storeInCache": True,
            "location": {
                "country": "CN",  # ä¸­å›½
                "languages": ["zh-CN"]  # ä¸­æ–‡
            }
        }
        
        # åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰é€‰é¡¹
        default_options.update(options)
        
        # éªŒè¯é…ç½®
        if default_options["waitFor"] >= default_options["timeout"] / 2:
            self.logger.error("waitFor must be less than half of timeout")
            return {}
        
        payload = {
            "urls": urls,
            **default_options
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/batch/scrape",
                json=payload,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                self.logger.info(f"æ‰¹é‡æŠ“å–ä»»åŠ¡å·²æäº¤: {result.get('id', 'æœªçŸ¥')}")
                return result
            else:
                self.logger.error(f"æ‰¹é‡æŠ“å–å¤±è´¥: {response.status_code} - {response.text}")
                return {}
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"æ‰¹é‡æŠ“å–ç½‘ç»œé”™è¯¯: {e}")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"æ‰¹é‡æŠ“å–JSONè§£æé”™è¯¯: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æŠ“å–æœªçŸ¥é”™è¯¯: {e}")
            return {}
    
    def check_batch_status(self, task_id: str) -> Dict:
        """æ£€æŸ¥æ‰¹é‡æŠ“å–ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            Dict: ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        if not task_id or not isinstance(task_id, str):
            self.logger.error("ä»»åŠ¡IDæ— æ•ˆ")
            return {}
        
        try:
            response = requests.get(
                f"{self.base_url}/batch/scrape/{task_id}",
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {response.status_code}")
                return {}
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"çŠ¶æ€æŸ¥è¯¢ç½‘ç»œé”™è¯¯: {e}")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"çŠ¶æ€æŸ¥è¯¢JSONè§£æé”™è¯¯: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"çŠ¶æ€æŸ¥è¯¢æœªçŸ¥é”™è¯¯: {e}")
            return {}
    
    def get_batch_results(self, task_id: str) -> List[Dict]:
        """è·å–æ‰¹é‡æŠ“å–ç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            List[Dict]: æˆåŠŸæŠ“å–çš„æ–°é—»åˆ—è¡¨
        """
        self.logger.info(f"è·å–ä»»åŠ¡ {task_id} çš„ç»“æœ...")
        
        status = self.check_batch_status(task_id)
        
        if status.get('status') == 'completed':
            results = []
            for item in status.get('data', []):
                # ä¿®å¤ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºï¼Œè€Œä¸æ˜¯successå­—æ®µ
                content = item.get('markdown', '')
                if content and len(content.strip()) > 0:
                    results.append({
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'content': content,
                        'metadata': item.get('metadata', {})
                    })
            
            self.logger.info(f"è·å–åˆ° {len(results)} æ¡æˆåŠŸæŠ“å–çš„æ–°é—»")
            return results
        else:
            self.logger.info(f"ä»»åŠ¡çŠ¶æ€: {status.get('status', 'æœªçŸ¥')}")
            return []
    
    def scrape_toutiao_news(self, queries: List[str], limit_per_query: int = 5) -> List[Dict]:
        """å®Œæ•´çš„ä»Šæ—¥å¤´æ¡æ–°é—»é‡‡é›†æµç¨‹
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            limit_per_query: æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            List[Dict]: é‡‡é›†åˆ°çš„æ–°é—»åˆ—è¡¨
        """
        self.logger.info("å¼€å§‹ä»Šæ—¥å¤´æ¡æ–°é—»é‡‡é›†æµç¨‹")
        
        # è¾“å…¥éªŒè¯
        if not queries or not isinstance(queries, list):
            self.logger.error("æŸ¥è¯¢åˆ—è¡¨æ— æ•ˆ")
            return []
        
        all_news = []
        
        for query in queries:
            self.logger.info(f"å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. æœç´¢æ–°é—»
            news_urls = self.search_news(query, limit_per_query)
            
            if not news_urls:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æœªæ‰¾åˆ°ç»“æœ")
                continue
            
            # 2. æå–URLåˆ—è¡¨
            urls = [item['url'] for item in news_urls if item.get('url')]
            
            if not urls:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æœªæ‰¾åˆ°æœ‰æ•ˆURL")
                continue
            
            # 3. æ‰¹é‡æŠ“å–
            batch_result = self.batch_scrape_news(urls)
            
            if not batch_result.get('id'):
                self.logger.error(f"æŸ¥è¯¢ '{query}' æ‰¹é‡æŠ“å–ä»»åŠ¡åˆ›å»ºå¤±è´¥")
                continue
            
            task_id = batch_result['id']
            
            # 4. ç­‰å¾…ä»»åŠ¡å®Œæˆ
            self.logger.info("ç­‰å¾…æ‰¹é‡æŠ“å–å®Œæˆ...")
            max_wait = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            wait_time = 0
            
            while wait_time < max_wait:
                status = self.check_batch_status(task_id)
                
                if status.get('status') == 'completed':
                    break
                elif status.get('status') == 'failed':
                    self.logger.error("æ‰¹é‡æŠ“å–ä»»åŠ¡å¤±è´¥")
                    break
                
                time.sleep(10)  # ç­‰å¾…10ç§’
                wait_time += 10
                self.logger.info(f"ç­‰å¾…ä¸­... ({wait_time}s)")
            
            # 5. è·å–ç»“æœ
            results = self.get_batch_results(task_id)
            
            # 6. åˆå¹¶åˆ°æ€»ç»“æœ
            for result in results:
                # æ·»åŠ æœç´¢æ—¶çš„æ ‡é¢˜å’Œæè¿°
                for news_info in news_urls:
                    if news_info['url'] == result['url']:
                        result['search_title'] = news_info['title']
                        result['search_description'] = news_info['description']
                        break
                
                all_news.append(result)
        
        self.logger.info(f"é‡‡é›†å®Œæˆ! æ€»å…±è·å¾— {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def save_results(self, results: List[Dict], filename: Optional[str] = None) -> str:
        """ä¿å­˜é‡‡é›†ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: é‡‡é›†ç»“æœåˆ—è¡¨
            filename: æ–‡ä»¶åï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶å
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"toutiao_news_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv('FIRECRAWL_API_KEY'):
        print("âŒ è¯·è®¾ç½®FIRECRAWL_API_KEYç¯å¢ƒå˜é‡")
        print("ä¾‹å¦‚: export FIRECRAWL_API_KEY='your-api-key'")
        return
    
    try:
        # åˆ›å»ºé‡‡é›†å™¨å®ä¾‹
        scraper = ToutiaoBatchScraper()
        
        # å®šä¹‰æœç´¢æŸ¥è¯¢
        queries = [
            "äººå·¥æ™ºèƒ½ æœ€æ–°æ–°é—»",
            "ç§‘æŠ€èµ„è®¯ å¤´æ¡",
            "AIæŠ€æœ¯å‘å±•"
        ]
        
        print("ğŸ”¥ ä»Šæ—¥å¤´æ¡æ–°é—»æ‰¹é‡é‡‡é›†å™¨ (ä¿®å¤ç‰ˆ)")
        print("=" * 50)
        
        # æ‰§è¡Œé‡‡é›†
        results = scraper.scrape_toutiao_news(queries, limit_per_query=3)
        
        if results:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"\nğŸ“Š é‡‡é›†ç»“æœæ‘˜è¦:")
            for i, news in enumerate(results[:5], 1):
                print(f"{i}. {news.get('search_title', news.get('title', 'æ— æ ‡é¢˜'))}")
                print(f"   URL: {news.get('url', 'æ— URL')}")
                print(f"   å†…å®¹é•¿åº¦: {len(news.get('content', ''))} å­—ç¬¦")
                print()
            
            # ä¿å­˜ç»“æœ
            filename = scraper.save_results(results)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        else:
            print("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ–°é—»")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        print(f"âŒ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
