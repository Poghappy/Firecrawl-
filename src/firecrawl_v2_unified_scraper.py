#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Firecrawl v2 ç»Ÿä¸€æ–°é—»é‡‡é›†å™¨
åŸºäºæ·±åº¦é—®é¢˜è¯Šæ–­çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

è§£å†³çš„æ ¸å¿ƒé—®é¢˜:
1. APIç‰ˆæœ¬å…¼å®¹æ€§ - ç»Ÿä¸€ä½¿ç”¨v2 API
2. JavaScriptæ¸²æŸ“ - ä¼˜åŒ–ç­‰å¾…æ—¶é—´å’Œæ¸²æŸ“ç­–ç•¥
3. ä¸­æ–‡å†…å®¹å¤„ç† - ç‰¹æ®Šç¼–ç å’Œè¯­è¨€å¤„ç†
4. æ•°æ®è´¨é‡ä¿è¯ - æ™ºèƒ½å†…å®¹è¿‡æ»¤å’ŒéªŒè¯

ä½œè€…: AIä»£ç å®¡æŸ¥åŠ©æ‰‹ + æ·±åº¦è¯Šæ–­åˆ†æ
åˆ›å»ºæ—¶é—´: 2025-01-22
ç‰ˆæœ¬: v3.0 (ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ)
"""

import requests
import json
import time
import os
import logging
import re
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
from urllib.parse import urlparse
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class FirecrawlV2UnifiedScraper:
    """Firecrawl v2 ç»Ÿä¸€æ–°é—»é‡‡é›†å™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–é‡‡é›†å™¨
        
        Args:
            api_key: Firecrawl APIå¯†é’¥
        """
        self.api_key = api_key or os.getenv('FIRECRAWL_API_KEY')
        if not self.api_key:
            raise ValueError("FIRECRAWL_API_KEY environment variable is required")
        
        self.base_url = "https://api.firecrawl.dev/v2"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        self.logger = logging.getLogger(__name__)
        
        # ä¸­æ–‡æ–°é—»é‡‡é›†ä¼˜åŒ–é…ç½®
        self.chinese_config = {
            "waitFor": 5000,  # å¢åŠ ç­‰å¾…æ—¶é—´å¤„ç†JavaScriptæ¸²æŸ“
            "timeout": 90000,  # å¢åŠ è¶…æ—¶æ—¶é—´
            "blockAds": True,
            "removeBase64Images": True,
            "location": {
                "country": "CN",
                "languages": ["zh-CN"]
            },
            "formats": ["markdown", "summary"],
            "onlyMainContent": True
        }
    
    def search_news_v2(self, query: str, limit: int = 10, sources: List[str] = None) -> List[Dict]:
        """ä½¿ç”¨v2 APIæœç´¢æ–°é—»
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: ç»“æœæ•°é‡é™åˆ¶
            sources: æŒ‡å®šæœç´¢æº ['web', 'images', 'news']
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"æœç´¢æ–°é—»: {query}")
        
        # è¾“å…¥éªŒè¯
        if not query or not isinstance(query, str):
            self.logger.error("æŸ¥è¯¢å‚æ•°æ— æ•ˆ")
            return []
        
        search_payload = {
            "query": query,
            "limit": limit,
            "scrapeOptions": self.chinese_config
        }
        
        # å¦‚æœæŒ‡å®šäº†æœç´¢æºï¼Œæ·»åŠ åˆ°payload
        if sources:
            search_payload["sources"] = sources
        
        try:
            response = requests.post(
                f"{self.base_url}/search",
                json=search_payload,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                # ç»Ÿä¸€å¤„ç†v2 APIå“åº”ç»“æ„
                for source_type in ['web', 'images', 'news']:
                    if 'data' in data and source_type in data['data']:
                        for item in data['data'][source_type]:
                            if item.get('url'):
                                results.append({
                                    'url': item.get('url'),
                                    'title': item.get('title', ''),
                                    'description': item.get('description', ''),
                                    'source_type': source_type,
                                    'position': item.get('position', 0)
                                })
                
                self.logger.info(f"æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return results
            else:
                self.logger.error(f"æœç´¢å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"æœç´¢ç½‘ç»œé”™è¯¯: {e}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"æœç´¢JSONè§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            self.logger.error(f"æœç´¢æœªçŸ¥é”™è¯¯: {e}")
            return []
    
    def scrape_single_url(self, url: str, enhanced_config: Dict = None) -> Optional[Dict]:
        """æŠ“å–å•ä¸ªURL
        
        Args:
            url: ç›®æ ‡URL
            enhanced_config: å¢å¼ºé…ç½®
            
        Returns:
            Optional[Dict]: æŠ“å–ç»“æœ
        """
        if not url:
            return None
        
        config = self.chinese_config.copy()
        if enhanced_config:
            config.update(enhanced_config)
        
        scrape_payload = {
            "url": url,
            **config
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/scrape",
                json=scrape_payload,
                headers=self.headers,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # æ£€æŸ¥å†…å®¹è´¨é‡
                content = data.get('markdown', '')
                if self._is_valid_content(content):
                    return {
                        'url': url,
                        'title': data.get('title', ''),
                        'content': content,
                        'summary': data.get('summary', ''),
                        'metadata': data.get('metadata', {}),
                        'success': True
                    }
                else:
                    self.logger.warning(f"å†…å®¹è´¨é‡ä¸ç¬¦åˆè¦æ±‚: {url}")
                    return None
            else:
                self.logger.error(f"æŠ“å–å¤±è´¥: {url} - {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"æŠ“å–é”™è¯¯: {url} - {e}")
            return None
    
    def batch_scrape_v2(self, urls: List[str], max_concurrency: int = 2) -> List[Dict]:
        """ä½¿ç”¨v2 APIæ‰¹é‡æŠ“å–
        
        Args:
            urls: URLåˆ—è¡¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            List[Dict]: æŠ“å–ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} ä¸ªURL")
        
        if not urls:
            return []
        
        # ä¼˜åŒ–é…ç½®ç”¨äºæ‰¹é‡æŠ“å–
        batch_config = self.chinese_config.copy()
        batch_config.update({
            "maxConcurrency": max_concurrency,
            "ignoreInvalidURLs": True,
            "timeout": 120000,  # å¢åŠ æ‰¹é‡æŠ“å–è¶…æ—¶æ—¶é—´
        })
        
        batch_payload = {
            "urls": urls,
            **batch_config
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/batch/scrape",
                json=batch_payload,
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                task_id = result.get('id')
                
                if task_id:
                    self.logger.info(f"æ‰¹é‡æŠ“å–ä»»åŠ¡å·²æäº¤: {task_id}")
                    return self._monitor_batch_task(task_id)
                else:
                    self.logger.error("æ‰¹é‡æŠ“å–ä»»åŠ¡åˆ›å»ºå¤±è´¥")
                    return []
            else:
                self.logger.error(f"æ‰¹é‡æŠ“å–å¤±è´¥: {response.status_code} - {response.text}")
                return []
                
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æŠ“å–é”™è¯¯: {e}")
            return []
    
    def _monitor_batch_task(self, task_id: str, max_wait: int = 300) -> List[Dict]:
        """ç›‘æ§æ‰¹é‡æŠ“å–ä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            max_wait: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            List[Dict]: æˆåŠŸæŠ“å–çš„ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"ç›‘æ§ä»»åŠ¡: {task_id}")
        
        wait_time = 0
        while wait_time < max_wait:
            try:
                response = requests.get(
                    f"{self.base_url}/batch/scrape/{task_id}",
                    headers=self.headers,
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    status = data.get('status')
                    
                    self.logger.info(f"ä»»åŠ¡çŠ¶æ€: {status} (ç­‰å¾… {wait_time}s)")
                    
                    if status == 'completed':
                        return self._process_batch_results(data)
                    elif status == 'failed':
                        self.logger.error(f"ä»»åŠ¡å¤±è´¥: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        return []
                    
                    time.sleep(10)
                    wait_time += 10
                else:
                    self.logger.error(f"çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {response.status_code}")
                    break
                    
            except Exception as e:
                self.logger.error(f"ç›‘æ§é”™è¯¯: {e}")
                break
        
        self.logger.warning("ä»»åŠ¡ç›‘æ§è¶…æ—¶")
        return []
    
    def _process_batch_results(self, data: Dict) -> List[Dict]:
        """å¤„ç†æ‰¹é‡æŠ“å–ç»“æœ
        
        Args:
            data: ä»»åŠ¡ç»“æœæ•°æ®
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç»“æœåˆ—è¡¨
        """
        results = []
        raw_results = data.get('data', [])
        
        for item in raw_results:
            # åŸºäºæ‚¨çš„è¯Šæ–­ï¼šæ£€æŸ¥å®é™…å†…å®¹è€Œésuccesså­—æ®µ
            content = item.get('markdown', '')
            if self._is_valid_content(content):
                results.append({
                    'url': item.get('url', ''),
                    'title': item.get('title', ''),
                    'content': content,
                    'summary': item.get('summary', ''),
                    'metadata': item.get('metadata', {}),
                    'success': True
                })
            else:
                self.logger.debug(f"è·³è¿‡æ— æ•ˆå†…å®¹: {item.get('url', 'unknown')}")
        
        self.logger.info(f"å¤„ç†å®Œæˆ: {len(results)}/{len(raw_results)} æ¡æœ‰æ•ˆç»“æœ")
        return results
    
    def _is_valid_content(self, content: str, min_length: int = 100) -> bool:
        """éªŒè¯å†…å®¹è´¨é‡
        
        Args:
            content: å†…å®¹æ–‡æœ¬
            min_length: æœ€å°é•¿åº¦è¦æ±‚
            
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        if not content or not isinstance(content, str):
            return False
        
        content = content.strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(content) < min_length:
            return False
        
        # ä¸­æ–‡å†…å®¹è´¨é‡æ£€æŸ¥
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        if chinese_chars < 20:  # è‡³å°‘20ä¸ªä¸­æ–‡å­—ç¬¦
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
        meaningful_patterns = [
            r'[\u4e00-\u9fff]{2,}',  # ä¸­æ–‡è¯æ±‡
            r'[a-zA-Z]{3,}',         # è‹±æ–‡è¯æ±‡
            r'\d+',                  # æ•°å­—
        ]
        
        meaningful_count = sum(len(re.findall(pattern, content)) for pattern in meaningful_patterns)
        if meaningful_count < 5:  # è‡³å°‘5ä¸ªæœ‰æ„ä¹‰çš„å†…å®¹ç‰‡æ®µ
            return False
        
        return True
    
    def extract_news_content(self, url: str) -> Optional[Dict]:
        """æå–æ–°é—»å†…å®¹ï¼ˆé’ˆå¯¹æ–°é—»ç½‘ç«™ä¼˜åŒ–ï¼‰
        
        Args:
            url: æ–°é—»URL
            
        Returns:
            Optional[Dict]: æå–çš„æ–°é—»å†…å®¹
        """
        # é’ˆå¯¹æ–°é—»ç½‘ç«™çš„å¢å¼ºé…ç½®
        news_config = self.chinese_config.copy()
        news_config.update({
            "waitFor": 8000,  # æ–°é—»ç½‘ç«™éœ€è¦æ›´é•¿çš„ç­‰å¾…æ—¶é—´
            "blockAds": True,
            "removeBase64Images": True,
            "formats": ["markdown", "summary", "links"],
            "onlyMainContent": True,
            "actions": [
                {"type": "wait", "milliseconds": 3000},
                {"type": "scroll", "direction": "down"},
                {"type": "wait", "milliseconds": 2000}
            ]
        })
        
        result = self.scrape_single_url(url, news_config)
        
        if result and result.get('success'):
            # å¢å¼ºæ–°é—»å†…å®¹å¤„ç†
            content = result.get('content', '')
            
            # æå–æ–°é—»å…³é”®ä¿¡æ¯
            news_info = {
                'url': url,
                'title': result.get('title', ''),
                'content': content,
                'summary': result.get('summary', ''),
                'word_count': len(content),
                'chinese_char_count': len(re.findall(r'[\u4e00-\u9fff]', content)),
                'reading_time': max(1, len(content) // 200),  # é¢„ä¼°é˜…è¯»æ—¶é—´
                'extracted_at': datetime.now().isoformat(),
                'metadata': result.get('metadata', {})
            }
            
            return news_info
        
        return None
    
    def comprehensive_news_collection(self, queries: List[str], limit_per_query: int = 5) -> List[Dict]:
        """ç»¼åˆæ–°é—»é‡‡é›†æµç¨‹
        
        Args:
            queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            limit_per_query: æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: é‡‡é›†åˆ°çš„æ–°é—»åˆ—è¡¨
        """
        self.logger.info("å¼€å§‹ç»¼åˆæ–°é—»é‡‡é›†æµç¨‹")
        
        all_news = []
        
        for query in queries:
            self.logger.info(f"å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. æœç´¢æ–°é—»
            search_results = self.search_news_v2(query, limit_per_query, ['web', 'news'])
            
            if not search_results:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æœªæ‰¾åˆ°ç»“æœ")
                continue
            
            # 2. æå–URLåˆ—è¡¨
            urls = [item['url'] for item in search_results if item.get('url')]
            
            if not urls:
                self.logger.warning(f"æŸ¥è¯¢ '{query}' æœªæ‰¾åˆ°æœ‰æ•ˆURL")
                continue
            
            # 3. æ‰¹é‡æŠ“å–
            scrape_results = self.batch_scrape_v2(urls[:3])  # é™åˆ¶å¹¶å‘æ•°é‡
            
            # 4. åˆå¹¶ç»“æœ
            for result in scrape_results:
                # æ·»åŠ æœç´¢æ—¶çš„å…ƒæ•°æ®
                for search_item in search_results:
                    if search_item['url'] == result['url']:
                        result['search_title'] = search_item['title']
                        result['search_description'] = search_item['description']
                        result['search_source_type'] = search_item['source_type']
                        break
                
                all_news.append(result)
        
        self.logger.info(f"ç»¼åˆé‡‡é›†å®Œæˆ: è·å¾— {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def save_results(self, results: List[Dict], filename: Optional[str] = None) -> str:
        """ä¿å­˜é‡‡é›†ç»“æœ
        
        Args:
            results: é‡‡é›†ç»“æœåˆ—è¡¨
            filename: æ–‡ä»¶å
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶å
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"firecrawl_v2_news_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv('FIRECRAWL_API_KEY'):
        print("âŒ è¯·è®¾ç½®FIRECRAWL_API_KEYç¯å¢ƒå˜é‡")
        print("ä¾‹å¦‚: export FIRECRAWL_API_KEY='your-api-key'")
        return
    
    try:
        # åˆ›å»ºç»Ÿä¸€é‡‡é›†å™¨
        scraper = FirecrawlV2UnifiedScraper()
        
        # å®šä¹‰æœç´¢æŸ¥è¯¢
        queries = [
            "äººå·¥æ™ºèƒ½ æœ€æ–°æ–°é—»",
            "ç§‘æŠ€èµ„è®¯ å¤´æ¡",
            "AIæŠ€æœ¯å‘å±• 2025"
        ]
        
        print("ğŸ”¥ Firecrawl v2 ç»Ÿä¸€æ–°é—»é‡‡é›†å™¨")
        print("=" * 60)
        
        # æ‰§è¡Œç»¼åˆé‡‡é›†
        results = scraper.comprehensive_news_collection(queries, limit_per_query=3)
        
        if results:
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"\nğŸ“Š é‡‡é›†ç»“æœæ‘˜è¦:")
            for i, news in enumerate(results[:5], 1):
                print(f"{i}. {news.get('search_title', news.get('title', 'æ— æ ‡é¢˜'))}")
                print(f"   URL: {news.get('url', 'æ— URL')}")
                print(f"   å†…å®¹é•¿åº¦: {news.get('word_count', len(news.get('content', '')))} å­—ç¬¦")
                print(f"   ä¸­æ–‡å­—ç¬¦: {news.get('chinese_char_count', 0)} ä¸ª")
                print(f"   é˜…è¯»æ—¶é—´: {news.get('reading_time', 0)} åˆ†é’Ÿ")
                print()
            
            # ä¿å­˜ç»“æœ
            filename = scraper.save_results(results)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        else:
            print("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ–°é—»")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
    except Exception as e:
        print(f"âŒ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
