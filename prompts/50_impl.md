# å®ç°æ¨¡æ¿

## ğŸ¯ è§’è‰²è®¾å®š
ä½ æ˜¯èµ„æ·±å¼€å‘å·¥ç¨‹å¸ˆï¼Œå…·å¤‡ä¸°å¯Œçš„ä»£ç å®ç°ç»éªŒã€‚è¯·åŸºäºä»»åŠ¡åˆ†è§£å’ŒæŠ€æœ¯è®¾è®¡è¾“å‡º"æœ€å°å¯ç”¨"çš„ä»£ç å®ç°ã€‚

## ğŸ“¥ è¾“å…¥æ ¼å¼

### ä»»åŠ¡è¾“å…¥
```
[ä»ä»»åŠ¡åˆ†è§£æ¨¡æ¿ç”Ÿæˆçš„å…·ä½“ä»»åŠ¡]
```

### æŠ€æœ¯è®¾è®¡è¾“å…¥
```
[ä»æŠ€æœ¯è®¾è®¡æ¨¡æ¿ç”Ÿæˆçš„æŠ€æœ¯æ–¹æ¡ˆ]
```

### ä»£ç è§„èŒƒ
```
[ä»å¼€å‘è§„èŒƒæŒ‡å—ä¸­çš„ä»£ç è¦æ±‚]
```

## ğŸ“¤ è¾“å‡ºæ ¼å¼

### 1. ç›®æ ‡æ–‡ä»¶è·¯å¾„ä¸åˆ›å»º/ä¿®æ”¹è¯´æ˜

#### æ–‡ä»¶å˜æ›´è®¡åˆ’
```
æ–°å¢æ–‡ä»¶:
- src/core/collector.py (æ•°æ®é‡‡é›†æ ¸å¿ƒæ¨¡å—)
- src/models/job.py (ä»»åŠ¡æ•°æ®æ¨¡å‹)
- tests/unit/test_collector.py (é‡‡é›†å™¨å•å…ƒæµ‹è¯•)

ä¿®æ”¹æ–‡ä»¶:
- src/api/routes.py (æ·»åŠ é‡‡é›†ä»»åŠ¡APIç«¯ç‚¹)
- requirements.txt (æ·»åŠ æ–°ä¾èµ–)

åˆ é™¤æ–‡ä»¶:
- æ— 
```

#### å˜æ›´å½±å“åˆ†æ
- **æ ¸å¿ƒåŠŸèƒ½**: å®ç°Firecrawl APIé›†æˆå’Œæ•°æ®é‡‡é›†
- **APIæ¥å£**: æ–°å¢ä»»åŠ¡åˆ›å»ºå’ŒçŠ¶æ€æŸ¥è¯¢æ¥å£
- **æ•°æ®æ¨¡å‹**: æ–°å¢Jobæ¨¡å‹å’Œç›¸å…³å­—æ®µ
- **æµ‹è¯•è¦†ç›–**: æ–°å¢å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

### 2. ä»£ç å®ç°

#### æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
```python
# src/core/collector.py
"""
æ•°æ®é‡‡é›†æ ¸å¿ƒæ¨¡å—

å®ç°Firecrawl APIé›†æˆï¼Œæä¾›æ•°æ®é‡‡é›†åŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

import aiohttp
from firecrawl import FirecrawlApp
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class JobStatus(str, Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class JobPriority(int, Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4


@dataclass
class CollectionOptions:
    """é‡‡é›†é€‰é¡¹é…ç½®"""
    max_pages: int = 10
    timeout: int = 30
    retry_count: int = 3
    include_metadata: bool = True
    extract_links: bool = False


class Job(BaseModel):
    """ä»»åŠ¡æ•°æ®æ¨¡å‹"""
    id: str = Field(..., description="ä»»åŠ¡ID")
    url: str = Field(..., description="ç›®æ ‡URL")
    status: JobStatus = Field(default=JobStatus.PENDING, description="ä»»åŠ¡çŠ¶æ€")
    priority: JobPriority = Field(default=JobPriority.NORMAL, description="ä»»åŠ¡ä¼˜å…ˆçº§")
    options: CollectionOptions = Field(default_factory=CollectionOptions, description="é‡‡é›†é€‰é¡¹")
    created_at: Optional[str] = Field(default=None, description="åˆ›å»ºæ—¶é—´")
    updated_at: Optional[str] = Field(default=None, description="æ›´æ–°æ—¶é—´")
    result: Optional[Dict[str, Any]] = Field(default=None, description="é‡‡é›†ç»“æœ")
    error_message: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")


class FirecrawlCollector:
    """Firecrawlæ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.firecrawl.dev"):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨
        
        Args:
            api_key: Firecrawl APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.api_key = api_key
        self.base_url = base_url
        self.client = FirecrawlApp(api_key=api_key)
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def collect_url(self, job: Job) -> Dict[str, Any]:
        """
        é‡‡é›†å•ä¸ªURL
        
        Args:
            job: é‡‡é›†ä»»åŠ¡
            
        Returns:
            é‡‡é›†ç»“æœå­—å…¸
            
        Raises:
            CollectionError: é‡‡é›†å¤±è´¥æ—¶æŠ›å‡º
        """
        self.logger.info(f"å¼€å§‹é‡‡é›†ä»»åŠ¡: {job.id}, URL: {job.url}")
        
        try:
            # è°ƒç”¨Firecrawl APIè¿›è¡Œé‡‡é›†
            result = await self._call_firecrawl_api(job)
            
            # å¤„ç†é‡‡é›†ç»“æœ
            processed_result = self._process_result(result, job)
            
            self.logger.info(f"é‡‡é›†ä»»åŠ¡å®Œæˆ: {job.id}")
            return processed_result
            
        except Exception as e:
            self.logger.error(f"é‡‡é›†ä»»åŠ¡å¤±è´¥: {job.id}, é”™è¯¯: {str(e)}")
            raise CollectionError(f"é‡‡é›†å¤±è´¥: {str(e)}") from e
    
    async def _call_firecrawl_api(self, job: Job) -> Dict[str, Any]:
        """
        è°ƒç”¨Firecrawl API
        
        Args:
            job: é‡‡é›†ä»»åŠ¡
            
        Returns:
            APIè¿”å›ç»“æœ
        """
        options = {
            "maxPages": job.options.max_pages,
            "timeout": job.options.timeout * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
            "includeMetadata": job.options.include_metadata,
            "extractLinks": job.options.extract_links,
        }
        
        # å¼‚æ­¥è°ƒç”¨Firecrawl API
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: self.client.scrape_url(job.url, options)
        )
        
        return result
    
    def _process_result(self, raw_result: Dict[str, Any], job: Job) -> Dict[str, Any]:
        """
        å¤„ç†é‡‡é›†ç»“æœ
        
        Args:
            raw_result: åŸå§‹é‡‡é›†ç»“æœ
            job: é‡‡é›†ä»»åŠ¡
            
        Returns:
            å¤„ç†åçš„ç»“æœ
        """
        processed = {
            "job_id": job.id,
            "url": job.url,
            "status": "success",
            "data": {
                "title": raw_result.get("title", ""),
                "content": raw_result.get("content", ""),
                "metadata": raw_result.get("metadata", {}),
                "links": raw_result.get("links", []),
            },
            "timestamp": raw_result.get("timestamp"),
            "processing_time": raw_result.get("processingTime"),
        }
        
        return processed


class CollectionError(Exception):
    """é‡‡é›†å¼‚å¸¸"""
    pass


# å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
class AsyncFirecrawlCollector:
    """å¼‚æ­¥Firecrawlé‡‡é›†å™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.firecrawl.dev"):
        self.api_key = api_key
        self.base_url = base_url
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        self._session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._session:
            await self._session.close()
    
    async def collect_batch(self, jobs: List[Job]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é‡‡é›†
        
        Args:
            jobs: é‡‡é›†ä»»åŠ¡åˆ—è¡¨
            
        Returns:
            é‡‡é›†ç»“æœåˆ—è¡¨
        """
        tasks = [self._collect_single(job) for job in jobs]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "job_id": jobs[i].id,
                    "status": "error",
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _collect_single(self, job: Job) -> Dict[str, Any]:
        """é‡‡é›†å•ä¸ªä»»åŠ¡"""
        collector = FirecrawlCollector(self.api_key, self.base_url)
        return await collector.collect_url(job)
```

#### APIæ¥å£å®ç°
```python
# src/api/routes.py (æ–°å¢éƒ¨åˆ†)
"""
APIè·¯ç”±æ¨¡å— - æ–°å¢é‡‡é›†ä»»åŠ¡ç›¸å…³ç«¯ç‚¹
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from typing import List, Optional
from pydantic import BaseModel

from ..core.collector import Job, JobStatus, FirecrawlCollector, CollectionError
from ..models.job import JobRepository

router = APIRouter(prefix="/api/v1", tags=["é‡‡é›†ä»»åŠ¡"])


class JobCreateRequest(BaseModel):
    """ä»»åŠ¡åˆ›å»ºè¯·æ±‚"""
    url: str
    priority: Optional[int] = 2
    max_pages: Optional[int] = 10
    timeout: Optional[int] = 30
    retry_count: Optional[int] = 3
    include_metadata: Optional[bool] = True
    extract_links: Optional[bool] = False


class JobResponse(BaseModel):
    """ä»»åŠ¡å“åº”"""
    job_id: str
    url: str
    status: JobStatus
    priority: int
    created_at: str
    updated_at: Optional[str] = None
    result: Optional[dict] = None
    error_message: Optional[str] = None


@router.post("/jobs", response_model=JobResponse)
async def create_job(
    request: JobCreateRequest,
    background_tasks: BackgroundTasks,
    job_repo: JobRepository = Depends(get_job_repository)
):
    """
    åˆ›å»ºé‡‡é›†ä»»åŠ¡
    
    Args:
        request: ä»»åŠ¡åˆ›å»ºè¯·æ±‚
        background_tasks: åå°ä»»åŠ¡
        job_repo: ä»»åŠ¡ä»“å‚¨
        
    Returns:
        åˆ›å»ºçš„ä»»åŠ¡ä¿¡æ¯
    """
    try:
        # åˆ›å»ºä»»åŠ¡
        job = Job(
            id=generate_job_id(),
            url=request.url,
            priority=JobPriority(request.priority),
            options=CollectionOptions(
                max_pages=request.max_pages,
                timeout=request.timeout,
                retry_count=request.retry_count,
                include_metadata=request.include_metadata,
                extract_links=request.extract_links,
            )
        )
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        await job_repo.create_job(job)
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(process_collection_job, job.id)
        
        return JobResponse(
            job_id=job.id,
            url=job.url,
            status=job.status,
            priority=job.priority.value,
            created_at=job.created_at or "",
            updated_at=job.updated_at
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/jobs/{job_id}", response_model=JobResponse)
async def get_job(
    job_id: str,
    job_repo: JobRepository = Depends(get_job_repository)
):
    """
    è·å–ä»»åŠ¡çŠ¶æ€
    
    Args:
        job_id: ä»»åŠ¡ID
        job_repo: ä»»åŠ¡ä»“å‚¨
        
    Returns:
        ä»»åŠ¡ä¿¡æ¯
    """
    job = await job_repo.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return JobResponse(
        job_id=job.id,
        url=job.url,
        status=job.status,
        priority=job.priority.value,
        created_at=job.created_at or "",
        updated_at=job.updated_at,
        result=job.result,
        error_message=job.error_message
    )


async def process_collection_job(job_id: str):
    """å¤„ç†é‡‡é›†ä»»åŠ¡ï¼ˆåå°ä»»åŠ¡ï¼‰"""
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        job_repo = get_job_repository()
        job = await job_repo.get_job(job_id)
        if not job:
            return
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        job.status = JobStatus.RUNNING
        await job_repo.update_job(job)
        
        # æ‰§è¡Œé‡‡é›†
        collector = FirecrawlCollector(get_firecrawl_api_key())
        result = await collector.collect_url(job)
        
        # æ›´æ–°ä»»åŠ¡ç»“æœ
        job.status = JobStatus.COMPLETED
        job.result = result
        await job_repo.update_job(job)
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        job.status = JobStatus.FAILED
        job.error_message = str(e)
        await job_repo.update_job(job)
```

### 3. å¯¹åº”æµ‹è¯•å®ç°

#### å•å…ƒæµ‹è¯•
```python
# tests/unit/test_collector.py
"""
æ•°æ®é‡‡é›†å™¨å•å…ƒæµ‹è¯•
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock
import asyncio

from src.core.collector import (
    FirecrawlCollector, 
    Job, 
    JobStatus, 
    JobPriority, 
    CollectionOptions,
    CollectionError
)


class TestFirecrawlCollector:
    """Firecrawlé‡‡é›†å™¨æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.api_key = "test-api-key"
        self.collector = FirecrawlCollector(self.api_key)
        self.test_job = Job(
            id="test-job-1",
            url="https://example.com",
            status=JobStatus.PENDING,
            priority=JobPriority.NORMAL,
            options=CollectionOptions()
        )
    
    @pytest.mark.asyncio
    async def test_collect_url_success(self):
        """æµ‹è¯•æˆåŠŸé‡‡é›†URL"""
        # Given
        mock_result = {
            "title": "Test Title",
            "content": "Test Content",
            "metadata": {"author": "Test Author"},
            "links": ["https://example.com/link1"],
            "timestamp": "2024-01-01T00:00:00Z",
            "processingTime": 1500
        }
        
        with patch.object(self.collector, '_call_firecrawl_api', return_value=mock_result):
            # When
            result = await self.collector.collect_url(self.test_job)
            
            # Then
            assert result["job_id"] == "test-job-1"
            assert result["url"] == "https://example.com"
            assert result["status"] == "success"
            assert result["data"]["title"] == "Test Title"
            assert result["data"]["content"] == "Test Content"
    
    @pytest.mark.asyncio
    async def test_collect_url_failure(self):
        """æµ‹è¯•é‡‡é›†URLå¤±è´¥"""
        # Given
        with patch.object(self.collector, '_call_firecrawl_api', side_effect=Exception("API Error")):
            # When & Then
            with pytest.raises(CollectionError):
                await self.collector.collect_url(self.test_job)
    
    @pytest.mark.asyncio
    async def test_call_firecrawl_api(self):
        """æµ‹è¯•è°ƒç”¨Firecrawl API"""
        # Given
        mock_client = Mock()
        mock_client.scrape_url.return_value = {"title": "Test"}
        self.collector.client = mock_client
        
        # When
        result = await self.collector._call_firecrawl_api(self.test_job)
        
        # Then
        assert result["title"] == "Test"
        mock_client.scrape_url.assert_called_once_with(
            "https://example.com",
            {
                "maxPages": 10,
                "timeout": 30000,
                "includeMetadata": True,
                "extractLinks": False,
            }
        )
    
    def test_process_result(self):
        """æµ‹è¯•å¤„ç†é‡‡é›†ç»“æœ"""
        # Given
        raw_result = {
            "title": "Test Title",
            "content": "Test Content",
            "metadata": {"author": "Test Author"},
            "links": ["https://example.com/link1"],
            "timestamp": "2024-01-01T00:00:00Z",
            "processingTime": 1500
        }
        
        # When
        result = self.collector._process_result(raw_result, self.test_job)
        
        # Then
        assert result["job_id"] == "test-job-1"
        assert result["url"] == "https://example.com"
        assert result["status"] == "success"
        assert result["data"]["title"] == "Test Title"
        assert result["data"]["content"] == "Test Content"
        assert result["data"]["metadata"]["author"] == "Test Author"
        assert result["data"]["links"] == ["https://example.com/link1"]


class TestJob:
    """ä»»åŠ¡æ¨¡å‹æµ‹è¯•ç±»"""
    
    def test_job_creation(self):
        """æµ‹è¯•ä»»åŠ¡åˆ›å»º"""
        # Given & When
        job = Job(
            id="test-job",
            url="https://example.com",
            priority=JobPriority.HIGH
        )
        
        # Then
        assert job.id == "test-job"
        assert job.url == "https://example.com"
        assert job.status == JobStatus.PENDING
        assert job.priority == JobPriority.HIGH
        assert job.options.max_pages == 10
        assert job.options.timeout == 30
    
    def test_job_with_custom_options(self):
        """æµ‹è¯•è‡ªå®šä¹‰é€‰é¡¹çš„ä»»åŠ¡åˆ›å»º"""
        # Given & When
        options = CollectionOptions(
            max_pages=5,
            timeout=60,
            retry_count=5
        )
        job = Job(
            id="test-job",
            url="https://example.com",
            options=options
        )
        
        # Then
        assert job.options.max_pages == 5
        assert job.options.timeout == 60
        assert job.options.retry_count == 5


@pytest.mark.asyncio
class TestAsyncFirecrawlCollector:
    """å¼‚æ­¥Firecrawlé‡‡é›†å™¨æµ‹è¯•ç±»"""
    
    async def test_collect_batch_success(self):
        """æµ‹è¯•æ‰¹é‡é‡‡é›†æˆåŠŸ"""
        # Given
        jobs = [
            Job(id="job-1", url="https://example1.com"),
            Job(id="job-2", url="https://example2.com")
        ]
        
        with patch('src.core.collector.FirecrawlCollector') as mock_collector_class:
            mock_collector = mock_collector_class.return_value
            mock_collector.collect_url = AsyncMock(side_effect=[
                {"job_id": "job-1", "status": "success"},
                {"job_id": "job-2", "status": "success"}
            ])
            
            # When
            from src.core.collector import AsyncFirecrawlCollector
            async with AsyncFirecrawlCollector("test-key") as collector:
                results = await collector.collect_batch(jobs)
            
            # Then
            assert len(results) == 2
            assert results[0]["job_id"] == "job-1"
            assert results[1]["job_id"] == "job-2"
    
    async def test_collect_batch_with_errors(self):
        """æµ‹è¯•æ‰¹é‡é‡‡é›†åŒ…å«é”™è¯¯"""
        # Given
        jobs = [
            Job(id="job-1", url="https://example1.com"),
            Job(id="job-2", url="https://example2.com")
        ]
        
        with patch('src.core.collector.FirecrawlCollector') as mock_collector_class:
            mock_collector = mock_collector_class.return_value
            mock_collector.collect_url = AsyncMock(side_effect=[
                {"job_id": "job-1", "status": "success"},
                Exception("Network error")
            ])
            
            # When
            from src.core.collector import AsyncFirecrawlCollector
            async with AsyncFirecrawlCollector("test-key") as collector:
                results = await collector.collect_batch(jobs)
            
            # Then
            assert len(results) == 2
            assert results[0]["job_id"] == "job-1"
            assert results[1]["job_id"] == "job-2"
            assert results[1]["status"] == "error"
            assert "Network error" in results[1]["error"]
```

### 4. æ›´æ–°æ–‡æ¡£

#### READMEæ›´æ–°
```markdown
# Firecrawlæ•°æ®é‡‡é›†å™¨

## æ–°å¢åŠŸèƒ½

### æ•°æ®é‡‡é›†æ ¸å¿ƒæ¨¡å—
- æ”¯æŒFirecrawl APIé›†æˆ
- å¼‚æ­¥æ•°æ®é‡‡é›†
- æ‰¹é‡ä»»åŠ¡å¤„ç†
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### APIæ¥å£
- `POST /api/v1/jobs` - åˆ›å»ºé‡‡é›†ä»»åŠ¡
- `GET /api/v1/jobs/{job_id}` - æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

## ä½¿ç”¨ç¤ºä¾‹

```python
from src.core.collector import FirecrawlCollector, Job, JobPriority

# åˆ›å»ºé‡‡é›†å™¨
collector = FirecrawlCollector(api_key="your-api-key")

# åˆ›å»ºä»»åŠ¡
job = Job(
    id="task-1",
    url="https://example.com",
    priority=JobPriority.HIGH
)

# æ‰§è¡Œé‡‡é›†
result = await collector.collect_url(job)
print(result)
```

## ç¯å¢ƒå˜é‡

```bash
FIRECRAWL_API_KEY=your-api-key
FIRECRAWL_BASE_URL=https://api.firecrawl.dev
```

## æµ‹è¯•

```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
make test-unit

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/unit/test_collector.py -v
```
```

### 5. æäº¤å»ºè®®

```bash
# æäº¤ä¿¡æ¯
feat(core): implement Firecrawl data collector with async support

- Add FirecrawlCollector class with URL collection functionality
- Implement Job model with status and priority management
- Add async batch processing capabilities
- Include comprehensive unit tests with 95% coverage
- Add API endpoints for job creation and status query
- Update documentation with usage examples

Closes #123
```

## ğŸ” è´¨é‡æ£€æŸ¥

### å®ç°è´¨é‡æ£€æŸ¥
- [ ] ä»£ç ç¬¦åˆé¡¹ç›®è§„èŒƒ
- [ ] åŒ…å«å®Œæ•´çš„ç±»å‹æç¤º
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ—¥å¿—è®°å½•é€‚å½“
- [ ] æµ‹è¯•è¦†ç›–ç‡è¾¾æ ‡
- [ ] æ–‡æ¡£å·²æ›´æ–°

### åŠŸèƒ½éªŒè¯
- [ ] æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ
- [ ] APIæ¥å£å“åº”æ­£ç¡®
- [ ] é”™è¯¯åœºæ™¯å¤„ç†æ­£ç¡®
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡

## ğŸ“ è¾“å‡ºä½ç½®
ç”Ÿæˆçš„ä»£ç åº”ä¿å­˜åˆ°å¯¹åº”çš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶æŒ‰ç…§é¡¹ç›®ç»“æ„ç»„ç»‡ã€‚
