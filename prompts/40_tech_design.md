# æŠ€æœ¯è®¾è®¡æ¨¡æ¿

## ğŸ¯ è§’è‰²è®¾å®š
ä½ æ˜¯æ¶æ„å¸ˆï¼Œå…·å¤‡ä¸°å¯Œçš„ç³»ç»Ÿè®¾è®¡å’Œæ¶æ„ç»éªŒã€‚è¯·äº§å‡ºæŠ€æœ¯æ–¹æ¡ˆï¼Œå…¼é¡¾å¯å®ç°æ€§ä¸å¯æ¼”è¿›æ€§ã€‚

## ğŸ“¥ è¾“å…¥æ ¼å¼

### ä»»åŠ¡åˆ†è§£è¾“å…¥
```
[ä»ä»»åŠ¡åˆ†è§£æ¨¡æ¿ç”Ÿæˆçš„ä»»åŠ¡åˆ—è¡¨]
```

### æŠ€æœ¯çº¦æŸ
```
[åˆ—å‡ºæŠ€æœ¯å®ç°æ–¹é¢çš„çº¦æŸæ¡ä»¶]
```

### æ€§èƒ½è¦æ±‚
```
[åˆ—å‡ºæ€§èƒ½å’Œæ‰©å±•æ€§è¦æ±‚]
```

## ğŸ“¤ è¾“å‡ºç»“æ„

### 1. æ¶æ„/æ¨¡å—å›¾ä¸èŒè´£è¾¹ç•Œ

#### ç³»ç»Ÿæ¶æ„å›¾
```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[Web UI]
        API_Client[API Client]
    end
    
    subgraph "API Layer"
        Gateway[API Gateway]
        Auth[Authentication]
        Routes[Route Handlers]
    end
    
    subgraph "Business Layer"
        Scheduler[Task Scheduler]
        Collector[Data Collector]
        Processor[Data Processor]
        Observer[Task Observer]
    end
    
    subgraph "Data Layer"
        Cache[Redis Cache]
        DB[(PostgreSQL)]
        Storage[File Storage]
    end
    
    subgraph "External Services"
        Firecrawl[Firecrawl API]
        Monitoring[Monitoring System]
    end
    
    UI --> API_Client
    API_Client --> Gateway
    Gateway --> Auth
    Gateway --> Routes
    Routes --> Scheduler
    Routes --> Observer
    Scheduler --> Collector
    Collector --> Firecrawl
    Collector --> Processor
    Processor --> Cache
    Processor --> DB
    Observer --> Monitoring
```

#### æ¨¡å—èŒè´£è¾¹ç•Œ
| æ¨¡å—           | èŒè´£                    | æ¥å£            | ä¾èµ–                |
| -------------- | ----------------------- | --------------- | ------------------- |
| API Gateway    | è¯·æ±‚è·¯ç”±ã€è®¤è¯ã€é™æµ    | HTTP REST API   | Auth, Routes        |
| Task Scheduler | ä»»åŠ¡è°ƒåº¦ã€é˜Ÿåˆ—ç®¡ç†      | Queue Interface | Collector, Observer |
| Data Collector | æ•°æ®é‡‡é›†ã€Firecrawlé›†æˆ | Firecrawl API   | Processor           |
| Data Processor | æ•°æ®æ¸…æ´—ã€ç»“æ„åŒ–        | Data Pipeline   | Cache, DB           |
| Task Observer  | ä»»åŠ¡ç›‘æ§ã€çŠ¶æ€ç®¡ç†      | Event Interface | Monitoring          |

### 2. æ•°æ®æµä¸æ¥å£è®¾è®¡

#### æ•°æ®æµç¨‹å›¾
```mermaid
sequenceDiagram
    participant User
    participant API
    participant Scheduler
    participant Collector
    participant Processor
    participant DB
    
    User->>API: åˆ›å»ºé‡‡é›†ä»»åŠ¡
    API->>Scheduler: æäº¤ä»»åŠ¡
    Scheduler->>Collector: æ‰§è¡Œé‡‡é›†
    Collector->>Processor: åŸå§‹æ•°æ®
    Processor->>DB: ç»“æ„åŒ–æ•°æ®
    DB-->>User: è¿”å›ç»“æœ
```

#### æ¥å£è®¾è®¡
```yaml
# ä»»åŠ¡åˆ›å»ºæ¥å£
POST /api/v1/jobs:
  request:
    url: string
    options:
      max_pages: integer
      timeout: integer
      retry_count: integer
  response:
    job_id: string
    status: string
    created_at: timestamp

# ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£
GET /api/v1/jobs/{job_id}:
  response:
    job_id: string
    status: enum[pending, running, completed, failed]
    progress: integer
    result: object
    error_message: string

# æ•°æ®æŸ¥è¯¢æ¥å£
GET /api/v1/data:
  query:
    job_id: string
    page: integer
    limit: integer
  response:
    data: array
    total: integer
    page: integer
    limit: integer
```

#### é”™è¯¯å¤„ç†ä¸é‡è¯•ç­–ç•¥
```python
class RetryConfig:
    max_retries: int = 3
    backoff_factor: float = 2.0
    max_backoff: int = 60
    retry_on_status: List[int] = [500, 502, 503, 504]

class ErrorHandler:
    def handle_firecrawl_error(self, error: FirecrawlError):
        if error.status_code in [429, 503]:
            # é™æµæˆ–æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
            return self.retry_with_backoff()
        elif error.status_code == 404:
            # URLä¸å­˜åœ¨ï¼Œç›´æ¥å¤±è´¥
            return self.fail_task()
        else:
            # å…¶ä»–é”™è¯¯ï¼Œé‡è¯•
            return self.retry_task()
```

### 3. å…³é”®ç®—æ³•/åº“é€‰å‹ä¸å–èˆç†ç”±

#### æ ¸å¿ƒæŠ€æœ¯é€‰å‹
| æŠ€æœ¯     | é€‰æ‹©       | ç†ç”±                       | æ›¿ä»£æ–¹æ¡ˆ          | å–èˆåŸå›                         |
| -------- | ---------- | -------------------------- | ----------------- | ------------------------------- |
| Webæ¡†æ¶  | FastAPI    | é«˜æ€§èƒ½ã€è‡ªåŠ¨æ–‡æ¡£ã€ç±»å‹æç¤º | Flask, Django     | FastAPIæ€§èƒ½æ›´å¥½ï¼Œæ–‡æ¡£ç”Ÿæˆæ›´å®Œå–„ |
| æ•°æ®åº“   | PostgreSQL | æ”¯æŒJSONã€äº‹åŠ¡ã€æ‰©å±•æ€§å¥½   | MySQL, MongoDB    | PostgreSQLå¯¹JSONæ”¯æŒæ›´å¥½        |
| ç¼“å­˜     | Redis      | é«˜æ€§èƒ½ã€æ”¯æŒå¤šç§æ•°æ®ç»“æ„   | Memcached         | RedisåŠŸèƒ½æ›´ä¸°å¯Œ                 |
| ä»»åŠ¡é˜Ÿåˆ— | Celery     | æˆç†Ÿç¨³å®šã€åŠŸèƒ½å®Œæ•´         | RQ, Dramatiq      | Celeryç”Ÿæ€æ›´å®Œå–„                |
| ç›‘æ§     | Prometheus | æŒ‡æ ‡æ”¶é›†ã€å‘Šè­¦ã€å¯è§†åŒ–     | InfluxDB, DataDog | Prometheuså¼€æºä¸”åŠŸèƒ½å¼ºå¤§        |

#### ç®—æ³•é€‰æ‹©
```python
# æŒ‡æ•°é€€é¿é‡è¯•ç®—æ³•
class ExponentialBackoff:
    def __init__(self, base_delay: float = 1.0, max_delay: float = 60.0):
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.attempt = 0
    
    def get_delay(self) -> float:
        delay = self.base_delay * (2 ** self.attempt)
        return min(delay, self.max_delay)
    
    def next_attempt(self):
        self.attempt += 1

# æ•°æ®æ¸…æ´—ç®—æ³•
class DataCleaner:
    def clean_html(self, html: str) -> str:
        # ä½¿ç”¨BeautifulSoupæ¸…ç†HTML
        soup = BeautifulSoup(html, 'html.parser')
        return soup.get_text()
    
    def extract_structured_data(self, text: str) -> Dict:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç»“æ„åŒ–æ•°æ®
        patterns = {
            'title': r'<title>(.*?)</title>',
            'description': r'<meta name="description" content="(.*?)">',
            'author': r'<meta name="author" content="(.*?)">'
        }
        return {key: re.search(pattern, text) for key, pattern in patterns.items()}
```

### 4. å¯æµ‹è¯•æ€§è®¾è®¡

#### ä¾èµ–æ³¨å…¥è®¾è®¡
```python
from abc import ABC, abstractmethod

class DataCollectorInterface(ABC):
    @abstractmethod
    async def collect(self, url: str) -> Dict[str, Any]:
        pass

class FirecrawlCollector(DataCollectorInterface):
    def __init__(self, api_key: str, timeout: int = 30):
        self.api_key = api_key
        self.timeout = timeout
    
    async def collect(self, url: str) -> Dict[str, Any]:
        # å®ç°Firecrawlé‡‡é›†é€»è¾‘
        pass

class MockCollector(DataCollectorInterface):
    async def collect(self, url: str) -> Dict[str, Any]:
        # è¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        return {"title": "Test Title", "content": "Test Content"}

# ä½¿ç”¨ä¾èµ–æ³¨å…¥
class TaskProcessor:
    def __init__(self, collector: DataCollectorInterface):
        self.collector = collector
    
    async def process_task(self, url: str):
        data = await self.collector.collect(url)
        return self.process_data(data)
```

#### æ¥å£éš”ç¦»è®¾è®¡
```python
# æ•°æ®è®¿é—®æ¥å£
class JobRepositoryInterface(ABC):
    @abstractmethod
    async def create_job(self, job_data: Dict) -> Job:
        pass
    
    @abstractmethod
    async def get_job(self, job_id: str) -> Optional[Job]:
        pass
    
    @abstractmethod
    async def update_job_status(self, job_id: str, status: JobStatus):
        pass

# å®ç°ç±»
class PostgreSQLJobRepository(JobRepositoryInterface):
    def __init__(self, db_session):
        self.db_session = db_session
    
    async def create_job(self, job_data: Dict) -> Job:
        # å®ç°æ•°æ®åº“æ“ä½œ
        pass
```

#### å¥‘çº¦æµ‹è¯•è®¾è®¡
```python
# å¥‘çº¦æµ‹è¯•
class TestFirecrawlAPIContract:
    def test_collect_endpoint_contract(self):
        # æµ‹è¯•Firecrawl APIæ¥å£å¥‘çº¦
        response = self.firecrawl_client.collect(url="https://example.com")
        
        # éªŒè¯å“åº”ç»“æ„
        assert "title" in response
        assert "content" in response
        assert "metadata" in response
        
        # éªŒè¯æ•°æ®ç±»å‹
        assert isinstance(response["title"], str)
        assert isinstance(response["content"], str)
        assert isinstance(response["metadata"], dict)
```

### 5. è§‚æµ‹æ€§è®¾è®¡

#### æ—¥å¿—ç»“æ„åŒ–è®¾è®¡
```python
import structlog
import uuid

logger = structlog.get_logger()

class TaskProcessor:
    def __init__(self):
        self.logger = logger.bind(component="task_processor")
    
    async def process_task(self, task_id: str, url: str):
        trace_id = str(uuid.uuid4())
        self.logger = self.logger.bind(trace_id=trace_id, task_id=task_id)
        
        self.logger.info("å¼€å§‹å¤„ç†ä»»åŠ¡", url=url)
        
        try:
            result = await self.collect_data(url)
            self.logger.info("ä»»åŠ¡å¤„ç†æˆåŠŸ", result_size=len(str(result)))
            return result
        except Exception as e:
            self.logger.error("ä»»åŠ¡å¤„ç†å¤±è´¥", error=str(e), exc_info=True)
            raise
```

#### å…³é”®æŒ‡æ ‡è®¾è®¡
```python
from prometheus_client import Counter, Histogram, Gauge

# æŒ‡æ ‡å®šä¹‰
job_counter = Counter('firecrawl_jobs_total', 'Total number of jobs', ['status'])
job_duration = Histogram('firecrawl_job_duration_seconds', 'Job processing duration')
active_jobs = Gauge('firecrawl_active_jobs', 'Number of active jobs')
api_requests = Counter('firecrawl_api_requests_total', 'API requests', ['endpoint', 'status'])

class MetricsCollector:
    def record_job_start(self, job_id: str):
        active_jobs.inc()
    
    def record_job_completion(self, job_id: str, status: str, duration: float):
        job_counter.labels(status=status).inc()
        job_duration.observe(duration)
        active_jobs.dec()
    
    def record_api_request(self, endpoint: str, status: int):
        api_requests.labels(endpoint=endpoint, status=str(status)).inc()
```

#### é“¾è·¯è¿½è¸ªè®¾è®¡
```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# é“¾è·¯è¿½è¸ªé…ç½®
tracer = trace.get_tracer(__name__)

class TaskProcessor:
    async def process_task(self, task_id: str, url: str):
        with tracer.start_as_current_span("process_task") as span:
            span.set_attribute("task_id", task_id)
            span.set_attribute("url", url)
            
            # é‡‡é›†æ•°æ®
            with tracer.start_as_current_span("collect_data"):
                data = await self.collect_data(url)
            
            # å¤„ç†æ•°æ®
            with tracer.start_as_current_span("process_data"):
                result = await self.process_data(data)
            
            span.set_attribute("result_size", len(str(result)))
            return result
```

### 6. é£é™©ç¼“è§£ç­–ç•¥

#### é™çº§ç­–ç•¥
```python
class CircuitBreaker:
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    async def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitBreakerOpenError()
        
        try:
            result = await func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
```

#### é™æµç­–ç•¥
```python
import asyncio
from collections import deque

class RateLimiter:
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.time()
            
            # æ¸…ç†è¿‡æœŸè¯·æ±‚
            while self.requests and self.requests[0] <= now - self.time_window:
                self.requests.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.requests) >= self.max_requests:
                sleep_time = self.requests[0] + self.time_window - now
                await asyncio.sleep(sleep_time)
                return await self.acquire()
            
            self.requests.append(now)
```

#### å¹‚ç­‰æ€§è®¾è®¡
```python
class IdempotentTaskProcessor:
    def __init__(self, redis_client):
        self.redis_client = redis_client
    
    async def process_task(self, task_id: str, url: str):
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        cache_key = f"task_result:{task_id}"
        cached_result = await self.redis_client.get(cache_key)
        
        if cached_result:
            return json.loads(cached_result)
        
        # å¤„ç†ä»»åŠ¡
        result = await self._do_process_task(url)
        
        # ç¼“å­˜ç»“æœ
        await self.redis_client.setex(
            cache_key, 
            3600,  # 1å°æ—¶è¿‡æœŸ
            json.dumps(result)
        )
        
        return result
```

## ğŸ” è´¨é‡æ£€æŸ¥

### æŠ€æœ¯è®¾è®¡è´¨é‡æ£€æŸ¥
- [ ] æ¶æ„å›¾æ¸…æ™°ï¼Œæ¨¡å—èŒè´£æ˜ç¡®
- [ ] æ•°æ®æµè®¾è®¡åˆç†
- [ ] æ¥å£è®¾è®¡å®Œæ•´
- [ ] é”™è¯¯å¤„ç†ç­–ç•¥å®Œå–„
- [ ] å¯æµ‹è¯•æ€§è®¾è®¡å……åˆ†
- [ ] è§‚æµ‹æ€§è®¾è®¡å®Œæ•´
- [ ] é£é™©ç¼“è§£ç­–ç•¥æœ‰æ•ˆ

## ğŸ“ è¾“å‡ºä½ç½®
ç”Ÿæˆçš„æŠ€æœ¯è®¾è®¡æ–‡æ¡£åº”ä¿å­˜åˆ° `docs/TECH_DESIGN.md` æ–‡ä»¶ä¸­ï¼Œå¹¶æŒ‰ç…§ä¸Šè¿°ç»“æ„ç»„ç»‡å†…å®¹ã€‚
